{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Lexical Substitution Effect estimation using CausalForest\n",
    "- R package: https://cran.r-project.org/web/packages/SparseM/vignettes/SparseM.pdf <br>\n",
    "- n_trees = [200, 2000] <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'rpy2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-340744ce6226>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspatial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistance\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcdist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpairwise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mrpy2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrpy2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrobjects\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mro\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrpy2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrobjects\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'rpy2'"
     ]
    }
   ],
   "source": [
    "import requests, time, operator, re, json, csv, pickle, copy, gc, ast\n",
    "import timeit\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from ast import literal_eval\n",
    "from scipy import io\n",
    "from scipy.sparse import vstack\n",
    "from scipy.spatial.distance import cdist\n",
    "import sklearn.metrics.pairwise\n",
    "import rpy2\n",
    "import rpy2.robjects as ro\n",
    "from rpy2.robjects import r\n",
    "from rpy2.robjects import Matrix\n",
    "import rpy2.robjects.numpy2ri\n",
    "from rpy2.robjects.vectors import Matrix\n",
    "from rpy2.robjects.packages import importr\n",
    "importr('grf')\n",
    "rpy2.robjects.numpy2ri.activate()\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "AMT_path = '/data/2/zwang/2018_S_WordTreatment/V2_AMT/'\n",
    "airbnb_path = '/data/2/zwang/2018_S_WordTreatment/V2_airbnb/'\n",
    "tw_path = '/data/2/zwang/2018_S_WordTreatment/V2_twitter/'\n",
    "yp_path = '/data/2/zwang/2018_S_WordTreatment/V2_yelp/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CSF10_treatment_for_pair(sents,labels,divide,CSF_pred):\n",
    "    source_pos_treatment = []\n",
    "    source_neg_treatment = []\n",
    "    target_pos_treatment = []\n",
    "    target_neg_treatment = []\n",
    "    \n",
    "    for i in range(len(sents)):\n",
    "        if(i<divide and labels[i]==1):\n",
    "            source_pos_treatment.append((sents[i],round(float(CSF_pred[i]),5)))#,CSF_var[i]\n",
    "        elif(i<divide and labels[i]==0):\n",
    "            source_neg_treatment.append((sents[i],round(float(CSF_pred[i]),5)))#,CSF_var[i]\n",
    "        elif(i>=divide and labels[i]==1):\n",
    "            target_pos_treatment.append((sents[i],round(float(CSF_pred[i]),5)))#,CSF_var[i]\n",
    "        elif(i>=divide and labels[i]==0):\n",
    "            target_neg_treatment.append((sents[i],round(float(CSF_pred[i]),5)))#,CSF_var[i]\n",
    "            \n",
    "    return {'source_pos_sents_treatment':[source_pos_treatment],'source_neg_sents_treatment':[source_neg_treatment],\n",
    "            'target_pos_sents_treatment':[target_pos_treatment],'target_neg_sents_treatment':[target_neg_treatment]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_10CSF(X_src,y_src,W_src,X_src_testidx,X_src_trainidx,X_tar,y_tar,W_tar,X_tar_testidx,X_tar_trainidx,nfold):\n",
    "    #https://cran.r-project.org/web/packages/SparseM/vignettes/SparseM.pdf\n",
    "    \n",
    "    # train 10 CF for 10 folds of X_src\n",
    "    src_pred = []\n",
    "    #src_var = []\n",
    "    tar_pred = []\n",
    "    #tar_var = []\n",
    "    \n",
    "    for i in range(X_src.shape[0]):\n",
    "        src_pred.append(-2)\n",
    "        #src_var.append(-2)\n",
    "    for i in range(X_tar.shape[0]):\n",
    "        tar_pred.append(-2)\n",
    "        #tar_var.append(-2)\n",
    "    \n",
    "    X_src_arr = X_src.toarray()\n",
    "    X_tar_arr = X_tar.toarray()\n",
    "    y_src_arr = np.array(y_src)\n",
    "    y_tar_arr = np.array(y_tar)\n",
    "    W_src_arr = np.array(W_src)\n",
    "    W_tar_arr = np.array(W_tar)\n",
    "    \n",
    "    \n",
    "    for i in range(nfold):\n",
    "        test_vec_src = X_src_arr[X_src_testidx[i]]\n",
    "        #test_vec_src_arr = test_vec_src.toarray()\n",
    "        X_test_src = r.matrix(test_vec_src, nrow=test_vec_src.shape[0], ncol=test_vec_src.shape[1])\n",
    "        #X_test_src = r.matrix(test_vec_src, nrow=test_vec_src.shape[0], ncol=test_vec_src.shape[1],sparce=True)\n",
    "        train_vec_src = X_src_arr[X_src_trainidx[i]]\n",
    "        train_y_src = y_src_arr[X_src_trainidx[i]]\n",
    "        train_W_src = W_src_arr[X_src_trainidx[i]]\n",
    "        \n",
    "        test_vec_tar = X_tar_arr[X_tar_testidx[i]]\n",
    "        #test_vec_tar_arr = test_vec_tar.toarray()\n",
    "        X_test_tar = r.matrix(test_vec_tar, nrow=test_vec_tar.shape[0], ncol=test_vec_tar.shape[1])\n",
    "        train_vec_tar = X_tar_arr[X_tar_trainidx[i]]\n",
    "        train_y_tar = y_tar_arr[X_tar_trainidx[i]]\n",
    "        train_W_tar = W_tar_arr[X_tar_trainidx[i]]\n",
    "        \n",
    "        train_vec_all = np.vstack((train_vec_src,train_vec_tar))\n",
    "        #train_vec_all_arr = train_vec_all.toarray()\n",
    "        X_train = r.matrix(train_vec_all, nrow=train_vec_all.shape[0], ncol=train_vec_all.shape[1])   \n",
    "        y_train = list(train_y_src) + list(train_y_tar)\n",
    "        W_src_train = list(train_W_src) + list(train_W_tar)\n",
    "        W_tar_train = list(1-np.array(W_src_train))\n",
    "        \n",
    "        r.assign('X',X_train)\n",
    "        #r.assign('X',train_vec_all)\n",
    "        r.assign('Y',ro.FloatVector(y_train))\n",
    "        r.assign('W.src',ro.IntVector(W_src_train))\n",
    "        r.assign('W.tar',ro.IntVector(W_tar_train))\n",
    "        r.assign('X.test_src',X_test_src)\n",
    "        r.assign('X.test_tar',X_test_tar)\n",
    "        \n",
    "        # honesty = FALSE, ci.group.size=1, sample.fraction=0.8\n",
    "        r('c.src_forest <- causal_forest(X,Y,W.src, num.trees=2000, num.threads=8, seed=set.seed(10))')\n",
    "        r('c.pred_src <- predict(c.src_forest, X.test_src, num.threads=8)')\n",
    "        \n",
    "        r('c.tar_forest <- causal_forest(X,Y,W.tar, num.trees=2000, num.threads=8, seed=set.seed(10))')\n",
    "        r('c.pred_tar <- predict(c.tar_forest, X.test_tar, num.threads=8)')\n",
    "        \n",
    "        CSF_pred_src = list(r('c.pred_src')[0])\n",
    "        #CSF_var_src = list(r('c.pred_src')[1])\n",
    "        CSF_pred_tar = list(r('c.pred_tar')[0])\n",
    "        #CSF_var_tar = list(r('c.pred_tar')[1])\n",
    "        \n",
    "        \n",
    "        j=0\n",
    "        for idx in X_src_testidx[i]:\n",
    "            src_pred[idx] = CSF_pred_src[j]\n",
    "            #src_var[idx] = CSF_var_src[j]\n",
    "            j += 1\n",
    "            \n",
    "        k=0\n",
    "        for idx in X_tar_testidx[i]:\n",
    "            tar_pred[idx] = CSF_pred_tar[k]\n",
    "            #tar_var[idx] = CSF_var_tar[k]\n",
    "            k += 1\n",
    "       \n",
    "                \n",
    "    return src_pred,tar_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_folds(X_sents,labels,divide,nfold):\n",
    "    # split X_src into 10 folds, and split X_tar into 10 folds\n",
    "    X_src = X_sents[:divide]\n",
    "    X_tar = X_sents[divide:]\n",
    "    y_src = labels[:divide]\n",
    "    y_tar = labels[divide:]\n",
    "    \n",
    "    X_src_testidx = []\n",
    "    X_src_trainidx = []\n",
    "    X_tar_testidx = []\n",
    "    X_tar_trainidx = []\n",
    "    skf = StratifiedKFold(n_splits=nfold,random_state=42)\n",
    "    for train_index, test_index in skf.split(X_src, y_src):\n",
    "        X_src_testidx.append(test_index)\n",
    "        X_src_trainidx.append(train_index)\n",
    "    \n",
    "    for train_index, test_index in skf.split(X_tar, y_tar):\n",
    "        X_tar_testidx.append(test_index)\n",
    "        X_tar_trainidx.append(train_index)\n",
    "    \n",
    "    W_asg = list(np.zeros(divide))+list(np.ones(X_sents.shape[0]-divide))\n",
    "    \n",
    "    src_pred, tar_pred = train_10CSF(X_src,y_src,W_asg[:divide],X_src_testidx,X_src_trainidx,\n",
    "                                                       X_tar,y_tar,W_asg[divide:],X_tar_testidx,X_tar_trainidx,\n",
    "                                                       nfold)\n",
    "    \n",
    "    return list(src_pred)+list(tar_pred) #, list(src_var)+list(tar_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_10CSF_treatment(wdpair_file,treat_file,log_file,vocab_file,nfold):\n",
    "    wdpairs_pos_neg_pd = pd.DataFrame(pickle.load(open(wdpair_file,'rb')))\n",
    "    \n",
    "    if(vocab_file):\n",
    "        my_vocab = list(set(pd.read_csv(vocab_file).word.values))\n",
    "        ct_vec = CountVectorizer(min_df=3,binary=True,vocabulary=my_vocab)\n",
    "        #print(len(my_vocab))\n",
    "    else:\n",
    "        ct_vec = CountVectorizer(min_df=3,binary=True)\n",
    "    \n",
    "    with open(treat_file,'wt') as fw, open(log_file,'wt') as flog:\n",
    "        my_fields = ['source','target','source_pos_sents_treatment','source_neg_sents_treatment','target_pos_sents_treatment','target_neg_sents_treatment']\n",
    "        csv_f = csv.DictWriter(fw,fieldnames=my_fields)\n",
    "        csv_f.writeheader()\n",
    "            \n",
    "        log_fields = ['swd','twd','shape']\n",
    "        log_csv = csv.DictWriter(flog,fieldnames=log_fields)\n",
    "        log_csv.writeheader()\n",
    "\n",
    "        for idx, row in wdpairs_pos_neg_pd.iterrows():\n",
    "            if(idx %100 == 0):\n",
    "                print(\"-------------\",idx,\"-------------\")\n",
    "                \n",
    "            swd = row['source']\n",
    "            twd = row['target']\n",
    "            swd_pos_sents = row['source_pos_sents']\n",
    "            swd_neg_sents = row['source_neg_sents']\n",
    "            twd_pos_sents = row['target_pos_sents']\n",
    "            twd_neg_sents = row['target_neg_sents']\n",
    "            \n",
    "            #if((len(swd_pos_sents)>nfold) and (len(swd_neg_sents)>nfold) and (len(twd_pos_sents)>nfold) and (len(twd_neg_sents)>nfold)):\n",
    "            sents = swd_pos_sents + swd_neg_sents + twd_pos_sents + twd_neg_sents\n",
    "            labels = list(np.ones(len(swd_pos_sents)))+list(np.zeros(len(swd_neg_sents)))+list(np.ones(len(twd_pos_sents)))+list(np.zeros(len(twd_neg_sents)))\n",
    "            divide = len(swd_pos_sents)+len(swd_neg_sents)\n",
    "            X_sents = ct_vec.fit_transform(sents)\n",
    "            vocab = ct_vec.vocabulary_\n",
    "            swdi = vocab[swd]\n",
    "            twdi = vocab[twd]\n",
    "\n",
    "            X_sents[:,swdi] = 0\n",
    "            X_sents[:,twdi] = 0\n",
    "\n",
    "            #print(swd,twd,X_sents.shape)\n",
    "\n",
    "            log_csv.writerow({'swd':swd, 'twd':twd, 'shape': X_sents.shape})\n",
    "\n",
    "            CSF_pred = make_folds(X_sents,labels,divide,nfold)\n",
    "\n",
    "            treat_info = CSF10_treatment_for_pair(sents,labels,divide,CSF_pred)\n",
    "\n",
    "            if(treat_info):\n",
    "                csv_f.writerow({'source':swd,'target':twd,\n",
    "                        'source_pos_sents_treatment':treat_info['source_pos_sents_treatment'],'source_neg_sents_treatment':treat_info['source_neg_sents_treatment'],\n",
    "                            'target_pos_sents_treatment':treat_info['target_pos_sents_treatment'],'target_neg_sents_treatment':treat_info['target_neg_sents_treatment']})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Only using AMT labeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------- 0 -------------\n",
      "68.06570002237956\n"
     ]
    }
   ],
   "source": [
    "prefix = \"yp\"\n",
    "pair_file = AMT_path+'AMT_WdSents/Data/'+prefix+'_AMT_wdsents_markPPN.pickle'\n",
    "treat_file = AMT_path +\"AMT_WdSents/3_CSF/\"+prefix+\"_csf_2000tree_treatment.csv\"\n",
    "log_file =  AMT_path+\"AMT_WdSents/3_CSF/\"+prefix+\"_csf_2000tree_log.csv\"\n",
    "\n",
    "#vocab_file=airbnb_path+'0_Data/common_wds.csv'\n",
    "start = time.time()\n",
    "cal_10CSF_treatment(pair_file,treat_file,log_file,vocab_file='',nfold=10)\n",
    "end = time.time()\n",
    "print((end-start)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18, 6)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_pd = pd.read_csv(AMT_path +\"AMT_WdSents/3_CSF/yp_csf_param_treatment_0.csv\",index_col=False)\n",
    "res_pd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------- 0 -------------\n",
      "36.9123366912206\n"
     ]
    }
   ],
   "source": [
    "prefix = \"tw\"\n",
    "pair_file = AMT_path+'AMT_WdSents/Data/'+prefix+'_AMT_wdsents_markPPN.pickle'\n",
    "treat_file = AMT_path +\"AMT_WdSents/3_CSF/\"+prefix+\"_csf_param_treatment.csv\"\n",
    "log_file =  AMT_path+\"AMT_WdSents/3_CSF/\"+prefix+\"_csf_param_log.csv\"\n",
    "\n",
    "#vocab_file=airbnb_path+'0_Data/common_wds.csv'\n",
    "start = time.time()\n",
    "cal_10CSF_treatment(pair_file,treat_file,log_file,vocab_file='',nfold=10)\n",
    "end = time.time()\n",
    "print((end-start)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------- 0 -------------\n",
      "99.13416692813237\n"
     ]
    }
   ],
   "source": [
    "prefix = \"airbnb\"\n",
    "pair_file = AMT_path+'AMT_WdSents/Data/'+prefix+'_AMT_wdsents_markPPN.pickle'\n",
    "treat_file = AMT_path +\"AMT_WdSents/3_CSF/\"+prefix+\"_csf_param_treatment.csv\"\n",
    "log_file =  AMT_path+\"AMT_WdSents/3_CSF/\"+prefix+\"_csf_param_log.csv\"\n",
    "\n",
    "#vocab_file=airbnb_path+'0_Data/common_wds.csv'\n",
    "start = time.time()\n",
    "cal_10CSF_treatment(pair_file,treat_file,log_file,vocab_file='',nfold=10)\n",
    "end = time.time()\n",
    "print((end-start)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------- 5 -------------\n",
      "7.753159054120382\n"
     ]
    }
   ],
   "source": [
    "this_path = airbnb_path\n",
    "prefix = \"airbnb\"\n",
    "pair_file = this_path+\"1_Process/\"+prefix+\"_wdpair_sents_limit5000.pickle\"\n",
    "treat_file = this_path +\"3_CausalForest/\"+prefix+\"_csf_200tree_treatment.csv\"\n",
    "log_file =  this_path+\"3_CausalForest/\"+prefix+\"_csf_200tree_log.csv\"\n",
    "vocab_file = this_path+'0_Data/common_wds.csv'\n",
    "start = time.time()\n",
    "cal_10CSF_treatment(pair_file,vocab_file,treat_file,log_file,nfold=10)\n",
    "end = time.time()\n",
    "print((end-start)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------- 5 -------------\n",
      "6.3959188540776575\n"
     ]
    }
   ],
   "source": [
    "this_path = tw_path\n",
    "prefix = \"tw\"\n",
    "pair_file = this_path+\"1_Process/\"+prefix+\"_wdpair_sents_limit5000.pickle\"\n",
    "treat_file = this_path +\"3_CausalForest/\"+prefix+\"_csf_200tree_treatment.csv\"\n",
    "log_file =  this_path+\"3_CausalForest/\"+prefix+\"_csf_200tree_log.csv\"\n",
    "start = time.time()\n",
    "cal_10CSF_treatment(pair_file,treat_file,log_file,nfold=10)\n",
    "end = time.time()\n",
    "print((end-start)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------- 0 -------------\n",
      "------------- 100 -------------\n",
      "------------- 200 -------------\n",
      "------------- 300 -------------\n",
      "------------- 400 -------------\n",
      "------------- 500 -------------\n",
      "------------- 600 -------------\n",
      "------------- 700 -------------\n",
      "------------- 800 -------------\n",
      "681.4869681795439\n"
     ]
    }
   ],
   "source": [
    "this_path = yp_path\n",
    "prefix = \"yp\"\n",
    "pair_file = this_path+\"1_Process/\"+prefix+\"_wdpair_sents_limit5000.pickle\"\n",
    "treat_file = this_path +\"3_CausalForest/\"+prefix+\"_csf_200tree_treatment.csv\"\n",
    "log_file =  this_path+\"3_CausalForest/\"+prefix+\"_csf_200tree_log.csv\"\n",
    "start = time.time()\n",
    "cal_10CSF_treatment(pair_file,treat_file,log_file,nfold=10)\n",
    "end = time.time()\n",
    "print((end-start)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
