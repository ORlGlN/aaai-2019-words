{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Combine effects calculated by KNN, VT, CTF, CSF into same file\n",
    "- 1.1 Check if KNN, VT, CTF, CSF are in same order for each datasetIf not in same order, then the result is wrong. If in same order, record index for each word pair. <br>\n",
    "- 1.2 Map four effects from four files into one fileProcess by tuple (one wordpair, one sentence, knn_effect, vt_effect, ctf_effect, csf_effect) <br>\n",
    "\n",
    "### 2. Limit treatment effect file according to different criteria\n",
    "- 2.1 Limit treatment effect file to a given set of word pairs\n",
    "> 2.1.1 Limit data according to vocabulary get from postag intersection > 0 (word pairs that have at least one common pos tag)<br>\n",
    "> 2.1.2 Limit data according to vocabulary get from most_common(1) postag matching (word pairs have same most common POS tag)<br>\n",
    "- 2.2 Limit treatment effect file to bigram match strategy <br>\n",
    "> 2.2.1 check if (left_word target_word) or (target_word right_word) in bigram file <br>\n",
    "> 2.2.2 Further check n_bigram > a given value <br>\n",
    "> 2.2.3 Check to see if more strict threshold will produce better reasonable sentences <br>\n",
    "- 2.3 Limit treatment effect file by sensitive case and sentence length<br>\n",
    "- 2.4 Extract word pairs from a treatment effect file. <br>\n",
    "\n",
    "### 3. Select topn word pairs and sentences\n",
    "- 3.1. Get top-n word pairs <br>\n",
    "- 3.2 For each word pair, select 3 sentences with max, min and median treatment effects according to each method.<br>\n",
    "- 3.3 For each word pair, sort all sentences, select 1+10 sentences. <br>\n",
    "\n",
    "### 4. Generate paraphrase substitution sentences for selected word pairs and sentences\n",
    "\n",
    "### 5. Different evaluation methods\n",
    "- 5.1 Rank all word pairs according to different methods. <br>\n",
    "- 5.2 Get average treatment effect for each word pair. <br>\n",
    "- 5.3 Fetch information for word pairs with coef change and frequency in opposite class. <br>\n",
    "- 5.4 Get top-10 and bottom-10 word pair. <br>\n",
    "- 5.5 Find treatment words for a sentence. <br>\n",
    "- 5.6 Spearman rank correlation for sentences and word pairs. <br>\n",
    "- 5.7 Percentage of negative instances among topn high treatment sentences. <br>\n",
    "\n",
    "### 6. Revise for previous results\n",
    "- 6.1 Assign new treatment effect to existing amt labeled airbnb files (post process)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle, re, random, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ast,time\n",
    "from collections import Counter\n",
    "import warnings\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.stats import spearmanr\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tw_path = '/data/2/zwang/2018_S_WordTreatment/V2_twitter/'\n",
    "yp_path = '/data/2/zwang/2018_S_WordTreatment/V2_yelp/'\n",
    "airbnb_path = '/data/2/zwang/2018_S_WordTreatment/V2_airbnb/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Combine effects calculated by KNN, VT, CTF, CSF into same file\n",
    "- 1.1 Check if KNN, VT, CTF, CSF are in same order for each dataset\n",
    "> If not in same order, then the result is wrong. <br>\n",
    "> If in same order, record index for each word pair. <br>\n",
    "- 1.2 Map four effects from four files into one file\n",
    "> Process by tuple (one wordpair, one sentence, knn_effect, vt_effect, ctf_effect, csf_effect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Map four effects for airbnb, yelp, twitter separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_path = airbnb_path\n",
    "prefix = 'airbnb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "knn_file = this_path+'3_KNN/'+prefix+'_knn_30_treatment_limitvocab.csv'\n",
    "vt_file = this_path+'3_VirtualTwins/'+prefix+'_vt_200tree_treatment_limitvocab.csv'\n",
    "ctf_file = this_path+'3_CounterFactual/'+prefix+'_ctf_200tree_treatment_limitvocab.csv'\n",
    "csf_file = this_path+'3_CausalForest/'+prefix+'_csf_200tree_treatment_limitvocab.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_effect_forpair(row):\n",
    "    \"\"\"\n",
    "    Parameters: \n",
    "        one word pair with all sentences.\n",
    "    Function:\n",
    "        Extract treatment effect values for all sentences of one word pair.\n",
    "        Sentence order: src_pos, src_neg, tar_pos, tar_neg. \n",
    "        A sub-function called by combine_4effects.\n",
    "    Return: \n",
    "        a list of effect values for all sentences.\n",
    "    \"\"\"\n",
    "    effect = []\n",
    "    src_pos_effects = ast.literal_eval(row['source_pos_sents_treatment'].values[0])[0]\n",
    "    for i in range(len(src_pos_effects)):\n",
    "        effect.append(src_pos_effects[i][1])\n",
    "\n",
    "    src_neg_effects = ast.literal_eval(row['source_neg_sents_treatment'].values[0])[0]\n",
    "    for j in range(len(src_neg_effects)):\n",
    "        effect.append(src_neg_effects[j][1])\n",
    "        \n",
    "    tar_pos_effects = ast.literal_eval(row['target_pos_sents_treatment'].values[0])[0]\n",
    "    for m in range(len(tar_pos_effects)):\n",
    "        effect.append(tar_pos_effects[m][1])\n",
    "        \n",
    "    tar_neg_effects = ast.literal_eval(row['target_neg_sents_treatment'].values[0])[0]\n",
    "    for n in range(len(tar_neg_effects)):\n",
    "        effect.append(tar_neg_effects[n][1])\n",
    "        \n",
    "    return effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def combine_4effects(knn_file,vt_file,ctf_file,csf_file,all_effect_file):\n",
    "    \"\"\"\n",
    "    Parameters: \n",
    "        4 treatment effect files and one result file to store all treatment effects\n",
    "    Function:\n",
    "        Iterate over all word pairs:\n",
    "            Combine 4 effects for every sentence and make a dict for every sentence\n",
    "        Concatenate all dicts and pickle to file        \n",
    "    Return: \n",
    "        A pickle file with following fields:\n",
    "        ['id','source','target','sentence','knn_effect','vt_effect','ctf_effect','csf_effect','true_y']\n",
    "    \"\"\"\n",
    "    knn_pd = pd.read_csv(knn_file)\n",
    "    vt_pd = pd.read_csv(vt_file)\n",
    "    ctf_pd = pd.read_csv(ctf_file)\n",
    "    csf_pd = pd.read_csv(csf_file)\n",
    "    all_sents_effect = []\n",
    "    g_idx = -1\n",
    "    for idx,row in knn_pd.iterrows():\n",
    "        if(idx % 100 == 0):\n",
    "            print(idx)\n",
    "        vt_row = vt_pd[(vt_pd.source==row.source) & (vt_pd.target==row.target)]\n",
    "        vt_effect = get_effect_forpair(vt_row)\n",
    "        ctf_row = ctf_pd[(ctf_pd.source==row.source) & (ctf_pd.target==row.target)]\n",
    "        ctf_effect = get_effect_forpair(ctf_row)\n",
    "        csf_row = csf_pd[(csf_pd.source==row.source) & (csf_pd.target==row.target)]\n",
    "        csf_effect = get_effect_forpair(csf_row)\n",
    "        \n",
    "        local_idx = -1\n",
    "        knn_src_pos_effects = ast.literal_eval(row['source_pos_sents_treatment'])[0]\n",
    "        for i in range(len(knn_src_pos_effects)):\n",
    "            g_idx += 1\n",
    "            local_idx += 1\n",
    "            all_sents_effect.append({'id':g_idx,'source':row.source,'target':row.target,'sentence':knn_src_pos_effects[i][0],\n",
    "                                     'knn_effect':knn_src_pos_effects[i][1],'vt_effect':vt_effect[local_idx],\n",
    "                                     'ctf_effect':ctf_effect[local_idx],'csf_effect':csf_effect[local_idx],'true_y':1})\n",
    "        \n",
    "        knn_src_neg_effects = ast.literal_eval(row['source_neg_sents_treatment'])[0]\n",
    "        for j in range(len(knn_src_neg_effects)):\n",
    "            g_idx += 1\n",
    "            local_idx += 1\n",
    "            all_sents_effect.append({'id':g_idx,'source':row.source,'target':row.target,'sentence':knn_src_neg_effects[j][0],\n",
    "                                     'knn_effect':knn_src_neg_effects[j][1],'vt_effect':vt_effect[local_idx],\n",
    "                                     'ctf_effect':ctf_effect[local_idx],'csf_effect':csf_effect[local_idx],'true_y':0})\n",
    "        \n",
    "        knn_tar_pos_effects = ast.literal_eval(row['target_pos_sents_treatment'])[0]\n",
    "        for m in range(len(knn_tar_pos_effects)):\n",
    "            g_idx += 1\n",
    "            local_idx += 1\n",
    "            all_sents_effect.append({'id':g_idx,'source':row.target,'target':row.source,'sentence':knn_tar_pos_effects[m][0],\n",
    "                                     'knn_effect':knn_tar_pos_effects[m][1],'vt_effect':vt_effect[local_idx],\n",
    "                                     'ctf_effect':ctf_effect[local_idx],'csf_effect':csf_effect[local_idx],'true_y':1})\n",
    "        \n",
    "        knn_tar_neg_effects = ast.literal_eval(row['target_neg_sents_treatment'])[0]\n",
    "        for n in range(len(knn_tar_neg_effects)):\n",
    "            g_idx += 1\n",
    "            local_idx += 1\n",
    "            all_sents_effect.append({'id':g_idx,'source':row.target,'target':row.source,'sentence':knn_tar_neg_effects[n][0],\n",
    "                                     'knn_effect':knn_tar_neg_effects[n][1],'vt_effect':vt_effect[local_idx],\n",
    "                                     'ctf_effect':ctf_effect[local_idx],'csf_effect':csf_effect[local_idx],'true_y':0})\n",
    "    \n",
    "    pickle.dump(pd.DataFrame(all_sents_effect), open(all_effect_file,'wb'))     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n"
     ]
    }
   ],
   "source": [
    "all_effect_file = this_path+'5_Select/'+prefix+'_wdpair_sents_4effects_limitvocab.pickle'\n",
    "combine_4effects(knn_file,vt_file,ctf_file,csf_file,all_effect_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Check if the combination is correct\n",
    "- Double chek if the function matched all 4 effects to sentences correctly\n",
    "- Randomly pick a sentence, check the effects in combined file and 4 separate effect files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1562060, 9)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pd = pickle.load(open(airbnb_path+'5_Select/airbnb_wdpair_sents_4effects_limitvocab.pickle','rb'))\n",
    "test_pd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "csf_effect                                0.18227\n",
       "ctf_effect                                0.20484\n",
       "id                                         152163\n",
       "knn_effect                                   -0.1\n",
       "sentence      As a predominantly Catholic port ci\n",
       "source                              predominantly\n",
       "target                                     mostly\n",
       "true_y                                          1\n",
       "vt_effect                                 0.02126\n",
       "Name: 152163, dtype: object"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mys = \"As a predominantly Catholic port city, New Orleans provided community and job security for Irish immigrants fleeing famine in the early to mid 1800s.\"\n",
    "test_pd[(test_pd.source == 'predominantly') & (test_pd.target == 'mostly')].ix[152163]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2. Limit treatment effect file according to different criteria\n",
    "- 2.1 Limit treatment effect file to a given set of word pairs <br>\n",
    "> 2.1.1 Limit data according to vocabulary get from postag intersection > 0 (word pairs that have at least one common pos tag)<br>\n",
    "> 2.1.2 Limit data according to vocabulary get from most_common(1) postag matching (word pairs have same most common POS tag)<br>\n",
    "\n",
    "- 2.2 Limit treatment effect file to bigram match strategy <br>\n",
    "> 2.2.1 check if (left_word target_word) or (target_word right_word) in bigram file <br>\n",
    "> 2.2.2 Further check n_bigram > a given value <br>\n",
    "> 2.2.3 Check to see if more strict threshold will produce better reasonable sentences <br>\n",
    "\n",
    "- 2.3 Limit treatment effect file by sensitive case and sentence length<br>\n",
    "- 2.4 Extract word pairs from a treatment effect file. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Limit treatment effect file to a given set of word pairs\n",
    "- Used when we only care about treatment effects for a specific set of words instead of all word pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def limit_by_wdpair(wdpair_file,effect_file,result_file):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        File for specific word pairs, the whole effect file, effect file for limited word pairs\n",
    "    Function:\n",
    "        For the set of specific word pairs:\n",
    "            Map to find all sentences with treatment effects, bidirectional (source,target), (target,source)\n",
    "    Return:\n",
    "        Pickle file for specific word pairs with following fields:\n",
    "        ['source','target','src_sentence','tar_sentence','knn_effect','vt_effect','ctf_effect','csf_effect',\n",
    "        'true_y','id','src_ratings','src_ratings_avg','tar_ratings','tar_ratings_avg','amt_effect']\n",
    "        \n",
    "    \"\"\"\n",
    "    effect_pd = pd.DataFrame(pickle.load(open(effect_file,'rb')))\n",
    "    wdpair_pd = pd.read_csv(wdpair_file)\n",
    "    remain_id = []\n",
    "    for idx,row in wdpair_pd.iterrows():\n",
    "        if(idx % 100 == 0):\n",
    "            print(idx)\n",
    "        remain_id.extend(effect_pd[(effect_pd.source == row.source) & (effect_pd.target == row.target)].id.values)\n",
    "        remain_id.extend(effect_pd[(effect_pd.source == row.target) & (effect_pd.target == row.source)].id.values)\n",
    "        \n",
    "    remain_effect_pd = effect_pd[effect_pd['id'].isin(remain_id)]\n",
    "    pickle.dump(remain_effect_pd,open(result_file,'wb'))\n",
    "    print(\"%d word pairs with %d sentences remained\" % (wdpair_pd.shape[0],remain_effect_pd.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 2.1.1 Limit data according to vocabulary get from postag intersection > 0 (word pairs that have at least one common POS tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "750 word pairs with 1377527 sentences remained\n"
     ]
    }
   ],
   "source": [
    "wdpair_file = airbnb_path+'1_Process/airbnb_treat_pairs_posinters.csv'\n",
    "effect_file = airbnb_path+'5_Select/airbnb_wdpair_sents_4effects_limitvocab.pickle'\n",
    "result_file = airbnb_path+'5_Select/airbnb_wdpair_sents_4effects_posinters_limitvocab.pickle'\n",
    "limit_by_wdpair(wdpair_file,effect_file,result_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 2.1.2 Limit data according to vocabulary get from most_common(1) postag matching (word pairs have same most common POS tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "561 word pairs with 1000502 sentences remained\n"
     ]
    }
   ],
   "source": [
    "wdpair_file = airbnb_path+'1_Process/airbnb_treat_pairs_posinters_poscom1.csv'\n",
    "effect_file = airbnb_path+'5_Select/airbnb_wdpair_sents_4effects_posinters_limitvocab.pickle'\n",
    "result_file = airbnb_path+'5_Select/airbnb_wdpair_sents_4effects_posinters_poscom1_limitvocab.pickle'\n",
    "limit_by_wdpair(wdpair_file,effect_file,result_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 2.2 Limit treatment effect file to bigram match strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 2.2.1 check if (left_word target_word) or (target_word right_word) in bigram file <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wdpair_sentence_file = airbnb_path+'5_Select/airbnb_wdpair_sents_4effects_posinters_poscom1.pickle'\n",
    "wdpair_effect_pd = pd.DataFrame(pickle.load(open(wdpair_sentence_file,'rb')))\n",
    "result_file = yp_path+'5_Select/yp_wdpair_sents_4effects_posinters_poscom1_bigramcheck.pickle'\n",
    "big_effect_pd = pd.DataFrame(pickle.load(open(result_file,'rb')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "st_pd = wdpair_effect_pd.sort(columns=['knn_effect'],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#st_pd.iloc[:10].sentence.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>csf_effect</th>\n",
       "      <th>ctf_effect</th>\n",
       "      <th>id</th>\n",
       "      <th>knn_effect</th>\n",
       "      <th>sentence</th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>true_y</th>\n",
       "      <th>vt_effect</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1650086</th>\n",
       "      <td>0.70315</td>\n",
       "      <td>0.68375</td>\n",
       "      <td>1650086</td>\n",
       "      <td>0.96667</td>\n",
       "      <td>Long story short , my boyfriend loved and was ...</td>\n",
       "      <td>boyfriend</td>\n",
       "      <td>buddy</td>\n",
       "      <td>1</td>\n",
       "      <td>0.33178</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         csf_effect  ctf_effect       id  knn_effect  \\\n",
       "1650086     0.70315     0.68375  1650086     0.96667   \n",
       "\n",
       "                                                  sentence     source target  \\\n",
       "1650086  Long story short , my boyfriend loved and was ...  boyfriend  buddy   \n",
       "\n",
       "         true_y  vt_effect  \n",
       "1650086       1    0.33178  "
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mystr = \"Long story short , my boyfriend loved and was impressed with the chocolate .\"\n",
    "#wdpair_effect_pd[(wdpair_effect_pd.source=='boyfriend')]\n",
    "wdpair_effect_pd[(wdpair_effect_pd.source=='boyfriend') & (wdpair_effect_pd.target=='buddy') & ((wdpair_effect_pd.sentence==mystr))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>csf_effect</th>\n",
       "      <th>ctf_effect</th>\n",
       "      <th>id</th>\n",
       "      <th>knn_effect</th>\n",
       "      <th>sentence</th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>true_y</th>\n",
       "      <th>vt_effect</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [csf_effect, ctf_effect, id, knn_effect, sentence, source, target, true_y, vt_effect]\n",
       "Index: []"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_effect_pd[(big_effect_pd.source=='boyfriend') & (big_effect_pd.target=='buddy')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_bigram_match(effect_file,wdpair_file,vocab_bigram_file,result_file):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        Whole effect file, a specific set of word pairs, bigram vocabulary file, word pair and sentences passed bigram match\n",
    "    Function:\n",
    "        Iterate over each word pair, and each sentence:\n",
    "            check if the bigram exists after target word substitution:\n",
    "            left_word control_word right_word --> left_word treatment_word right_word\n",
    "            Only remain sentences that either (left_word treatment_word) or (treatment_word right_word) exist in treatment_word's history bigrams.\n",
    "        return remaining word pairs, number of remaining sentences, and write sentences for each word pairs.\n",
    "    Return:\n",
    "        Pickle file that stores word pair and sentences passed bigram match\n",
    "    \"\"\"\n",
    "    \n",
    "    wdpair_effect_pd = pd.DataFrame(pickle.load(open(effect_file,'rb')))\n",
    "    wdpair_pd = pd.read_csv(wdpair_file)\n",
    "    vocab_bigram_dfdict = pickle.load(open(vocab_bigram_file,'rb'))\n",
    "    remain_id = []\n",
    "    \n",
    "    for idx,row in wdpair_effect_pd.iterrows():\n",
    "        if(idx % 100000 ==0):\n",
    "            print(idx)\n",
    "        # check if this word pair still exists in the limited vocabulary, this is optional\n",
    "        src_tar_pd = wdpair_pd[(wdpair_pd.source==row.source) & (wdpair_pd.target==row.target)]\n",
    "        tar_src_pd = wdpair_pd[(wdpair_pd.source==row.target) & (wdpair_pd.target==row.source)]\n",
    "        if((src_tar_pd.shape[0]>0) or (tar_src_pd.shape[0]>0)):\n",
    "            word_list = re.findall('[a-z]+',row.sentence.lower())\n",
    "            if(row.source in word_list):\n",
    "                srci = word_list.index(row.source)\n",
    "                tar_bigram_l = ''\n",
    "                tar_bigram_r = ''\n",
    "                if(srci > 0):\n",
    "                    tar_bigram_l = word_list[srci-1]+' '+row.target\n",
    "                if(srci < len(word_list)-1):\n",
    "                    tar_bigram_r = row.target+' '+word_list[srci+1]\n",
    "\n",
    "                flag = False\n",
    "                if(tar_bigram_l in vocab_bigram_dfdict[row.target]):\n",
    "                    flag = True\n",
    "                elif(tar_bigram_r in vocab_bigram_dfdict[row.target]):\n",
    "                    flag = True\n",
    "\n",
    "                if(flag):\n",
    "                    remain_id.append(row.id)\n",
    "\n",
    "    remain_effect_pd = wdpair_effect_pd[wdpair_effect_pd['id'].isin(remain_id)]\n",
    "    pickle.dump(remain_effect_pd,open(result_file,'wb'))\n",
    "    print(\"%d word pairs remained out of %d in total\" % (len(remain_effect_pd.groupby([\"source\", \"target\"]).size()),wdpair_pd.shape[0]))\n",
    "    print(\"%d sentences remained out of %d in total\" % (len(remain_id),wdpair_effect_pd.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "this_path = airbnb_path\n",
    "prefix = \"airbnb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_bigram_file = '/data/2/zwang/2018_S_WordTreatment/V2_airbnb/vocab_bigram_dfdict.pickle'\n",
    "wdpair_file = this_path + '1_Process/'+prefix+'_treat_pairs_posinters_poscom1.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "900000\n",
      "1500000\n",
      "1108 word pairs remained out of 561 in total\n",
      "885786 sentences remained out of 1000502 in total\n"
     ]
    }
   ],
   "source": [
    "wdpair_sentence_file = this_path+'5_Select/'+prefix+'_wdpair_sents_4effects_posinters_poscom1_limitvocab.pickle'\n",
    "result_file = this_path+'5_Select/'+prefix+'_wdpair_sents_4effects_posinters_poscom1_bigramcheck_limitvocab.pickle'\n",
    "check_bigram_match(wdpair_sentence_file,wdpair_file,vocab_bigram_file,result_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 2.2.2 Further check n_bigram > a given value \n",
    "> check to see if more strict threshold will produce better reasonable sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_nbigram(effect_file,vocab_bigram_file,result_file,n_min):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        Whole effect file, bigram vocabulary file, word pair and sentences passed bigram match, threshold for n_bigram\n",
    "    Function:\n",
    "        Iterate over each word pair, and each sentence:\n",
    "            check if the bigram exists after target word substitution:\n",
    "            left_word control_word right_word --> left_word treatment_word right_word\n",
    "            Only remain sentences that either n_(left_word treatment_word) or n_(treatment_word right_word) > n_min in treatment_word bigrams.\n",
    "        return remaining word pairs, number of remaining sentences, and write sentences for each word pairs.\n",
    "    Return:\n",
    "        Pickle file that stores word pair and sentences passed bigram match with given threshold\n",
    "    \"\"\"\n",
    "    \n",
    "    wdpair_effect_pd = pd.DataFrame(pickle.load(open(effect_file,'rb')))\n",
    "    vocab_bigram_dfdict = pickle.load(open(vocab_bigram_file,'rb'))\n",
    "    remain_id = []\n",
    "    \n",
    "    for idx,row in wdpair_effect_pd.iterrows():\n",
    "        if(idx % 100000 ==0):\n",
    "            print(idx)\n",
    "\n",
    "        word_list = re.findall('[a-z]+',row.sentence.lower())\n",
    "        if(row.source in word_list):\n",
    "            srci = word_list.index(row.source)\n",
    "            tar_bigram_l = ''\n",
    "            tar_bigram_r = ''\n",
    "            if(srci > 0):\n",
    "                tar_bigram_l = word_list[srci-1]+' '+row.target\n",
    "            if(srci < len(word_list)-1):\n",
    "                tar_bigram_r = row.target+' '+word_list[srci+1]\n",
    "\n",
    "            flag = False\n",
    "            if(vocab_bigram_dfdict[row.target][tar_bigram_l] > n_min):\n",
    "                flag = True\n",
    "            elif(vocab_bigram_dfdict[row.target][tar_bigram_r] > n_min):\n",
    "                flag = True\n",
    "\n",
    "            if(flag):\n",
    "                remain_id.append(row.id)\n",
    "\n",
    "    remain_effect_pd = wdpair_effect_pd[wdpair_effect_pd['id'].isin(remain_id)]\n",
    "    pickle.dump(remain_effect_pd,open(result_file,'wb'))\n",
    "    print(\"%d word pairs remained in total\" % (len(remain_effect_pd.groupby([\"source\", \"target\"]).size())))\n",
    "    print(\"%d sentences remained out of %d in total\" % (len(remain_id),wdpair_effect_pd.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "200000\n",
      "700000\n",
      "900000\n",
      "1100000\n",
      "1400000\n",
      "1500000\n",
      "1600000\n",
      "1800000\n",
      "1900000\n",
      "2000000\n",
      "1020 word pairs remained in total\n",
      "799363 sentences remained out of 907939 in total\n"
     ]
    }
   ],
   "source": [
    "vocab_bigram_file = '/data/2/zwang/2018_S_WordTreatment/V2_airbnb/vocab_bigram_dfdict.pickle'\n",
    "wdpair_sentence_file = tw_path+'5_Select/tw_wdpair_sents_4effects_posinters_poscom1_bigramcheck_limit.pickle'\n",
    "result_file = tw_path+'5_Select/tw_wdpair_sents_4effects_posinters_poscom1_bigramcheck_limit_bicheck3.pickle'\n",
    "check_nbigram(wdpair_sentence_file,vocab_bigram_file,result_file,n_min=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 2.2.3 Check to see if more strict threshold will produce better reasonable sentences \n",
    "> Check if both n_left_bigram > n_thresh and n_right_bigram > n_thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_nbigram_lr(effect_file,vocab_bigram_file,result_file,n_min):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        Whole effect file, bigram vocabulary file, word pair and sentences passed bigram match, threshold for n_bigram\n",
    "    Function:\n",
    "        Iterate over each word pair, and each sentence:\n",
    "            check if the n_bigram > n_min after target word substitution:\n",
    "            left_word control_word right_word --> left_word treatment_word right_word\n",
    "            Only remain sentences that both n_(left_word treatment_word) and n_(treatment_word right_word) > n_min.\n",
    "        return remaining word pairs, number of remaining sentences, and write sentences for each word pairs.\n",
    "    Return:\n",
    "        Pickle file that stores word pair and sentences passed bigram match with given threshold.\n",
    "    \"\"\"\n",
    "    \n",
    "    wdpair_effect_pd = pd.DataFrame(pickle.load(open(effect_file,'rb')))\n",
    "    vocab_bigram_dfdict = pickle.load(open(vocab_bigram_file,'rb'))\n",
    "    remain_id = []\n",
    "    \n",
    "    for idx,row in wdpair_effect_pd.iterrows():\n",
    "        if(idx % 100000 ==0):\n",
    "            print(idx)\n",
    "\n",
    "        word_list = re.findall('[a-z]+',row.sentence.lower())\n",
    "        if(row.source in word_list):\n",
    "            srci = word_list.index(row.source)\n",
    "            tar_bigram_l = ''\n",
    "            tar_bigram_r = ''\n",
    "            if(srci > 0):\n",
    "                tar_bigram_l = word_list[srci-1]+' '+row.target\n",
    "            if(srci < len(word_list)-1):\n",
    "                tar_bigram_r = row.target+' '+word_list[srci+1]\n",
    "\n",
    "            flag = False\n",
    "            if((vocab_bigram_dfdict[row.target][tar_bigram_l] > n_min) and (vocab_bigram_dfdict[row.target][tar_bigram_r] > n_min)):\n",
    "                flag = True\n",
    "            \n",
    "            if(flag):\n",
    "                remain_id.append(row.id)\n",
    "\n",
    "    remain_effect_pd = wdpair_effect_pd[wdpair_effect_pd['id'].isin(remain_id)]\n",
    "    pickle.dump(remain_effect_pd,open(result_file,'wb'))\n",
    "    print(\"%d word pairs remained in total\" % (len(remain_effect_pd.groupby([\"source\", \"target\"]).size())))\n",
    "    print(\"%d sentences remained out of %d in total\" % (len(remain_id),wdpair_effect_pd.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_bigram_file = '/data/2/zwang/2018_S_WordTreatment/V2_airbnb/vocab_bigram_dfdict.pickle'\n",
    "wdpair_sentence_file = tw_path+'5_Select/tw_wdpair_sents_4effects_posinters_poscom1_bigramcheck_limit.pickle'\n",
    "result_file = tw_path+'5_Select/tw_wdpair_sents_4effects_posinters_poscom1_bigramcheck_limit_bicheck3_lr.pickle'\n",
    "#check_bigram2_lr(wdpair_sentence_file,vocab_bigram_file,result_file,n_min=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(293956, 9)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_pd = pd.DataFrame(pickle.load(open(result_file,'rb')))\n",
    "res_pd.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Limit treatment effect file by sensitive case and sentence length\n",
    "> If no match for source word lowercase, then remove. (Not consider if treat word is a proper noun). <br>\n",
    "> Limit sentence length to [3,50], to remove very long sentences (might be caused by wrong pre-processing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'unique' in 'We love going to all the Unique restaurants and bars in the area.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len('We love going to all the Unique restaurants and bars in the area.'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def limit_by_condition(effect_file,result_file,min_len,max_len):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        Whole effect file, file limited by conditions, limitations on sentence length.\n",
    "    Function:\n",
    "        Put limitation on sentence length and sensitive case.\n",
    "    Return:\n",
    "        A pickle file for instances passed the limitations.\n",
    "    \"\"\"\n",
    "    effect_pd = pd.DataFrame(pickle.load(open(effect_file,'rb')))\n",
    "    remain_id = []\n",
    "    for idx,row in effect_pd.iterrows():\n",
    "        sent_len = len(row.sentence.split())\n",
    "        if((sent_len>min_len) and (sent_len<max_len)): # limitation on sentence length\n",
    "            if((' '+row.source in row.sentence) or (row.source+' ' in row.sentence)): # limitation by case sensitive\n",
    "                remain_id.append(row.id)\n",
    "    \n",
    "    remain_effect_pd = effect_pd[effect_pd['id'].isin(remain_id)]\n",
    "    pickle.dump(remain_effect_pd,open(result_file,'wb'))\n",
    "    print(\"%d sentences out of %d in total remained.\" % (len(remain_id),effect_pd.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "723297 sentences out of 885786 in total remained.\n"
     ]
    }
   ],
   "source": [
    "effect_file = airbnb_path+'5_Select/airbnb_wdpair_sents_4effects_posinters_poscom1_bigramcheck_limitvocab.pickle'\n",
    "result_file = airbnb_path+'5_Select/airbnb_wdpair_sents_4effects_posinters_poscom1_bigramcheck_limit_limitvocab.pickle'\n",
    "limit_by_condition(effect_file,result_file,min_len=3,max_len=50)\n",
    "#723297 sentences out of 885786 in total remained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1490969 sentences out of 1659198 in total remained.\n"
     ]
    }
   ],
   "source": [
    "effect_file = yp_path+'5_Select/yp_wdpair_sents_4effects_posinters_poscom1_bigramcheck.pickle'\n",
    "result_file = yp_path+'5_Select/yp_wdpair_sents_4effects_posinters_poscom1_bigramcheck_limit.pickle'\n",
    "limit_by_condition(effect_file,result_file,min_len=3,max_len=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "907939 sentences out of 1069312 in total remained.\n"
     ]
    }
   ],
   "source": [
    "effect_file = tw_path+'5_Select/tw_wdpair_sents_4effects_posinters_poscom1_bigramcheck.pickle'\n",
    "result_file = tw_path+'5_Select/tw_wdpair_sents_4effects_posinters_poscom1_bigramcheck_limit.pickle'\n",
    "limit_by_condition(effect_file,result_file,min_len=3,max_len=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 2.4 Extract word pairs from a treatment effect file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_vocab(effect_file,vocab_file):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        A given effect file, file for extracted word pairs\n",
    "    Function:\n",
    "        Extract word pairs from a given effect file\n",
    "    Return:\n",
    "        A csv file with fields ['source', 'target']\n",
    "    \"\"\"\n",
    "    \n",
    "    res_pd = pd.DataFrame(pickle.load(open(effect_file,'rb')))\n",
    "    new_vocab = []\n",
    "    for idx,row in res_pd.iterrows():\n",
    "        src_tar_dict = {}\n",
    "        src_tar_dict['source'] = row['source']\n",
    "        src_tar_dict['target'] = row['target']\n",
    "        if(src_tar_dict not in new_vocab):\n",
    "            new_vocab.append(src_tar_dict)\n",
    "    \n",
    "    pd.DataFrame(new_vocab).to_csv(vocab_file,columns=['source','target'],index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "write_vocab(effect_file = airbnb_path + '5_Select/airbnb_wdpair_sents_4effects_posinters_poscom1_bigramcheck.pickle',\n",
    "            vocab_file = airbnb_path + '1_Process/airbnb_treat_pairs_posinters_poscom1_bigramcheck.csv')\n",
    "write_vocab(effect_file = yp_path + '5_Select/yp_wdpair_sents_4effects_posinters_poscom1_bigramcheck.pickle',\n",
    "            vocab_file = yp_path + '1_Process/yp_treat_pairs_posinters_poscom1_bigramcheck.csv')\n",
    "write_vocab(effect_file = tw_path + '5_Select/tw_wdpair_sents_4effects_posinters_poscom1_bigramcheck.pickle',\n",
    "            vocab_file = tw_path + '1_Process/tw_treat_pairs_posinters_poscom1_bigramcheck.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Select topn word pairs and sentences\n",
    "- 3.1 Get top-n word pairs<br>\n",
    "- 3.2 For each word pair, select 3 sentences with max, min and median treatment effects according to each method.<br>\n",
    "- 3.3 For each word pair, sort all sentences, select 1+10 sentences. <br>\n",
    "> top1 sentence <br>\n",
    "> Divide others into 10 effect score levels, for each level, select one sentence <br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "effect_file = airbnb_path+'5_Select/airbnb_wdpair_sents_4effects_posinters_poscom1_bigramcheck_limit.pickle'\n",
    "st_effect_pd = pd.DataFrame(pickle.load(open(effect_file,'rb'))).sort(columns=['vt_effect'],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "csf_effect                                              0.06497\n",
       "ctf_effect                                              0.27129\n",
       "id                                                       295558\n",
       "knn_effect                                              0.06667\n",
       "sentence      The neighborhood is amazing as righ next to th...\n",
       "source                                                  amazing\n",
       "target                                                 fabulous\n",
       "true_y                                                        0\n",
       "vt_effect                                                 0.024\n",
       "Name: 295558, dtype: object"
      ]
     },
     "execution_count": 443,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st_effect_pd[(st_effect_pd.source == 'amazing') & (st_effect_pd.target == 'fabulous')].sort(columns=['ctf_effect'],ascending=False).iloc[9] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 3.1 Get top-n word pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_topn_wdpairs(effect_file,n_pairs,min_sent_length,method):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        Treatment effect file, number of word pairs, condition on sentence length, according to which treatment method\n",
    "    Function:\n",
    "        Sort treatment effect file according to a specific method in descending order, and \n",
    "    Return:\n",
    "        A list of word pairs: source,target\n",
    "    \"\"\"\n",
    "    # Sort by one effect\n",
    "    st_effect_pd = pd.DataFrame(pickle.load(open(effect_file,'rb'))).sort(columns=[method],ascending=False)\n",
    "    top_pairs = []\n",
    "    for idx,row in st_effect_pd.iterrows():\n",
    "        if(len(top_pairs) < n_pairs):\n",
    "            if(len(re.findall('\\w+',row.sentence)) > min_sent_length):\n",
    "                if(row.source+','+row.target not in top_pairs):\n",
    "                    top_pairs.append(row.source+','+row.target)\n",
    "        else:\n",
    "            break\n",
    "    return top_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nice,gorgeous',\n",
       " 'diverse,many',\n",
       " 'famous,wonderful',\n",
       " 'apartment,condo',\n",
       " 'see,enjoy',\n",
       " 'arts,artists',\n",
       " 'chinese,famous',\n",
       " 'stores,shops',\n",
       " 'end,rest',\n",
       " 'district,home',\n",
       " 'stores,boutiques',\n",
       " 'great,gorgeous',\n",
       " 'diverse,several',\n",
       " 'famous,old',\n",
       " 'stores,restaurants',\n",
       " 'chinese,traditional',\n",
       " 'good,gorgeous',\n",
       " 'plaza,place',\n",
       " 'plaza,square',\n",
       " 'boulevard,avenue']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "effect_file = airbnb_path+'5_Select/airbnb_wdpair_sents_4effects_posinters_poscom1_bigramcheck_limit_limitvocab.pickle'\n",
    "select_topn_wdpairs(effect_file,n_pairs=20,min_sent_length=0,method='vt_effect')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 3.2 For each word pair, select 3 sentences with max, min and median treatment effects according to each method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def select_median_for_wdpairs(effect_file,wd_pairs,wd_pairs_file,topn_file,method):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        Treatment effect file, a list of topn word pairs, file to record word pair info, \n",
    "        file to record selected sentences for word pairs, the method which selection is based on.\n",
    "    Function:\n",
    "        For each given word pair, sort all sentences by treatment effect from a method, and select 3 with:\n",
    "        max, min and median treatment effect.\n",
    "    Return:\n",
    "        A csv file with selected sentences for each word pair.\n",
    "    \"\"\"\n",
    "    st_effect_pd = pd.DataFrame(pickle.load(open(effect_file,'rb'))).sort(columns=[method],ascending=False)\n",
    "    wdpair_sents = []\n",
    "    wdpair_info = []\n",
    "    for pair in wd_pairs:\n",
    "        wdpair_pd = st_effect_pd[(st_effect_pd.source == pair.split(',')[0]) & (st_effect_pd.target == pair.split(',')[1])].sort(columns=[method],ascending=False)\n",
    "        effect_list = wdpair_pd[method].values\n",
    "        wdpair_info.append({'source':pair.split(',')[0],'target':pair.split(',')[1],\n",
    "                            'min':min(effect_list),'max':max(effect_list),'n_sents':wdpair_pd.shape[0]})\n",
    "        \n",
    "        for i in [0,-1,int(wdpair_pd.shape[0]/2)]:\n",
    "            row = wdpair_pd.iloc[i]\n",
    "            wdpair_sents.append({'source':row.source,'target':row.target,\n",
    "                             'sentence':row.sentence,'knn_effect':row.knn_effect,'vt_effect':row.vt_effect,\n",
    "                             'ctf_effect':row.ctf_effect,'csf_effect':row.csf_effect,\n",
    "                             'true_y':row.true_y,'id':row.id})\n",
    "    \n",
    "    pd.DataFrame(wdpair_info).to_csv(wd_pairs_file,columns=['source','target','n_sents','min','max'],index=False)\n",
    "    pd.DataFrame(wdpair_sents).to_csv(topn_file,columns=['source','target','sentence','knn_effect','vt_effect','ctf_effect','csf_effect','true_y','id'],index=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "effect_file = airbnb_path+'5_Select/airbnb_wdpair_sents_4effects_posinters_poscom1_bigramcheck_limit_limitvocab.pickle'\n",
    "top_pairs = select_topn_wdpairs(effect_file,n_pairs=20,min_sent_length=0,method='knn_effect')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = airbnb_path+'5_Select/5_tolabel/'\n",
    "select_median_for_wdpairs(effect_file,top_pairs,\n",
    "                          wd_pairs_file = file_path + 'airbnb_knn_20wdpairs_limitvocab.csv',\n",
    "                          topn_file = file_path + 'airbnb_knn_20wdpairs_60sents_limitvocab.csv',\n",
    "                          method = 'knn_effect')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>csf_effect</th>\n",
       "      <th>ctf_effect</th>\n",
       "      <th>id</th>\n",
       "      <th>knn_effect</th>\n",
       "      <th>sentence</th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>true_y</th>\n",
       "      <th>vt_effect</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1477321</th>\n",
       "      <td>0.07856</td>\n",
       "      <td>-0.0129</td>\n",
       "      <td>1477321</td>\n",
       "      <td>-0.36667</td>\n",
       "      <td>The Mansion is situated mere steps from the hu...</td>\n",
       "      <td>famous</td>\n",
       "      <td>wonderful</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00494</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         csf_effect  ctf_effect       id  knn_effect  \\\n",
       "1477321     0.07856     -0.0129  1477321    -0.36667   \n",
       "\n",
       "                                                  sentence  source     target  \\\n",
       "1477321  The Mansion is situated mere steps from the hu...  famous  wonderful   \n",
       "\n",
       "         true_y  vt_effect  \n",
       "1477321       1    0.00494  "
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wdpair_pd.tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "csf_effect                                              0.07856\n",
       "ctf_effect                                              -0.0129\n",
       "id                                                      1477321\n",
       "knn_effect                                             -0.36667\n",
       "sentence      The Mansion is situated mere steps from the hu...\n",
       "source                                                   famous\n",
       "target                                                wonderful\n",
       "true_y                                                        1\n",
       "vt_effect                                               0.00494\n",
       "Name: 1477321, dtype: object"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wdpair_pd.iloc[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- 3.3 For each selected word pair, sort all sentences, select 10+1 sentences. <br>\n",
    "> Top1 sentence <br>\n",
    "> Divide others into 10 effect score levels, for each level, select one sentence <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "select_nsents_for_wdpairs(effect_file,top_pairs,wd_pairs_file = ,\n",
    "                          topn_file = airbnb_path+file_path+'airbnb_csf_top_20pair_11sents_poscom1_bigram_limit_limitvocab.csv',\n",
    "                          method = 'csf_effect')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def select_nsents_for_wdpairs(effect_file,wd_pairs,n_sents,min_sent_length,topn_file,method):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        Treatment effect file, a list of topn word pairs, number of sentences to select for each word pair,\n",
    "        limitation on sentence length, file to record selected sentences for word pairs, the method which selection is based on.\n",
    "    Function:\n",
    "        For each given word pair, sort all sentences by treatment effect from a method, and select 11 with:\n",
    "        1 max treatment effect + 10 sentences from 10 deciles of treatment effect.\n",
    "    Return:\n",
    "        A csv file with word pairs and 11 sentences for each pair.\n",
    "    \"\"\"\n",
    "    st_effect_pd = pd.DataFrame(pickle.load(open(effect_file,'rb'))).sort(columns=[method],ascending=False)\n",
    "    wdpair_sents = []\n",
    "    tmp_rdidx = {}\n",
    "    for pair in wd_pairs:\n",
    "        wdpair_pd = st_effect_pd[(st_effect_pd.source == pair.split(',')[0]) & (st_effect_pd.target == pair.split(',')[1])].sort(columns=[method],ascending=False)\n",
    "        first_row = wdpair_pd.iloc[0]\n",
    "        wdpair_sents.append({'source':first_row.source,'target':first_row.target,\n",
    "                             'sentence':first_row.sentence,'knn_effect':first_row.knn_effect,'vt_effect':first_row.vt_effect,\n",
    "                             'ctf_effect':first_row.ctf_effect,'csf_effect':first_row.csf_effect,\n",
    "                             'true_y':first_row.true_y,'id':first_row.id})\n",
    "        \n",
    "        effect_list = wdpair_pd[method].values\n",
    "        interv = (max(effect_list)-min(effect_list))/(n_sents-0.1)\n",
    "        intervid_list = {}\n",
    "        \n",
    "        for idx,row in wdpair_pd.iloc[1:].iterrows():\n",
    "            interv_id = math.floor((row[method]-min(effect_list))/interv)\n",
    "            if(interv_id not in intervid_list):\n",
    "                intervid_list[interv_id]=[]\n",
    "            intervid_list[interv_id].append(idx)\n",
    "        \n",
    "        tmp_rdidx['rand_idx'] = []\n",
    "        for i in range(min(n_sents,wdpair_pd.shape[0])):\n",
    "            if(i in intervid_list):\n",
    "                rd_i = random.sample(intervid_list[i],1)\n",
    "                #return wdpair_pd,intervid_list,i,rd_i\n",
    "                rd_row = wdpair_pd.ix[rd_i]\n",
    "                if(len(re.findall('\\w+',rd_row.sentence.values[0])) > min_sent_length):\n",
    "                    tmp_rdidx['rand_idx'].append(rd_i)\n",
    "                    wdpair_sents.append({'source':rd_row.source.values[0],'target':rd_row.target.values[0],\n",
    "                                         'sentence':rd_row.sentence.values[0],'knn_effect':rd_row.knn_effect.values[0],\n",
    "                                         'vt_effect':rd_row.vt_effect.values[0],'ctf_effect':rd_row.ctf_effect.values[0],\n",
    "                                         'csf_effect':rd_row.csf_effect.values[0],\n",
    "                                         'true_y':rd_row.true_y.values[0],'id':rd_row.id.values[0]})\n",
    "            else:\n",
    "                print(pair,i)\n",
    "        \n",
    "        #wdpair_rdidx.append(tmp_rdidx)\n",
    "        \n",
    "    #pd.DataFrame(wdpair_rdidx).to_csv()\n",
    "    pd.DataFrame(wdpair_sents).to_csv(topn_file,columns=['source','target','sentence','knn_effect','vt_effect','ctf_effect','csf_effect','true_y','id'],index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plaza,square 6\n",
      "entertainment,recreation 9\n",
      "plaza,place 7\n",
      "chinese,traditional 9\n",
      "hard,excellent 9\n",
      "store,boutique 9\n",
      "beautiful,wonderful 8\n",
      "beautiful,wonderful 9\n",
      "beautiful,picturesque 8\n",
      "exciting,unique 9\n"
     ]
    }
   ],
   "source": [
    "effect_file = airbnb_path+'5_Select/airbnb_wdpair_sents_4effects_posinters_poscom1_bigramcheck_limit_limitvocab.pickle'\n",
    "top_pairs = select_topn_wdpairs(effect_file,n_pairs=30,min_sent_length=3,method='knn_effect')\n",
    "select_nsents_for_wdpairs(effect_file,top_pairs,n_sents=10,min_sent_length=0,\n",
    "                          topn_file = airbnb_path+'5_Select/4_posinters_poscom1_bigram_limit/airbnb_knn_top_30pair_11sents_poscom1_bigram_limit_limitvocab.csv',\n",
    "                          method = 'knn_effect')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "airbnb,csf:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['yummy,good',\n",
       " 'annual,general',\n",
       " 'trips,tours',\n",
       " 'wide,multiple',\n",
       " 'wide,grand',\n",
       " 'various,several',\n",
       " 'wide,whole',\n",
       " 'predominantly,mostly',\n",
       " 'general,grand',\n",
       " 'predominantly,especially',\n",
       " 'yummy,delicious',\n",
       " 'suggestions,views',\n",
       " 'help,sit',\n",
       " 'wide,several',\n",
       " 'wide,numerous',\n",
       " 'comfortable,happy',\n",
       " 'apartments,homes',\n",
       " 'wide,open',\n",
       " 'closest,highest',\n",
       " 'wide,much']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"airbnb,csf:\")\n",
    "top_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(387213, 9)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pd = pd.DataFrame(pickle.load(open(effect_file,'rb'))).sort(columns=['ctf_effect'],ascending=False)\n",
    "test_pd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>csf_effect</th>\n",
       "      <th>ctf_effect</th>\n",
       "      <th>id</th>\n",
       "      <th>knn_effect</th>\n",
       "      <th>sentence</th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>true_y</th>\n",
       "      <th>vt_effect</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1167909</th>\n",
       "      <td>0.08530</td>\n",
       "      <td>0.67888</td>\n",
       "      <td>1167909</td>\n",
       "      <td>0.23333</td>\n",
       "      <td>Little Tokyo also has amazing food, we recomme...</td>\n",
       "      <td>amazing</td>\n",
       "      <td>outstanding</td>\n",
       "      <td>0</td>\n",
       "      <td>0.02356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1167563</th>\n",
       "      <td>0.06561</td>\n",
       "      <td>0.67222</td>\n",
       "      <td>1167563</td>\n",
       "      <td>0.23333</td>\n",
       "      <td>Little Tokyo also has amazing food, we recomme...</td>\n",
       "      <td>amazing</td>\n",
       "      <td>outstanding</td>\n",
       "      <td>0</td>\n",
       "      <td>0.01570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1167519</th>\n",
       "      <td>0.11755</td>\n",
       "      <td>0.66128</td>\n",
       "      <td>1167519</td>\n",
       "      <td>0.23333</td>\n",
       "      <td>Little Tokyo also has amazing food, we recomme...</td>\n",
       "      <td>amazing</td>\n",
       "      <td>outstanding</td>\n",
       "      <td>0</td>\n",
       "      <td>0.01968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1167687</th>\n",
       "      <td>0.17155</td>\n",
       "      <td>0.65167</td>\n",
       "      <td>1167687</td>\n",
       "      <td>0.23333</td>\n",
       "      <td>Little Tokyo also has amazing food, we recomme...</td>\n",
       "      <td>amazing</td>\n",
       "      <td>outstanding</td>\n",
       "      <td>0</td>\n",
       "      <td>0.02165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1167593</th>\n",
       "      <td>0.07898</td>\n",
       "      <td>0.60272</td>\n",
       "      <td>1167593</td>\n",
       "      <td>0.20000</td>\n",
       "      <td>Little Tokyo also has amazing food, we recomme...</td>\n",
       "      <td>amazing</td>\n",
       "      <td>outstanding</td>\n",
       "      <td>0</td>\n",
       "      <td>0.01551</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         csf_effect  ctf_effect       id  knn_effect  \\\n",
       "1167909     0.08530     0.67888  1167909     0.23333   \n",
       "1167563     0.06561     0.67222  1167563     0.23333   \n",
       "1167519     0.11755     0.66128  1167519     0.23333   \n",
       "1167687     0.17155     0.65167  1167687     0.23333   \n",
       "1167593     0.07898     0.60272  1167593     0.20000   \n",
       "\n",
       "                                                  sentence   source  \\\n",
       "1167909  Little Tokyo also has amazing food, we recomme...  amazing   \n",
       "1167563  Little Tokyo also has amazing food, we recomme...  amazing   \n",
       "1167519  Little Tokyo also has amazing food, we recomme...  amazing   \n",
       "1167687  Little Tokyo also has amazing food, we recomme...  amazing   \n",
       "1167593  Little Tokyo also has amazing food, we recomme...  amazing   \n",
       "\n",
       "              target  true_y  vt_effect  \n",
       "1167909  outstanding       0    0.02356  \n",
       "1167563  outstanding       0    0.01570  \n",
       "1167519  outstanding       0    0.01968  \n",
       "1167687  outstanding       0    0.02165  \n",
       "1167593  outstanding       0    0.01551  "
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('You', 'PRP'),\n",
       " ('will', 'MD'),\n",
       " ('be', 'VB'),\n",
       " ('able', 'JJ'),\n",
       " ('to', 'TO'),\n",
       " ('walk', 'VB'),\n",
       " ('to', 'TO'),\n",
       " ('award', 'VB'),\n",
       " ('winning', 'VBG'),\n",
       " ('restaurants,', 'NN'),\n",
       " ('as', 'RB'),\n",
       " ('well', 'RB'),\n",
       " ('as', 'IN'),\n",
       " ('great', 'JJ'),\n",
       " ('bars,', 'JJ'),\n",
       " ('casual', 'JJ'),\n",
       " ('dining', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('cozy', 'JJ'),\n",
       " ('coffee', 'NN'),\n",
       " ('shops.', 'NN')]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tag(\"You will be able to walk to award winning restaurants, as well as great bars, casual dining and cozy coffee shops.\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('You', 'PRP'),\n",
       " ('will', 'MD'),\n",
       " ('be', 'VB'),\n",
       " ('able', 'JJ'),\n",
       " ('to', 'TO'),\n",
       " ('walk', 'VB'),\n",
       " ('to', 'TO'),\n",
       " ('award', 'VB'),\n",
       " ('won', 'NN'),\n",
       " ('restaurants,', 'NN'),\n",
       " ('as', 'RB'),\n",
       " ('well', 'RB'),\n",
       " ('as', 'IN'),\n",
       " ('great', 'JJ'),\n",
       " ('bars,', 'JJ'),\n",
       " ('casual', 'JJ'),\n",
       " ('dining', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('cozy', 'JJ'),\n",
       " ('coffee', 'NN'),\n",
       " ('shops.', 'NN')]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tag(\"You will be able to walk to award won restaurants, as well as great bars, casual dining and cozy coffee shops.\".split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Generate paraphrase substitution sentences for selected word pairs and sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mystr = \"My boyfriend and I came here for the boyfriend first time on a recent trip to Vegas and could not have been more pleased with the quality of food and service .\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My buddy and I came here for the buddy first time on a recent trip to Vegas and could not have been more pleased with the quality of food and service .'"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub('boyfriend','buddy',mystr,re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def substitute_sentence(wd_sents_file,subs_file):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        File for word pairs with selected sentences\n",
    "    Function:\n",
    "        Generate new sentences by substituting source words with target words.\n",
    "    Return:\n",
    "        A csv file with original sentences and substituted sentences.\n",
    "    \"\"\"\n",
    "    wd_sents_pd = pd.read_csv(wd_sents_file)\n",
    "    all_info = []\n",
    "    for idx,row in wd_sents_pd.iterrows():\n",
    "        tar_sent = re.sub(row.source,row.target,row.sentence)#flags = re.IGNORECASE\n",
    "        all_info.append({'source':row.source,'target':row.target,'src_sentence':row.sentence,'tar_sentence':tar_sent,\n",
    "                         'knn_effect':row.knn_effect,'vt_effect':row.vt_effect,'ctf_effect':row.ctf_effect,\n",
    "                         'csf_effect':row.csf_effect,\n",
    "                         'true_y':row.true_y,'id':row.id})\n",
    "\n",
    "    pd.DataFrame(all_info).to_csv(subs_file,columns=['source','target','src_sentence','tar_sentence',\n",
    "                                                    'knn_effect','vt_effect','ctf_effect','csf_effect','true_y','id'],index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "this_path = tw_path\n",
    "prefix = 'tw'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd_sents_file = this_path+'5_Select/5_tolabel/'+prefix+'_knn_10wdpairs_30sents.csv'\n",
    "subs_file = this_path+'5_Select/5_tolabel/'+prefix+'_knn_10wdpairs_60sents.csv'\n",
    "substitute_sentence(wd_sents_file,subs_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wd_sents_file = this_path+'5_Select/5_tolabel/'+prefix+'_vt_10wdpairs_30sents.csv'\n",
    "subs_file = this_path+'5_Select/5_tolabel/'+prefix+'_vt_10wdpairs_60sents.csv'\n",
    "substitute_sentence(wd_sents_file,subs_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd_sents_file = this_path+'5_Select/5_tolabel/'+prefix+'_ctf_10wdpairs_30sents.csv'\n",
    "subs_file = this_path+'5_Select/5_tolabel/'+prefix+'_ctf_10wdpairs_60sents.csv'\n",
    "substitute_sentence(wd_sents_file,subs_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wd_sents_file = this_path+'5_Select/5_tolabel/'+prefix+'_csf_10wdpairs_30sents.csv'\n",
    "subs_file = this_path+'5_Select/5_tolabel/'+prefix+'_csf_10wdpairs_60sents.csv'\n",
    "substitute_sentence(wd_sents_file,subs_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_path = yp_path\n",
    "prefix = 'yp'\n",
    "subs_file = this_path+'5_Select/4_posinters_poscom1_bigram_limit/'+prefix+'_knn_top_30pair_11sents_limit_lb_bidirection.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_pd = pd.read_csv(subs_file)\n",
    "pair_ct = Counter()\n",
    "for idx,row in csv_pd.iterrows():\n",
    "    pair_ct.update(row.source+','+row.target)\n",
    "len(pair_ct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Different evaluation methods\n",
    "- 5.1 Rank all word pairs according to different methods. <br>\n",
    "- 5.2 Get average treatment effect for each word pair. <br>\n",
    "- 5.3 Fetch information for word pairs with coef change and frequency in opposite class. <br>\n",
    "- 5.4 Get top-10 and bottom-10 word pair. <br>\n",
    "- 5.5 Find treatment words for a sentence. <br>\n",
    "- 5.6 Spearman rank correlation for sentences and word pairs. <br>\n",
    "- 5.7 Percentage of negative instances among topn high treatment sentences. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- 5.1 Rank all word pairs according to different methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sort_allpairs(effect_file,method,flag):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        Treatment effect file, sort according to treatment effect calculated by which method, flag for descending or asending\n",
    "    Function:\n",
    "        Sort word pairs according to treatment effect calculated by a specific method.\n",
    "    Return:\n",
    "        DataFrame for de-duplicate word pairs, only contain 2 fields: ['source','target']\n",
    "    \"\"\"\n",
    "    \n",
    "    effect_pd = pd.DataFrame(pickle.load(open(effect_file,'rb'))).sort(columns=[method],ascending=flag)\n",
    "    method_pd = effect_pd[['source','target']].drop_duplicates(keep='first').reset_index()\n",
    "    return method_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rank_wdpairs(effect_file,rank_file,increase):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        Treatment effect file, file that contains ranks for word pairs according to 4 methods, increase order or decrease\n",
    "    Function:\n",
    "        Get order for each word pair according to 4 methods\n",
    "    Return:\n",
    "        A csv file with word pairs and ranks according to 4 methods.\n",
    "    \"\"\"\n",
    "    # Sort by one effect, increase = True, 1_0, increase = False, 0_1 \n",
    "    knn_pd = sort_allpairs(effect_file,method='knn_effect',flag=increase)\n",
    "    vt_pd = sort_allpairs(effect_file,method='vt_effect',flag=increase)\n",
    "    ctf_pd = sort_allpairs(effect_file,method='ctf_effect',flag=increase)\n",
    "    csf_pd = sort_allpairs(effect_file,method='csf_effect',flag=increase)\n",
    "    \n",
    "    wdpair_info = []\n",
    "    \n",
    "    for idx,row in knn_pd.iterrows():\n",
    "        tmp_pair = {}\n",
    "        tmp_pair['source'] = row.source\n",
    "        tmp_pair['target'] = row.target\n",
    "        tmp_pair['knn_rank'] = idx\n",
    "        tmp_pair['vt_rank'] = vt_pd[(vt_pd.source == row.source) & (vt_pd.target == row.target)].index[0]\n",
    "        tmp_pair['ctf_rank'] = ctf_pd[(ctf_pd.source == row.source) & (ctf_pd.target == row.target)].index[0]\n",
    "        tmp_pair['csf_rank'] = csf_pd[(csf_pd.source == row.source) & (csf_pd.target == row.target)].index[0]\n",
    "        \n",
    "        wdpair_info.append(tmp_pair)\n",
    "    pd.DataFrame(wdpair_info).to_csv(rank_file,columns=['source','target','knn_rank','vt_rank','ctf_rank','csf_rank'],index=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "effect_file = airbnb_path+'5_Select/airbnb_wdpair_sents_4effects_posinters_poscom1_bigramcheck_limit_limitvocab.pickle'\n",
    "rank_file =  airbnb_path+'5_Select/6_PairRank/0_1/airbnb_pairrank_limitvocab.csv'\n",
    "rank_wdpairs(effect_file,rank_file,increase = False)#increase = True, 1_0, increase = False, 0_1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Get average treatment effect for each word pair \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_avg_effect(effect_file,wdpair_file,avg_effect_file):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        Treatment effect file, ranked word pairs file (optional), file to store average treatment effect for each word pair\n",
    "    Function:\n",
    "        Calculate average treatment effect for each word pair according to 4 methods\n",
    "    Return:\n",
    "        A csv file with records average treatment effect and rank for each word pair.\n",
    "    \"\"\"\n",
    "    effect_pd = pd.DataFrame(pickle.load(open(effect_file,'rb')))\n",
    "    wdpair_pd = pd.read_csv(wdpair_file)\n",
    "    avg_info = []\n",
    "    for idx, row in wdpair_pd.iterrows():\n",
    "        pair_df = effect_pd[(effect_pd.source == row.source) & (effect_pd.target == row.target)]\n",
    "        row_info = {}\n",
    "        row_info['source'] = row.source\n",
    "        row_info['target'] = row.target\n",
    "        row_info['avg_knn'] = np.mean(pair_df.knn_effect.values)\n",
    "        row_info['avg_vt'] = np.mean(pair_df.vt_effect.values)\n",
    "        row_info['avg_ctf'] = np.mean(pair_df.ctf_effect.values)\n",
    "        row_info['avg_csf'] = np.mean(pair_df.csf_effect.values)\n",
    "    \n",
    "        avg_info.append(row_info)\n",
    "    \n",
    "    mg_df = wdpair_pd.merge(pd.DataFrame(avg_info),left_on=['source','target'],right_on=['source','target'],how='inner')\n",
    "    mg_df.to_csv(avg_effect_file,columns=['source','target','knn_rank','vt_rank','ctf_rank','csf_rank',\n",
    "                                          'avg_knn','avg_vt','avg_ctf','avg_csf'],index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "effect_file = yp_path+'5_Select/yp_wdpair_sents_4effects_posinters_poscom1_bigramcheck_limit.pickle'\n",
    "wdpair_file =  yp_path+'5_Select/6_PairRank/1_0/yp_pairrank.csv'\n",
    "avg_effect_file = yp_path+'5_Select/6_PairRank/1_0/yp_pairrank_avgeffect.csv'\n",
    "get_avg_effect(effect_file,wdpair_file,avg_effect_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "effect_file = tw_path+'5_Select/tw_wdpair_sents_4effects_posinters_poscom1_bigramcheck_limit.pickle'\n",
    "wdpair_file =  tw_path+'5_Select/6_PairRank/1_0/tw_pairrank.csv'\n",
    "avg_effect_file = tw_path+'5_Select/6_PairRank/1_0/tw_pairrank_avgeffect.csv'\n",
    "get_avg_effect(effect_file,wdpair_file,avg_effect_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "effect_file = airbnb_path+'5_Select/airbnb_wdpair_sents_4effects_posinters_poscom1_bigramcheck_limit_limitvocab.pickle'\n",
    "wdpair_file =  airbnb_path+'5_Select/6_PairRank/1_0/airbnb_pairrank_limitvocab.csv'\n",
    "avg_effect_file = airbnb_path+'5_Select/6_PairRank/1_0/airbnb_pairrank_avgeffect_limitvocab.csv'\n",
    "get_avg_effect(effect_file,wdpair_file,avg_effect_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3 Fetch information for word pairs with coef change and frequency in opposite class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_coef(avg_effect_file,coef_file,res_file):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        Treatment effect file with average treatment effect, file with coefficient information for each word pair, \n",
    "        result file to store all information for each word pair.\n",
    "    Function:\n",
    "        Get coefficient change of each word pair and frequency in opposite class.\n",
    "    Return:\n",
    "        A csv file with rank info, avg effect, coefficient change, and frequency in opposite class.\n",
    "    \"\"\"\n",
    "    effect_pd = pd.read_csv(avg_effect_file)\n",
    "    coef_pd = pd.read_csv(coef_file)\n",
    "    \n",
    "    all_info = []\n",
    "    for idx,row in effect_pd.iterrows():\n",
    "        tmp_info = {}\n",
    "        tmp_info['source'] = row.source\n",
    "        tmp_info['target'] = row.target\n",
    "        \n",
    "        src_tar_coef = coef_pd[(coef_pd.source == row.source) & (coef_pd.target == row.target)]\n",
    "        tar_src_coef = coef_pd[(coef_pd.target == row.source) & (coef_pd.source == row.target)]\n",
    "        if(src_tar_coef.shape[0]>0):\n",
    "            tmp_info['coef_change'] = float(src_tar_coef.tar_coef.values[0]) - float(src_tar_coef.src_coef.values[0])\n",
    "            tmp_info['tar_n_neg'] = src_tar_coef.tar_n_neg.values[0]\n",
    "            #tmp_info['tar_n_pos'] = src_tar_coef.tar_n_pos.values[0]\n",
    "        elif(tar_src_coef.shape[0]>0):\n",
    "            tmp_info['coef_change'] = float(tar_src_coef.src_coef.values[0]) - float(tar_src_coef.tar_coef.values[0])\n",
    "            tmp_info['tar_n_neg'] = tar_src_coef.src_n_neg.values[0]\n",
    "            #tmp_info['tar_n_pos'] = tar_src_coef.src_n_pos.values[0]\n",
    "            \n",
    "        else:\n",
    "            print(\"Error:\",row.source,row.target)\n",
    "        \n",
    "        all_info.append(tmp_info)\n",
    "        \n",
    "    mg_df = effect_pd.merge(pd.DataFrame(all_info),left_on=['source','target'],right_on=['source','target'],how='inner')\n",
    "    mg_df.to_csv(res_file,columns=['source','target','knn_rank','vt_rank','ctf_rank','csf_rank',\n",
    "                                          'avg_knn','avg_vt','avg_ctf','avg_csf','coef_change','tar_n_neg'],index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_effect_file = airbnb_path+'5_Select/6_PairRank/0_1/airbnb_pairrank_avgeffect_limitvocab.csv'\n",
    "coef_file = airbnb_path+'1_Process/airbnb_treat_pairs.csv'\n",
    "res_file = airbnb_path+'5_Select/6_PairRank/0_1/airbnb_pairrank_avgeffect_coef_limitvocab.csv'\n",
    "merge_coef(avg_effect_file,coef_file,res_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4 Get top-10 and bottom-10 word pair "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_top10(effect_file,res01_file,res10_file):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        Treatment effect file, file for top 10 word pairs and bottom 10 word pairs.\n",
    "    Function:\n",
    "        Get top 10 and bottom 10 word pairs.\n",
    "        Top word pairs are positive treatment effect, bottom word pairs are negative treatment effect.\n",
    "    Return:\n",
    "        A csv file with top 10 word pairs and a csv file with bottom 10 word pairs.\n",
    "    \"\"\"\n",
    "    effect_pd = pd.DataFrame(pickle.load(open(effect_file,'rb')))\n",
    "    method = ['knn_effect','vt_effect','ctf_effect','csf_effect']\n",
    "    df01_list = []\n",
    "    df10_list = []\n",
    "    for i in range(len(method)):\n",
    "        st_df = effect_pd.sort(columns = [method[i]],ascending=False)\n",
    "        df01_list.append(st_df.iloc[:10])\n",
    "        df10_list.append(st_df.iloc[-10:])\n",
    "    \n",
    "    pd.concat(df01_list).to_csv(res01_file,columns=['source','target','true_y','sentence',\n",
    "                                                    'knn_effect','vt_effect','ctf_effect','csf_effect'], index=False)\n",
    "    \n",
    "    pd.concat(df10_list).to_csv(res10_file,columns=['source','target','true_y','sentence',\n",
    "                                                    'knn_effect','vt_effect','ctf_effect','csf_effect'], index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "effect_file = airbnb_path+'5_Select/airbnb_wdpair_sents_4effects_posinters_poscom1_bigramcheck_limit_limitvocab.pickle'\n",
    "res01_file = airbnb_path+'5_Select/7_PairInSents/airbnb_top10_limitvocab.csv'\n",
    "res10_file = airbnb_path+'5_Select/7_PairInSents/airbnb_end10_limitvocab.csv'\n",
    "get_top10(effect_file,res01_file,res10_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.5 Find treatment words for a sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fetch_words_forsents(effect_file,wdpair_file,res_file):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        Treatment effect file, treatment word pairs, result file with all treatment word pairs for each sentence.\n",
    "    Function:\n",
    "        Search for all treatment word pairs for every sentence.\n",
    "    Return:\n",
    "        A pickle file to record all treatment word pairs for every sentence.\n",
    "    \"\"\"\n",
    "    effect_pd = pd.DataFrame(pickle.load(open(effect_file,'rb')))\n",
    "    wdpair_pd = pd.read_csv(wdpair_file)\n",
    "    src_wds = list(set(wdpair_pd.source.values))\n",
    "    ct_vec = CountVectorizer(binary=True,vocabulary=src_wds)\n",
    "    sents = effect_pd[effect_pd.true_y == 0].drop_duplicates(['sentence'],keep='first').sentence.values\n",
    "    X_sents = ct_vec.fit_transform(sents)\n",
    "    vocab = ct_vec.get_feature_names()\n",
    "    sent_pd_list = []\n",
    "    sent_id_list = []\n",
    "    n_one = 0\n",
    "    for i in range(10):#X_sents.shape[0]\n",
    "        row_wdidx = X_sents[i].nonzero()[1]\n",
    "        if(len(row_wdidx)>1):\n",
    "            all_para_info = []\n",
    "            for srci in row_wdidx:\n",
    "                src_wd = vocab[srci]\n",
    "                tar_wd_list = wdpair_pd[(wdpair_pd.source == src_wd)].target.values\n",
    "                for tarj in range(len(tar_wd_list)):\n",
    "                    para_info = {}\n",
    "                    tar_wd = tar_wd_list[tarj]\n",
    "                    select_pd=effect_pd[(effect_pd.source==src_wd)&(effect_pd.target==tar_wd)&(effect_pd.sentence==sents[i])]\n",
    "                    if(select_pd.shape[0]>0):\n",
    "                        para_info['source']=src_wd\n",
    "                        para_info['target']=tar_wd\n",
    "                        para_info['knn_effect']=select_pd.knn_effect.values[0]\n",
    "                        para_info['vt_effect']=select_pd.vt_effect.values[0]\n",
    "                        para_info['ctf_effect']=select_pd.ctf_effect.values[0]\n",
    "                        para_info['csf_effect']=select_pd.csf_effect.values[0]\n",
    "                        all_para_info.append(para_info)\n",
    "            sent_pd_list.append(pd.DataFrame(all_para_info))\n",
    "            sent_id_list.append(i)\n",
    "    #return sent_pd_list\n",
    "    all_info = []\n",
    "    for j in range(len(sent_pd_list)):#X_sents.shape[0]\n",
    "        sent_info = {}\n",
    "        sent_info['sentence']=sents[sent_id_list[j]]\n",
    "        sent_info['treat_words']=sent_pd_list[j]\n",
    "        all_info.append(sent_info)\n",
    "                \n",
    "    #print(\"%d, %.5f has more than one positive treatment word.\" % ((X_sents.shape[0]-n_one),((X_sents.shape[0]-n_one)/X_sents.shape[0])))\n",
    "    pd.DataFrame(all_info).reset_index().to_csv(res_file,columns=['sentence','treat_words'],index=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "effect_file = yp_path+'5_Select/yp_wdpair_sents_4effects_posinters_poscom1_bigramcheck_limit.pickle'\n",
    "wdpair_file = yp_path+'5_Select/6_PairRank/0_1/yp_pairrank_avgeffect_coef.csv'\n",
    "res_file = yp_path+'5_Select/7_PairInSents/yp_pairinsents_pd.csv'\n",
    "fetch_words_forsents(effect_file,wdpair_file,res_file)\n",
    "#139708, 0.51890 has more than one treatment word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_pd = pd.read_csv(yp_path+'5_Select/7_PairInSents/yp_pairinsents_pd.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'   csf_effect  ctf_effect  knn_effect source    target  vt_effect\\n0    -0.05456     0.00950     0.06667   shop  boutique   -0.01361\\n1     0.02197     0.02569    -0.03333   shop      mall   -0.00058\\n2    -0.05700    -0.04470     0.03333   shop     store   -0.01193'"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pd.iloc[0].treat_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find treatment words for a sentence\n",
    "> Used for selecting example sentences to present in paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_treat_words(effect_file,wdpair_file,my_sents):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        Treatment effect file, treatment word pairs, a given sentence.\n",
    "    Function:\n",
    "        Search for all treatment word pairs for a given sentence.\n",
    "    Return:\n",
    "        A DataFrame with all treatment word pairs and corresponding treatment effects.\n",
    "    \"\"\"\n",
    "    effect_pd = pd.DataFrame(pickle.load(open(effect_file,'rb')))\n",
    "    wdpair_pd = pd.read_csv(wdpair_file)\n",
    "    src_wds = list(set(wdpair_pd.source.values))\n",
    "    ct_vec = CountVectorizer(binary=True,vocabulary=src_wds)\n",
    "    #sents = effect_pd[effect_pd.true_y == 0].drop_duplicates(['sentence'],keep='first').sentence.values\n",
    "    X_sents = ct_vec.fit_transform([my_sents])\n",
    "    vocab = ct_vec.get_feature_names()\n",
    "\n",
    "    treat_info = []\n",
    "    row_wdidx = X_sents[0].nonzero()[1]\n",
    "    if(len(row_wdidx)>0):\n",
    "        for srci in row_wdidx:\n",
    "            src_wd = vocab[srci]\n",
    "            tar_wd_list = wdpair_pd[(wdpair_pd.source == src_wd)].target.values\n",
    "            for tarj in range(len(tar_wd_list)):\n",
    "                para_info = {}\n",
    "                tar_wd = tar_wd_list[tarj]\n",
    "                select_pd=effect_pd[(effect_pd.source==src_wd)&(effect_pd.target==tar_wd)&(effect_pd.sentence==my_sents)]\n",
    "                if(select_pd.shape[0]>0):\n",
    "                    para_info['source']=src_wd\n",
    "                    para_info['target']=tar_wd\n",
    "                    para_info['knn_effect']=select_pd.knn_effect.values[0]\n",
    "                    para_info['vt_effect']=select_pd.vt_effect.values[0]\n",
    "                    para_info['ctf_effect']=select_pd.ctf_effect.values[0]\n",
    "                    para_info['csf_effect']=select_pd.csf_effect.values[0]\n",
    "                    treat_info.append(para_info)\n",
    "    return pd.DataFrame(treat_info,columns=['source','target','knn_effect','vt_effect','ctf_effect','csf_effect'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "effect_file = airbnb_path+'5_Select/airbnb_wdpair_sents_4effects_posinters_poscom1_bigramcheck_limit_limitvocab.pickle'\n",
    "wdpair_file = airbnb_path+'5_Select/6_PairRank/0_1/airbnb_pairrank_avgeffect_coef_limitvocab.csv'\n",
    "#my_sents = 'It\\'s predominantly hipsterville, making it very affordable and good for people watching.'\n",
    "treat_pd = get_treat_words(effect_file,wdpair_file,airbnb_sents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>knn_effect</th>\n",
       "      <th>vt_effect</th>\n",
       "      <th>ctf_effect</th>\n",
       "      <th>csf_effect</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>exciting</td>\n",
       "      <td>spectacular</td>\n",
       "      <td>0.13333</td>\n",
       "      <td>0.01976</td>\n",
       "      <td>0.17731</td>\n",
       "      <td>0.20004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>exciting</td>\n",
       "      <td>stunning</td>\n",
       "      <td>0.23333</td>\n",
       "      <td>0.01883</td>\n",
       "      <td>0.16231</td>\n",
       "      <td>0.16514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>exciting</td>\n",
       "      <td>fantastic</td>\n",
       "      <td>0.23333</td>\n",
       "      <td>0.00894</td>\n",
       "      <td>0.08822</td>\n",
       "      <td>0.15960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>exciting</td>\n",
       "      <td>wonderful</td>\n",
       "      <td>0.16667</td>\n",
       "      <td>0.02686</td>\n",
       "      <td>0.12160</td>\n",
       "      <td>0.15543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>exciting</td>\n",
       "      <td>fabulous</td>\n",
       "      <td>0.33333</td>\n",
       "      <td>0.01996</td>\n",
       "      <td>0.15407</td>\n",
       "      <td>0.15069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>exciting</td>\n",
       "      <td>incredible</td>\n",
       "      <td>0.20000</td>\n",
       "      <td>0.01720</td>\n",
       "      <td>0.08051</td>\n",
       "      <td>0.14791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>exciting</td>\n",
       "      <td>awesome</td>\n",
       "      <td>0.20000</td>\n",
       "      <td>0.00926</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>exciting</td>\n",
       "      <td>excellent</td>\n",
       "      <td>0.23333</td>\n",
       "      <td>0.02928</td>\n",
       "      <td>0.10999</td>\n",
       "      <td>0.13894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>exciting</td>\n",
       "      <td>great</td>\n",
       "      <td>0.03333</td>\n",
       "      <td>0.01958</td>\n",
       "      <td>0.03010</td>\n",
       "      <td>0.12944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>exciting</td>\n",
       "      <td>cool</td>\n",
       "      <td>0.16667</td>\n",
       "      <td>0.01985</td>\n",
       "      <td>0.10167</td>\n",
       "      <td>0.11462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>exciting</td>\n",
       "      <td>good</td>\n",
       "      <td>0.16667</td>\n",
       "      <td>0.02539</td>\n",
       "      <td>0.04681</td>\n",
       "      <td>0.10928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>exciting</td>\n",
       "      <td>vibrant</td>\n",
       "      <td>0.06667</td>\n",
       "      <td>0.01372</td>\n",
       "      <td>-0.05032</td>\n",
       "      <td>0.10857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>exciting</td>\n",
       "      <td>interesting</td>\n",
       "      <td>0.33333</td>\n",
       "      <td>0.02937</td>\n",
       "      <td>0.13344</td>\n",
       "      <td>0.10017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>exciting</td>\n",
       "      <td>grand</td>\n",
       "      <td>0.23333</td>\n",
       "      <td>0.02070</td>\n",
       "      <td>0.13417</td>\n",
       "      <td>0.09895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>exciting</td>\n",
       "      <td>unique</td>\n",
       "      <td>0.30000</td>\n",
       "      <td>0.02974</td>\n",
       "      <td>0.13635</td>\n",
       "      <td>0.09669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>exciting</td>\n",
       "      <td>super</td>\n",
       "      <td>0.23333</td>\n",
       "      <td>0.01960</td>\n",
       "      <td>0.06895</td>\n",
       "      <td>0.09373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>exciting</td>\n",
       "      <td>amazing</td>\n",
       "      <td>0.20000</td>\n",
       "      <td>0.00333</td>\n",
       "      <td>0.09330</td>\n",
       "      <td>0.09193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>exciting</td>\n",
       "      <td>last</td>\n",
       "      <td>0.13333</td>\n",
       "      <td>0.00888</td>\n",
       "      <td>0.09524</td>\n",
       "      <td>0.08612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>exciting</td>\n",
       "      <td>colorful</td>\n",
       "      <td>0.03333</td>\n",
       "      <td>0.00123</td>\n",
       "      <td>0.05012</td>\n",
       "      <td>0.08475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>exciting</td>\n",
       "      <td>hot</td>\n",
       "      <td>0.20000</td>\n",
       "      <td>0.01539</td>\n",
       "      <td>0.12196</td>\n",
       "      <td>0.05762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>definitely</td>\n",
       "      <td>absolutely</td>\n",
       "      <td>0.16667</td>\n",
       "      <td>0.00095</td>\n",
       "      <td>0.01642</td>\n",
       "      <td>0.05469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>stay</td>\n",
       "      <td>stand</td>\n",
       "      <td>0.06667</td>\n",
       "      <td>0.01345</td>\n",
       "      <td>0.08887</td>\n",
       "      <td>0.05165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>definitely</td>\n",
       "      <td>really</td>\n",
       "      <td>0.23333</td>\n",
       "      <td>-0.00183</td>\n",
       "      <td>-0.02664</td>\n",
       "      <td>0.02289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vibrant</td>\n",
       "      <td>dynamic</td>\n",
       "      <td>0.10000</td>\n",
       "      <td>0.00217</td>\n",
       "      <td>0.07067</td>\n",
       "      <td>0.02240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>definitely</td>\n",
       "      <td>truly</td>\n",
       "      <td>0.16667</td>\n",
       "      <td>0.00006</td>\n",
       "      <td>-0.00334</td>\n",
       "      <td>0.01669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vibrant</td>\n",
       "      <td>super</td>\n",
       "      <td>0.13333</td>\n",
       "      <td>-0.00069</td>\n",
       "      <td>-0.00136</td>\n",
       "      <td>-0.00052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>let</td>\n",
       "      <td>pass</td>\n",
       "      <td>-0.16667</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>-0.04898</td>\n",
       "      <td>-0.02053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>let</td>\n",
       "      <td>help</td>\n",
       "      <td>-0.20000</td>\n",
       "      <td>0.00113</td>\n",
       "      <td>-0.04099</td>\n",
       "      <td>-0.09573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vibrant</td>\n",
       "      <td>exciting</td>\n",
       "      <td>-0.16667</td>\n",
       "      <td>0.00283</td>\n",
       "      <td>-0.04501</td>\n",
       "      <td>-0.09862</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        source       target  knn_effect  vt_effect  ctf_effect  csf_effect\n",
       "8     exciting  spectacular     0.13333    0.01976     0.17731     0.20004\n",
       "17    exciting     stunning     0.23333    0.01883     0.16231     0.16514\n",
       "10    exciting    fantastic     0.23333    0.00894     0.08822     0.15960\n",
       "9     exciting    wonderful     0.16667    0.02686     0.12160     0.15543\n",
       "11    exciting     fabulous     0.33333    0.01996     0.15407     0.15069\n",
       "7     exciting   incredible     0.20000    0.01720     0.08051     0.14791\n",
       "20    exciting      awesome     0.20000    0.00926     0.10340     0.14167\n",
       "21    exciting    excellent     0.23333    0.02928     0.10999     0.13894\n",
       "16    exciting        great     0.03333    0.01958     0.03010     0.12944\n",
       "18    exciting         cool     0.16667    0.01985     0.10167     0.11462\n",
       "25    exciting         good     0.16667    0.02539     0.04681     0.10928\n",
       "13    exciting      vibrant     0.06667    0.01372    -0.05032     0.10857\n",
       "12    exciting  interesting     0.33333    0.02937     0.13344     0.10017\n",
       "15    exciting        grand     0.23333    0.02070     0.13417     0.09895\n",
       "6     exciting       unique     0.30000    0.02974     0.13635     0.09669\n",
       "23    exciting        super     0.23333    0.01960     0.06895     0.09373\n",
       "19    exciting      amazing     0.20000    0.00333     0.09330     0.09193\n",
       "22    exciting         last     0.13333    0.00888     0.09524     0.08612\n",
       "14    exciting     colorful     0.03333    0.00123     0.05012     0.08475\n",
       "24    exciting          hot     0.20000    0.01539     0.12196     0.05762\n",
       "3   definitely   absolutely     0.16667    0.00095     0.01642     0.05469\n",
       "26        stay        stand     0.06667    0.01345     0.08887     0.05165\n",
       "4   definitely       really     0.23333   -0.00183    -0.02664     0.02289\n",
       "1      vibrant      dynamic     0.10000    0.00217     0.07067     0.02240\n",
       "5   definitely        truly     0.16667    0.00006    -0.00334     0.01669\n",
       "0      vibrant        super     0.13333   -0.00069    -0.00136    -0.00052\n",
       "27         let         pass    -0.16667    0.00010    -0.04898    -0.02053\n",
       "28         let         help    -0.20000    0.00113    -0.04099    -0.09573\n",
       "2      vibrant     exciting    -0.16667    0.00283    -0.04501    -0.09862"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treat_pd.sort(columns=['csf_effect'],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "airbnb_sents = [\n",
    "    'I don\\'t suggest long walks after dark, but I would definitely not let this neighborhood discourage your stay, it\\'s vibrant, fun and exciting. ',\n",
    "    'If you prefer to enjoy night life then you\\' ll like the location even more as every famous club and bar in Hollywood is located in walking distance from the apartment.',\n",
    "    'The neighborhood features notable historical landmarks that include the 1836 Clarke House, one of Chicago’s oldest residences; a diverse dining scene; blues clubs and other nightlife options; and the convenience of the Museum Campus and Loop and McCormick Place a short stroll away.\tThe neighborhood features unique historical landmarks that include the 1836 Clarke House, one of Chicago’s oldest residences; a diverse dining scene; blues clubs and other nightlife options; and the convenience of the Museum Campus and Loop and McCormick Place a short stroll away.',\n",
    "    'We\\'re a mile or so to great bars (Bacchanal, Vaughan\\'s, BJ\\'s), coffee shops (Solo, Satsuma), and cheap eats (Sneaky Pickle, Pizza Delicious, The Joint); as well as the art and music venues of St. Claude Avenue.',\n",
    "    'If you want great (and cheap) tacos, just walk about three minutes towards Oxnard St. to San Marcos.',\n",
    "    'Quiet but also in the middle of Hollywood, one side accessible to the hub of Hollywood movie premieres, Jimmy Kimmel, Hotel Roosevelt and renowned locations, one side accessible to tranquility of Runyon Canyon.'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(723297, 9)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pd = pd.DataFrame(pickle.load(open(effect_file,'rb')))\n",
    "test_pd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There is also a Whole Foods grocery store 5 min (0.3 mile) walk northeast, or a Jewell-Osco grocery and pharmacy 3 min - 2 red line stops south on Berwyn.'"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pd[(test_pd.source =='store') & (test_pd.target == 'boutique')].sort(['csf_effect'],ascending=True).iloc[1].sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#knn: exciting -> interesting, vibrant -> dynamic, definitely -> truely\n",
    "#vt: exciting -> stunning, vibrant -> dynamic, definitely -> really\n",
    "#ctf: exciting -> spectacular, vibrant -> dynamic, definitely -> absolutely\n",
    "#csf: exciting -> spectacular, vibrant -> dynamic, definitely -> absolutely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#knn: cheap -> inexpensive, shops -> stores, venues -> locations\n",
    "#vt: cheap -> inexpensive, shops -> stores, venues -> locations\n",
    "#ctf: cheap -> inexpensive, shops -> boutiques, venues -> locations\n",
    "#csf: cheap -> inexpensive, shops -> boutiques, venues -> sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "effect_file = airbnb_path+'5_Select/airbnb_wdpair_sents_4effects_posinters_poscom1_bigramcheck_limit.pickle'\n",
    "wdpair_file = airbnb_path+'5_Select/6_PairRank/0_1/airbnb_pairrank_avgeffect_coef.csv'\n",
    "#treat_pd = get_treat_words(effect_file,wdpair_file,airbnb_sents[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [],
   "source": [
    "#treat_pd.sort(columns=['knn_effect'],ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tw_sents = [\n",
    "    'Every girl I know is with it and makes nice dinners for their boyfriends while I just order pizza and drink beer with mine #sorrybabe',\n",
    "    'Hung out with this cutie and my sweetheart at lunch today . #butterfly #flowers #nature http://url',\n",
    "    'I think I like Luke Hemmings more than I\\'ve liked any of my real boyfriends ... Lol',\n",
    "    'I got burn holes in my hoodies all my homies think its dank .',\n",
    "    'Salty af I got no classes with my dude @user this shit gonna be tough lol'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#knn: boyfriends -> buddies, nice -> good\n",
    "#vt: boyfriends -> buddies, beer -> brew\n",
    "#ctf: boyfriends -> buddies, beer -> brew, nice -> good\n",
    "#csf: boyfriends -> buddies, beer -> brew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "effect_file = tw_path+'5_Select/tw_wdpair_sents_4effects_posinters_poscom1_bigramcheck_limit.pickle'\n",
    "wdpair_file = tw_path+'5_Select/6_PairRank/0_1/tw_pairrank_avgeffect_coef.csv'\n",
    "treat_pd = get_treat_words(effect_file,wdpair_file,tw_sents[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>knn_effect</th>\n",
       "      <th>vt_effect</th>\n",
       "      <th>ctf_effect</th>\n",
       "      <th>csf_effect</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tough</td>\n",
       "      <td>strict</td>\n",
       "      <td>-0.37143</td>\n",
       "      <td>-0.09055</td>\n",
       "      <td>-0.34185</td>\n",
       "      <td>-0.65278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tough</td>\n",
       "      <td>hard</td>\n",
       "      <td>-0.13333</td>\n",
       "      <td>-0.02964</td>\n",
       "      <td>-0.08774</td>\n",
       "      <td>-0.20720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dude</td>\n",
       "      <td>man</td>\n",
       "      <td>0.03333</td>\n",
       "      <td>-0.01117</td>\n",
       "      <td>-0.04092</td>\n",
       "      <td>-0.04794</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  source  target  knn_effect  vt_effect  ctf_effect  csf_effect\n",
       "1  tough  strict    -0.37143   -0.09055    -0.34185    -0.65278\n",
       "0  tough    hard    -0.13333   -0.02964    -0.08774    -0.20720\n",
       "2   dude     man     0.03333   -0.01117    -0.04092    -0.04794"
      ]
     },
     "execution_count": 500,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treat_pd.sort(columns=['knn_effect'],ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yp_sents = [\n",
    "    'There are different bar areas , the waterfall is gorgeous !',\n",
    "    'I came for my birthday on a 3 night comp ... my room was gorgeous , the bathroom was fit for a queen , and the service was excellent .',\n",
    "    'It \\'s gorgeous inside so I usually end up taking a bunch of pictures .',\n",
    "    #'I ordered the vegetarian breakfast and was not disappointed -- great portion and fabulous taste .',\n",
    "    'Dr. Newland was so thorough in his workup and care .',\n",
    "    #'The salesperson shows me the item but walks away and answers me while he is still walking away .',\n",
    "    'one morning I came here and wanted this crazy alcoholic coffee concoction my buddy told me about , its basically 10 % coffee and 90 % booze .',\n",
    "    'My buddy had the Original `` G '' spicy Po boy , and it was also tasty .',\n",
    "    'Tasty tasty tasty .',\n",
    "    'Very fresh , and tasty herbs and spring rolls as well !'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#knn: different -> alternative, gorgeous -> terrific\n",
    "#vt: different -> alternative, gorgeous -> terrific\n",
    "#ctf: different -> typical, gorgeous -> terrific\n",
    "#csf: different -> alternative, gorgeous -> outstanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "effect_file = yp_path+'5_Select/yp_wdpair_sents_4effects_posinters_poscom1_bigramcheck_limit.pickle'\n",
    "effect_df = pd.DataFrame(pickle.load(open(effect_file,'rb')))\n",
    "wdpair_file = yp_path+'5_Select/6_PairRank/0_1/yp_pairrank_avgeffect_coef.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>csf_effect</th>\n",
       "      <th>ctf_effect</th>\n",
       "      <th>id</th>\n",
       "      <th>knn_effect</th>\n",
       "      <th>sentence</th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>true_y</th>\n",
       "      <th>vt_effect</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1543638</th>\n",
       "      <td>0.31043</td>\n",
       "      <td>0.27833</td>\n",
       "      <td>1543638</td>\n",
       "      <td>0.23333</td>\n",
       "      <td>The server , who was very cute and attractive ...</td>\n",
       "      <td>cute</td>\n",
       "      <td>attractive</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1543529</th>\n",
       "      <td>0.40095</td>\n",
       "      <td>0.26853</td>\n",
       "      <td>1543529</td>\n",
       "      <td>0.30000</td>\n",
       "      <td>Go and get your drink on , they got a cute vie...</td>\n",
       "      <td>cute</td>\n",
       "      <td>attractive</td>\n",
       "      <td>1</td>\n",
       "      <td>0.06801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1543427</th>\n",
       "      <td>0.42040</td>\n",
       "      <td>0.26442</td>\n",
       "      <td>1543427</td>\n",
       "      <td>0.26667</td>\n",
       "      <td>Our cute Long Island native , Mary suggested t...</td>\n",
       "      <td>cute</td>\n",
       "      <td>attractive</td>\n",
       "      <td>1</td>\n",
       "      <td>0.07196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1544210</th>\n",
       "      <td>0.35726</td>\n",
       "      <td>0.27552</td>\n",
       "      <td>1544210</td>\n",
       "      <td>0.20000</td>\n",
       "      <td>Not very busy the night we were there ; lots o...</td>\n",
       "      <td>cute</td>\n",
       "      <td>attractive</td>\n",
       "      <td>0</td>\n",
       "      <td>0.07201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1544221</th>\n",
       "      <td>0.32231</td>\n",
       "      <td>0.31974</td>\n",
       "      <td>1544221</td>\n",
       "      <td>0.36667</td>\n",
       "      <td>The veggies were great as well as the tofu and...</td>\n",
       "      <td>cute</td>\n",
       "      <td>attractive</td>\n",
       "      <td>0</td>\n",
       "      <td>0.07230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1543837</th>\n",
       "      <td>0.30839</td>\n",
       "      <td>0.37658</td>\n",
       "      <td>1543837</td>\n",
       "      <td>0.43333</td>\n",
       "      <td>Very tasty food , fine -LRB- not amazing -RRB-...</td>\n",
       "      <td>cute</td>\n",
       "      <td>attractive</td>\n",
       "      <td>0</td>\n",
       "      <td>0.07883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1543548</th>\n",
       "      <td>0.39580</td>\n",
       "      <td>0.26169</td>\n",
       "      <td>1543548</td>\n",
       "      <td>0.30000</td>\n",
       "      <td>The wait staff are friendly and cute , our fav...</td>\n",
       "      <td>cute</td>\n",
       "      <td>attractive</td>\n",
       "      <td>1</td>\n",
       "      <td>0.08000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1544436</th>\n",
       "      <td>0.25297</td>\n",
       "      <td>0.28698</td>\n",
       "      <td>1544436</td>\n",
       "      <td>0.36667</td>\n",
       "      <td>Smoking inside and there is a cute bar outside...</td>\n",
       "      <td>cute</td>\n",
       "      <td>attractive</td>\n",
       "      <td>0</td>\n",
       "      <td>0.08115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1543694</th>\n",
       "      <td>0.34551</td>\n",
       "      <td>0.29225</td>\n",
       "      <td>1543694</td>\n",
       "      <td>0.33333</td>\n",
       "      <td>I will be coming back to try out a few other m...</td>\n",
       "      <td>cute</td>\n",
       "      <td>attractive</td>\n",
       "      <td>0</td>\n",
       "      <td>0.08176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1543417</th>\n",
       "      <td>0.40486</td>\n",
       "      <td>0.33363</td>\n",
       "      <td>1543417</td>\n",
       "      <td>0.46667</td>\n",
       "      <td>This location is great as far as customer serv...</td>\n",
       "      <td>cute</td>\n",
       "      <td>attractive</td>\n",
       "      <td>1</td>\n",
       "      <td>0.08429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1543399</th>\n",
       "      <td>0.34332</td>\n",
       "      <td>0.27051</td>\n",
       "      <td>1543399</td>\n",
       "      <td>0.36667</td>\n",
       "      <td>I 've been back 3 or 4 times since I first dro...</td>\n",
       "      <td>cute</td>\n",
       "      <td>attractive</td>\n",
       "      <td>1</td>\n",
       "      <td>0.08547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1543885</th>\n",
       "      <td>0.26180</td>\n",
       "      <td>0.31539</td>\n",
       "      <td>1543885</td>\n",
       "      <td>0.33333</td>\n",
       "      <td>It 's all really high platforms the cute bilin...</td>\n",
       "      <td>cute</td>\n",
       "      <td>attractive</td>\n",
       "      <td>0</td>\n",
       "      <td>0.08575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1543487</th>\n",
       "      <td>0.28740</td>\n",
       "      <td>0.30646</td>\n",
       "      <td>1543487</td>\n",
       "      <td>0.43333</td>\n",
       "      <td>From the looks of it , it has that wilderness ...</td>\n",
       "      <td>cute</td>\n",
       "      <td>attractive</td>\n",
       "      <td>1</td>\n",
       "      <td>0.08626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1543410</th>\n",
       "      <td>0.37678</td>\n",
       "      <td>0.25547</td>\n",
       "      <td>1543410</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>Good Burgers , Good Wings , Cold Beer , Nicely...</td>\n",
       "      <td>cute</td>\n",
       "      <td>attractive</td>\n",
       "      <td>1</td>\n",
       "      <td>0.08647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1543559</th>\n",
       "      <td>0.32937</td>\n",
       "      <td>0.30089</td>\n",
       "      <td>1543559</td>\n",
       "      <td>0.23333</td>\n",
       "      <td>Came in for breakfast/lunch and ended up getti...</td>\n",
       "      <td>cute</td>\n",
       "      <td>attractive</td>\n",
       "      <td>1</td>\n",
       "      <td>0.08828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1543477</th>\n",
       "      <td>0.40017</td>\n",
       "      <td>0.33500</td>\n",
       "      <td>1543477</td>\n",
       "      <td>0.36667</td>\n",
       "      <td>We get there and there is no line but we see p...</td>\n",
       "      <td>cute</td>\n",
       "      <td>attractive</td>\n",
       "      <td>1</td>\n",
       "      <td>0.08846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1544087</th>\n",
       "      <td>0.37820</td>\n",
       "      <td>0.34218</td>\n",
       "      <td>1544087</td>\n",
       "      <td>0.26667</td>\n",
       "      <td>The attire is cute but casual , do n't have to...</td>\n",
       "      <td>cute</td>\n",
       "      <td>attractive</td>\n",
       "      <td>0</td>\n",
       "      <td>0.09051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1543446</th>\n",
       "      <td>0.37393</td>\n",
       "      <td>0.24857</td>\n",
       "      <td>1543446</td>\n",
       "      <td>0.33333</td>\n",
       "      <td>From the incredibly cute waitresses -LRB- for ...</td>\n",
       "      <td>cute</td>\n",
       "      <td>attractive</td>\n",
       "      <td>1</td>\n",
       "      <td>0.09101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1543535</th>\n",
       "      <td>0.34442</td>\n",
       "      <td>0.32337</td>\n",
       "      <td>1543535</td>\n",
       "      <td>0.30000</td>\n",
       "      <td>-LRB- The wait staff are actually pretty cute ...</td>\n",
       "      <td>cute</td>\n",
       "      <td>attractive</td>\n",
       "      <td>1</td>\n",
       "      <td>0.09155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1543622</th>\n",
       "      <td>0.36190</td>\n",
       "      <td>0.30562</td>\n",
       "      <td>1543622</td>\n",
       "      <td>0.26667</td>\n",
       "      <td>The girls that work at this location are all s...</td>\n",
       "      <td>cute</td>\n",
       "      <td>attractive</td>\n",
       "      <td>1</td>\n",
       "      <td>0.09228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1544072</th>\n",
       "      <td>0.40135</td>\n",
       "      <td>0.30002</td>\n",
       "      <td>1544072</td>\n",
       "      <td>0.40000</td>\n",
       "      <td>People observing will just stare , comment on ...</td>\n",
       "      <td>cute</td>\n",
       "      <td>attractive</td>\n",
       "      <td>0</td>\n",
       "      <td>0.09230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1544521</th>\n",
       "      <td>0.38685</td>\n",
       "      <td>0.38058</td>\n",
       "      <td>1544521</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>The restaurant is located in `` Las Vegas Town...</td>\n",
       "      <td>cute</td>\n",
       "      <td>attractive</td>\n",
       "      <td>0</td>\n",
       "      <td>0.09230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1544389</th>\n",
       "      <td>0.44984</td>\n",
       "      <td>0.22455</td>\n",
       "      <td>1544389</td>\n",
       "      <td>0.36667</td>\n",
       "      <td>I kind of felt badly for Claire because she 's...</td>\n",
       "      <td>cute</td>\n",
       "      <td>attractive</td>\n",
       "      <td>0</td>\n",
       "      <td>0.09318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1544205</th>\n",
       "      <td>0.41380</td>\n",
       "      <td>0.32485</td>\n",
       "      <td>1544205</td>\n",
       "      <td>0.40000</td>\n",
       "      <td>I always love going to H&amp;M because their cloth...</td>\n",
       "      <td>cute</td>\n",
       "      <td>attractive</td>\n",
       "      <td>0</td>\n",
       "      <td>0.09425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1544319</th>\n",
       "      <td>0.30920</td>\n",
       "      <td>0.28537</td>\n",
       "      <td>1544319</td>\n",
       "      <td>0.20000</td>\n",
       "      <td>I 've also been here for drinks before , which...</td>\n",
       "      <td>cute</td>\n",
       "      <td>attractive</td>\n",
       "      <td>0</td>\n",
       "      <td>0.09459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1544283</th>\n",
       "      <td>0.41758</td>\n",
       "      <td>0.35529</td>\n",
       "      <td>1544283</td>\n",
       "      <td>0.36667</td>\n",
       "      <td>The waitresses were wonderful and cute , and t...</td>\n",
       "      <td>cute</td>\n",
       "      <td>attractive</td>\n",
       "      <td>0</td>\n",
       "      <td>0.09684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1544315</th>\n",
       "      <td>0.38334</td>\n",
       "      <td>0.35315</td>\n",
       "      <td>1544315</td>\n",
       "      <td>0.26667</td>\n",
       "      <td>But there is ALWAYS a long line to wait in , a...</td>\n",
       "      <td>cute</td>\n",
       "      <td>attractive</td>\n",
       "      <td>0</td>\n",
       "      <td>0.09694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1543696</th>\n",
       "      <td>0.42384</td>\n",
       "      <td>0.32145</td>\n",
       "      <td>1543696</td>\n",
       "      <td>0.13333</td>\n",
       "      <td>As I was handing the menu to my boyfriend , sh...</td>\n",
       "      <td>cute</td>\n",
       "      <td>attractive</td>\n",
       "      <td>0</td>\n",
       "      <td>0.09702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1544003</th>\n",
       "      <td>0.21573</td>\n",
       "      <td>0.27633</td>\n",
       "      <td>1544003</td>\n",
       "      <td>0.16667</td>\n",
       "      <td>First off - our waitress -LRB- cute lady with ...</td>\n",
       "      <td>cute</td>\n",
       "      <td>attractive</td>\n",
       "      <td>0</td>\n",
       "      <td>0.09742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1543509</th>\n",
       "      <td>0.32076</td>\n",
       "      <td>0.33802</td>\n",
       "      <td>1543509</td>\n",
       "      <td>0.10000</td>\n",
       "      <td>All in all , the decor is upscale yet comforta...</td>\n",
       "      <td>cute</td>\n",
       "      <td>attractive</td>\n",
       "      <td>1</td>\n",
       "      <td>0.09755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1544598</th>\n",
       "      <td>0.39090</td>\n",
       "      <td>0.45249</td>\n",
       "      <td>1544598</td>\n",
       "      <td>0.30000</td>\n",
       "      <td>Lefty is cute &amp; funny &amp; Chloe is gorgeous &amp; en...</td>\n",
       "      <td>cute</td>\n",
       "      <td>attractive</td>\n",
       "      <td>0</td>\n",
       "      <td>0.22717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1543806</th>\n",
       "      <td>0.42789</td>\n",
       "      <td>0.45690</td>\n",
       "      <td>1543806</td>\n",
       "      <td>0.66667</td>\n",
       "      <td>Such a cute little place .</td>\n",
       "      <td>cute</td>\n",
       "      <td>attractive</td>\n",
       "      <td>0</td>\n",
       "      <td>0.22722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1543512</th>\n",
       "      <td>0.43739</td>\n",
       "      <td>0.47188</td>\n",
       "      <td>1543512</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>Clean , and smartly decorated , with comfortab...</td>\n",
       "      <td>cute</td>\n",
       "      <td>attractive</td>\n",
       "      <td>1</td>\n",
       "      <td>0.22738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1544181</th>\n",
       "      <td>0.44337</td>\n",
       "      <td>0.48173</td>\n",
       "      <td>1544181</td>\n",
       "      <td>0.80000</td>\n",
       "      <td>they wear a cute outfit .</td>\n",
       "      <td>cute</td>\n",
       "      <td>attractive</td>\n",
       "      <td>0</td>\n",
       "      <td>0.22782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1543515</th>\n",
       "      <td>0.41854</td>\n",
       "      <td>0.47577</td>\n",
       "      <td>1543515</td>\n",
       "      <td>0.56667</td>\n",
       "      <td>A cute little place with lovely service .</td>\n",
       "      <td>cute</td>\n",
       "      <td>attractive</td>\n",
       "      <td>1</td>\n",
       "      <td>0.22785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1544607</th>\n",
       "      <td>0.35482</td>\n",
       "      <td>0.44114</td>\n",
       "      <td>1544607</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>In addition to a huge collection of limited ed...</td>\n",
       "      <td>cute</td>\n",
       "      <td>attractive</td>\n",
       "      <td>0</td>\n",
       "      <td>0.22805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1543444</th>\n",
       "      <td>0.42444</td>\n",
       "      <td>0.45763</td>\n",
       "      <td>1543444</td>\n",
       "      <td>0.73333</td>\n",
       "      <td>Such a cute place .</td>\n",
       "      <td>cute</td>\n",
       "      <td>attractive</td>\n",
       "      <td>1</td>\n",
       "      <td>0.22821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1544192</th>\n",
       "      <td>0.43499</td>\n",
       "      <td>0.49977</td>\n",
       "      <td>1544192</td>\n",
       "      <td>0.56667</td>\n",
       "      <td>they had the cutest clothes , magnets , everyt...</td>\n",
       "      <td>cute</td>\n",
       "      <td>attractive</td>\n",
       "      <td>0</td>\n",
       "      <td>0.22838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1544068</th>\n",
       "      <td>0.48612</td>\n",
       "      <td>0.48754</td>\n",
       "      <td>1544068</td>\n",
       "      <td>0.30000</td>\n",
       "      <td>The bun was cute with the U on it .</td>\n",
       "      <td>cute</td>\n",
       "      <td>attractive</td>\n",
       "      <td>0</td>\n",
       "      <td>0.22868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1544027</th>\n",
       "      <td>0.41524</td>\n",
       "      <td>0.43840</td>\n",
       "      <td>1544027</td>\n",
       "      <td>0.36667</td>\n",
       "      <td>The lake cabin style decor is cute and relaxing .</td>\n",
       "      <td>cute</td>\n",
       "      <td>attractive</td>\n",
       "      <td>0</td>\n",
       "      <td>0.22885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1543816</th>\n",
       "      <td>0.39138</td>\n",
       "      <td>0.45691</td>\n",
       "      <td>1543816</td>\n",
       "      <td>0.46667</td>\n",
       "      <td>The place was cute .</td>\n",
       "      <td>cute</td>\n",
       "      <td>attractive</td>\n",
       "      <td>0</td>\n",
       "      <td>0.22887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1543920</th>\n",
       "      <td>0.43040</td>\n",
       "      <td>0.47432</td>\n",
       "      <td>1543920</td>\n",
       "      <td>0.60000</td>\n",
       "      <td>In fact , it 's a cute place for a date night !</td>\n",
       "      <td>cute</td>\n",
       "      <td>attractive</td>\n",
       "      <td>0</td>\n",
       "      <td>0.22923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1544001</th>\n",
       "      <td>0.42834</td>\n",
       "      <td>0.44650</td>\n",
       "      <td>1544001</td>\n",
       "      <td>0.56667</td>\n",
       "      <td>It 's really cute place .</td>\n",
       "      <td>cute</td>\n",
       "      <td>attractive</td>\n",
       "      <td>0</td>\n",
       "      <td>0.23014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1543850</th>\n",
       "      <td>0.41600</td>\n",
       "      <td>0.43657</td>\n",
       "      <td>1543850</td>\n",
       "      <td>0.36667</td>\n",
       "      <td>Super cute , and eclectic .</td>\n",
       "      <td>cute</td>\n",
       "      <td>attractive</td>\n",
       "      <td>0</td>\n",
       "      <td>0.23026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1543955</th>\n",
       "      <td>0.42446</td>\n",
       "      <td>0.44880</td>\n",
       "      <td>1543955</td>\n",
       "      <td>0.70000</td>\n",
       "      <td>Totally cute place .</td>\n",
       "      <td>cute</td>\n",
       "      <td>attractive</td>\n",
       "      <td>0</td>\n",
       "      <td>0.23030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1543849</th>\n",
       "      <td>0.33804</td>\n",
       "      <td>0.47618</td>\n",
       "      <td>1543849</td>\n",
       "      <td>0.53333</td>\n",
       "      <td>And not just ugly granny shoes , but really cu...</td>\n",
       "      <td>cute</td>\n",
       "      <td>attractive</td>\n",
       "      <td>0</td>\n",
       "      <td>0.23067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1543904</th>\n",
       "      <td>0.42812</td>\n",
       "      <td>0.44484</td>\n",
       "      <td>1543904</td>\n",
       "      <td>0.60000</td>\n",
       "      <td>It 's a really cute and small place .</td>\n",
       "      <td>cute</td>\n",
       "      <td>attractive</td>\n",
       "      <td>0</td>\n",
       "      <td>0.23088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1543847</th>\n",
       "      <td>0.44090</td>\n",
       "      <td>0.47746</td>\n",
       "      <td>1543847</td>\n",
       "      <td>0.63333</td>\n",
       "      <td>They have a concrete floor and cute little cha...</td>\n",
       "      <td>cute</td>\n",
       "      <td>attractive</td>\n",
       "      <td>0</td>\n",
       "      <td>0.23150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1543961</th>\n",
       "      <td>0.43233</td>\n",
       "      <td>0.43253</td>\n",
       "      <td>1543961</td>\n",
       "      <td>0.40000</td>\n",
       "      <td>Such a fun and cute idea !!</td>\n",
       "      <td>cute</td>\n",
       "      <td>attractive</td>\n",
       "      <td>0</td>\n",
       "      <td>0.23163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1543469</th>\n",
       "      <td>0.40523</td>\n",
       "      <td>0.47088</td>\n",
       "      <td>1543469</td>\n",
       "      <td>0.40000</td>\n",
       "      <td>4.5 Flight of fries was cute and noshy .</td>\n",
       "      <td>cute</td>\n",
       "      <td>attractive</td>\n",
       "      <td>1</td>\n",
       "      <td>0.23227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1543479</th>\n",
       "      <td>0.41339</td>\n",
       "      <td>0.45155</td>\n",
       "      <td>1543479</td>\n",
       "      <td>0.43333</td>\n",
       "      <td>Train ride is cute and realistic .</td>\n",
       "      <td>cute</td>\n",
       "      <td>attractive</td>\n",
       "      <td>1</td>\n",
       "      <td>0.23305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1544629</th>\n",
       "      <td>0.37034</td>\n",
       "      <td>0.44608</td>\n",
       "      <td>1544629</td>\n",
       "      <td>0.43333</td>\n",
       "      <td>It was such a cute touch and a great way to en...</td>\n",
       "      <td>cute</td>\n",
       "      <td>attractive</td>\n",
       "      <td>0</td>\n",
       "      <td>0.23354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1543621</th>\n",
       "      <td>0.38270</td>\n",
       "      <td>0.45576</td>\n",
       "      <td>1543621</td>\n",
       "      <td>0.30000</td>\n",
       "      <td>I was n't expecting it to be cute</td>\n",
       "      <td>cute</td>\n",
       "      <td>attractive</td>\n",
       "      <td>1</td>\n",
       "      <td>0.23464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1544024</th>\n",
       "      <td>0.43340</td>\n",
       "      <td>0.43331</td>\n",
       "      <td>1544024</td>\n",
       "      <td>0.70000</td>\n",
       "      <td>cute little spot .</td>\n",
       "      <td>cute</td>\n",
       "      <td>attractive</td>\n",
       "      <td>0</td>\n",
       "      <td>0.23678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1544005</th>\n",
       "      <td>0.41544</td>\n",
       "      <td>0.43202</td>\n",
       "      <td>1544005</td>\n",
       "      <td>0.40000</td>\n",
       "      <td>The joint is cute and clean and parking is a b...</td>\n",
       "      <td>cute</td>\n",
       "      <td>attractive</td>\n",
       "      <td>0</td>\n",
       "      <td>0.23819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1543455</th>\n",
       "      <td>0.40226</td>\n",
       "      <td>0.45833</td>\n",
       "      <td>1543455</td>\n",
       "      <td>0.43333</td>\n",
       "      <td>That is n't ` cute ' .</td>\n",
       "      <td>cute</td>\n",
       "      <td>attractive</td>\n",
       "      <td>1</td>\n",
       "      <td>0.23937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1543459</th>\n",
       "      <td>0.45489</td>\n",
       "      <td>0.49468</td>\n",
       "      <td>1543459</td>\n",
       "      <td>0.40000</td>\n",
       "      <td>The other patrons were cute and fun .</td>\n",
       "      <td>cute</td>\n",
       "      <td>attractive</td>\n",
       "      <td>1</td>\n",
       "      <td>0.23958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1544593</th>\n",
       "      <td>0.43855</td>\n",
       "      <td>0.45643</td>\n",
       "      <td>1544593</td>\n",
       "      <td>0.56667</td>\n",
       "      <td>It 's really cute inside .</td>\n",
       "      <td>cute</td>\n",
       "      <td>attractive</td>\n",
       "      <td>0</td>\n",
       "      <td>0.24000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1543851</th>\n",
       "      <td>0.43674</td>\n",
       "      <td>0.45052</td>\n",
       "      <td>1543851</td>\n",
       "      <td>0.63333</td>\n",
       "      <td>It 's small and cute .</td>\n",
       "      <td>cute</td>\n",
       "      <td>attractive</td>\n",
       "      <td>0</td>\n",
       "      <td>0.24068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1543899</th>\n",
       "      <td>0.43873</td>\n",
       "      <td>0.44942</td>\n",
       "      <td>1543899</td>\n",
       "      <td>0.36667</td>\n",
       "      <td>it was soo cute and quaint .</td>\n",
       "      <td>cute</td>\n",
       "      <td>attractive</td>\n",
       "      <td>0</td>\n",
       "      <td>0.24104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1031 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         csf_effect  ctf_effect       id  knn_effect  \\\n",
       "1543638     0.31043     0.27833  1543638     0.23333   \n",
       "1543529     0.40095     0.26853  1543529     0.30000   \n",
       "1543427     0.42040     0.26442  1543427     0.26667   \n",
       "1544210     0.35726     0.27552  1544210     0.20000   \n",
       "1544221     0.32231     0.31974  1544221     0.36667   \n",
       "1543837     0.30839     0.37658  1543837     0.43333   \n",
       "1543548     0.39580     0.26169  1543548     0.30000   \n",
       "1544436     0.25297     0.28698  1544436     0.36667   \n",
       "1543694     0.34551     0.29225  1543694     0.33333   \n",
       "1543417     0.40486     0.33363  1543417     0.46667   \n",
       "1543399     0.34332     0.27051  1543399     0.36667   \n",
       "1543885     0.26180     0.31539  1543885     0.33333   \n",
       "1543487     0.28740     0.30646  1543487     0.43333   \n",
       "1543410     0.37678     0.25547  1543410     0.50000   \n",
       "1543559     0.32937     0.30089  1543559     0.23333   \n",
       "1543477     0.40017     0.33500  1543477     0.36667   \n",
       "1544087     0.37820     0.34218  1544087     0.26667   \n",
       "1543446     0.37393     0.24857  1543446     0.33333   \n",
       "1543535     0.34442     0.32337  1543535     0.30000   \n",
       "1543622     0.36190     0.30562  1543622     0.26667   \n",
       "1544072     0.40135     0.30002  1544072     0.40000   \n",
       "1544521     0.38685     0.38058  1544521     0.50000   \n",
       "1544389     0.44984     0.22455  1544389     0.36667   \n",
       "1544205     0.41380     0.32485  1544205     0.40000   \n",
       "1544319     0.30920     0.28537  1544319     0.20000   \n",
       "1544283     0.41758     0.35529  1544283     0.36667   \n",
       "1544315     0.38334     0.35315  1544315     0.26667   \n",
       "1543696     0.42384     0.32145  1543696     0.13333   \n",
       "1544003     0.21573     0.27633  1544003     0.16667   \n",
       "1543509     0.32076     0.33802  1543509     0.10000   \n",
       "...             ...         ...      ...         ...   \n",
       "1544598     0.39090     0.45249  1544598     0.30000   \n",
       "1543806     0.42789     0.45690  1543806     0.66667   \n",
       "1543512     0.43739     0.47188  1543512     0.50000   \n",
       "1544181     0.44337     0.48173  1544181     0.80000   \n",
       "1543515     0.41854     0.47577  1543515     0.56667   \n",
       "1544607     0.35482     0.44114  1544607     0.50000   \n",
       "1543444     0.42444     0.45763  1543444     0.73333   \n",
       "1544192     0.43499     0.49977  1544192     0.56667   \n",
       "1544068     0.48612     0.48754  1544068     0.30000   \n",
       "1544027     0.41524     0.43840  1544027     0.36667   \n",
       "1543816     0.39138     0.45691  1543816     0.46667   \n",
       "1543920     0.43040     0.47432  1543920     0.60000   \n",
       "1544001     0.42834     0.44650  1544001     0.56667   \n",
       "1543850     0.41600     0.43657  1543850     0.36667   \n",
       "1543955     0.42446     0.44880  1543955     0.70000   \n",
       "1543849     0.33804     0.47618  1543849     0.53333   \n",
       "1543904     0.42812     0.44484  1543904     0.60000   \n",
       "1543847     0.44090     0.47746  1543847     0.63333   \n",
       "1543961     0.43233     0.43253  1543961     0.40000   \n",
       "1543469     0.40523     0.47088  1543469     0.40000   \n",
       "1543479     0.41339     0.45155  1543479     0.43333   \n",
       "1544629     0.37034     0.44608  1544629     0.43333   \n",
       "1543621     0.38270     0.45576  1543621     0.30000   \n",
       "1544024     0.43340     0.43331  1544024     0.70000   \n",
       "1544005     0.41544     0.43202  1544005     0.40000   \n",
       "1543455     0.40226     0.45833  1543455     0.43333   \n",
       "1543459     0.45489     0.49468  1543459     0.40000   \n",
       "1544593     0.43855     0.45643  1544593     0.56667   \n",
       "1543851     0.43674     0.45052  1543851     0.63333   \n",
       "1543899     0.43873     0.44942  1543899     0.36667   \n",
       "\n",
       "                                                  sentence source      target  \\\n",
       "1543638  The server , who was very cute and attractive ...   cute  attractive   \n",
       "1543529  Go and get your drink on , they got a cute vie...   cute  attractive   \n",
       "1543427  Our cute Long Island native , Mary suggested t...   cute  attractive   \n",
       "1544210  Not very busy the night we were there ; lots o...   cute  attractive   \n",
       "1544221  The veggies were great as well as the tofu and...   cute  attractive   \n",
       "1543837  Very tasty food , fine -LRB- not amazing -RRB-...   cute  attractive   \n",
       "1543548  The wait staff are friendly and cute , our fav...   cute  attractive   \n",
       "1544436  Smoking inside and there is a cute bar outside...   cute  attractive   \n",
       "1543694  I will be coming back to try out a few other m...   cute  attractive   \n",
       "1543417  This location is great as far as customer serv...   cute  attractive   \n",
       "1543399  I 've been back 3 or 4 times since I first dro...   cute  attractive   \n",
       "1543885  It 's all really high platforms the cute bilin...   cute  attractive   \n",
       "1543487  From the looks of it , it has that wilderness ...   cute  attractive   \n",
       "1543410  Good Burgers , Good Wings , Cold Beer , Nicely...   cute  attractive   \n",
       "1543559  Came in for breakfast/lunch and ended up getti...   cute  attractive   \n",
       "1543477  We get there and there is no line but we see p...   cute  attractive   \n",
       "1544087  The attire is cute but casual , do n't have to...   cute  attractive   \n",
       "1543446  From the incredibly cute waitresses -LRB- for ...   cute  attractive   \n",
       "1543535  -LRB- The wait staff are actually pretty cute ...   cute  attractive   \n",
       "1543622  The girls that work at this location are all s...   cute  attractive   \n",
       "1544072  People observing will just stare , comment on ...   cute  attractive   \n",
       "1544521  The restaurant is located in `` Las Vegas Town...   cute  attractive   \n",
       "1544389  I kind of felt badly for Claire because she 's...   cute  attractive   \n",
       "1544205  I always love going to H&M because their cloth...   cute  attractive   \n",
       "1544319  I 've also been here for drinks before , which...   cute  attractive   \n",
       "1544283  The waitresses were wonderful and cute , and t...   cute  attractive   \n",
       "1544315  But there is ALWAYS a long line to wait in , a...   cute  attractive   \n",
       "1543696  As I was handing the menu to my boyfriend , sh...   cute  attractive   \n",
       "1544003  First off - our waitress -LRB- cute lady with ...   cute  attractive   \n",
       "1543509  All in all , the decor is upscale yet comforta...   cute  attractive   \n",
       "...                                                    ...    ...         ...   \n",
       "1544598  Lefty is cute & funny & Chloe is gorgeous & en...   cute  attractive   \n",
       "1543806                         Such a cute little place .   cute  attractive   \n",
       "1543512  Clean , and smartly decorated , with comfortab...   cute  attractive   \n",
       "1544181                          they wear a cute outfit .   cute  attractive   \n",
       "1543515          A cute little place with lovely service .   cute  attractive   \n",
       "1544607  In addition to a huge collection of limited ed...   cute  attractive   \n",
       "1543444                                Such a cute place .   cute  attractive   \n",
       "1544192  they had the cutest clothes , magnets , everyt...   cute  attractive   \n",
       "1544068                The bun was cute with the U on it .   cute  attractive   \n",
       "1544027  The lake cabin style decor is cute and relaxing .   cute  attractive   \n",
       "1543816                               The place was cute .   cute  attractive   \n",
       "1543920    In fact , it 's a cute place for a date night !   cute  attractive   \n",
       "1544001                          It 's really cute place .   cute  attractive   \n",
       "1543850                        Super cute , and eclectic .   cute  attractive   \n",
       "1543955                               Totally cute place .   cute  attractive   \n",
       "1543849  And not just ugly granny shoes , but really cu...   cute  attractive   \n",
       "1543904              It 's a really cute and small place .   cute  attractive   \n",
       "1543847  They have a concrete floor and cute little cha...   cute  attractive   \n",
       "1543961                        Such a fun and cute idea !!   cute  attractive   \n",
       "1543469           4.5 Flight of fries was cute and noshy .   cute  attractive   \n",
       "1543479                 Train ride is cute and realistic .   cute  attractive   \n",
       "1544629  It was such a cute touch and a great way to en...   cute  attractive   \n",
       "1543621                  I was n't expecting it to be cute   cute  attractive   \n",
       "1544024                                 cute little spot .   cute  attractive   \n",
       "1544005  The joint is cute and clean and parking is a b...   cute  attractive   \n",
       "1543455                             That is n't ` cute ' .   cute  attractive   \n",
       "1543459              The other patrons were cute and fun .   cute  attractive   \n",
       "1544593                         It 's really cute inside .   cute  attractive   \n",
       "1543851                             It 's small and cute .   cute  attractive   \n",
       "1543899                       it was soo cute and quaint .   cute  attractive   \n",
       "\n",
       "         true_y  vt_effect  \n",
       "1543638       1    0.05278  \n",
       "1543529       1    0.06801  \n",
       "1543427       1    0.07196  \n",
       "1544210       0    0.07201  \n",
       "1544221       0    0.07230  \n",
       "1543837       0    0.07883  \n",
       "1543548       1    0.08000  \n",
       "1544436       0    0.08115  \n",
       "1543694       0    0.08176  \n",
       "1543417       1    0.08429  \n",
       "1543399       1    0.08547  \n",
       "1543885       0    0.08575  \n",
       "1543487       1    0.08626  \n",
       "1543410       1    0.08647  \n",
       "1543559       1    0.08828  \n",
       "1543477       1    0.08846  \n",
       "1544087       0    0.09051  \n",
       "1543446       1    0.09101  \n",
       "1543535       1    0.09155  \n",
       "1543622       1    0.09228  \n",
       "1544072       0    0.09230  \n",
       "1544521       0    0.09230  \n",
       "1544389       0    0.09318  \n",
       "1544205       0    0.09425  \n",
       "1544319       0    0.09459  \n",
       "1544283       0    0.09684  \n",
       "1544315       0    0.09694  \n",
       "1543696       0    0.09702  \n",
       "1544003       0    0.09742  \n",
       "1543509       1    0.09755  \n",
       "...         ...        ...  \n",
       "1544598       0    0.22717  \n",
       "1543806       0    0.22722  \n",
       "1543512       1    0.22738  \n",
       "1544181       0    0.22782  \n",
       "1543515       1    0.22785  \n",
       "1544607       0    0.22805  \n",
       "1543444       1    0.22821  \n",
       "1544192       0    0.22838  \n",
       "1544068       0    0.22868  \n",
       "1544027       0    0.22885  \n",
       "1543816       0    0.22887  \n",
       "1543920       0    0.22923  \n",
       "1544001       0    0.23014  \n",
       "1543850       0    0.23026  \n",
       "1543955       0    0.23030  \n",
       "1543849       0    0.23067  \n",
       "1543904       0    0.23088  \n",
       "1543847       0    0.23150  \n",
       "1543961       0    0.23163  \n",
       "1543469       1    0.23227  \n",
       "1543479       1    0.23305  \n",
       "1544629       0    0.23354  \n",
       "1543621       1    0.23464  \n",
       "1544024       0    0.23678  \n",
       "1544005       0    0.23819  \n",
       "1543455       1    0.23937  \n",
       "1543459       1    0.23958  \n",
       "1544593       0    0.24000  \n",
       "1543851       0    0.24068  \n",
       "1543899       0    0.24104  \n",
       "\n",
       "[1031 rows x 9 columns]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "effect_df = pd.DataFrame(pickle.load(open(effect_file,'rb')))\n",
    "st_df = effect_df[(effect_df.source=='cute') & (effect_df.target=='attractive')].sort(columns=['vt_effect'],ascending=True)\n",
    "#st_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Our cute Long Island native , Mary suggested the best things on the menu - even telling us what was off and on from the specials board that would work or not .'"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st_df.iloc[2].sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Very fresh , and tasty herbs and spring rolls as well !'"
      ]
     },
     "execution_count": 519,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "effect_df[(effect_df.source=='tasty')].sort(columns=['csf_effect']).iloc[1].sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>csf_effect</th>\n",
       "      <th>ctf_effect</th>\n",
       "      <th>id</th>\n",
       "      <th>knn_effect</th>\n",
       "      <th>sentence</th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>true_y</th>\n",
       "      <th>vt_effect</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>905001</th>\n",
       "      <td>-0.26992</td>\n",
       "      <td>-0.27337</td>\n",
       "      <td>905001</td>\n",
       "      <td>-1.00000</td>\n",
       "      <td>Great , great , great !</td>\n",
       "      <td>great</td>\n",
       "      <td>fabulous</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.20525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481044</th>\n",
       "      <td>-0.28591</td>\n",
       "      <td>-0.28453</td>\n",
       "      <td>481044</td>\n",
       "      <td>-1.00000</td>\n",
       "      <td>Tasty tasty tasty .</td>\n",
       "      <td>tasty</td>\n",
       "      <td>yummy</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.20831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1499699</th>\n",
       "      <td>-0.28051</td>\n",
       "      <td>-0.28531</td>\n",
       "      <td>1499699</td>\n",
       "      <td>-1.00000</td>\n",
       "      <td>Great , great , great !</td>\n",
       "      <td>great</td>\n",
       "      <td>gorgeous</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.21915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2234703</th>\n",
       "      <td>-0.19621</td>\n",
       "      <td>-0.17188</td>\n",
       "      <td>2234703</td>\n",
       "      <td>-0.96667</td>\n",
       "      <td>overall , delightful .</td>\n",
       "      <td>delightful</td>\n",
       "      <td>adorable</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.07908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1267190</th>\n",
       "      <td>-0.29497</td>\n",
       "      <td>-0.33908</td>\n",
       "      <td>1267190</td>\n",
       "      <td>-0.96667</td>\n",
       "      <td>A superb recommendation .</td>\n",
       "      <td>superb</td>\n",
       "      <td>gorgeous</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.20656</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         csf_effect  ctf_effect       id  knn_effect  \\\n",
       "905001     -0.26992    -0.27337   905001    -1.00000   \n",
       "481044     -0.28591    -0.28453   481044    -1.00000   \n",
       "1499699    -0.28051    -0.28531  1499699    -1.00000   \n",
       "2234703    -0.19621    -0.17188  2234703    -0.96667   \n",
       "1267190    -0.29497    -0.33908  1267190    -0.96667   \n",
       "\n",
       "                          sentence      source    target  true_y  vt_effect  \n",
       "905001     Great , great , great !       great  fabulous       0   -0.20525  \n",
       "481044         Tasty tasty tasty .       tasty     yummy       1   -0.20831  \n",
       "1499699    Great , great , great !       great  gorgeous       0   -0.21915  \n",
       "2234703     overall , delightful .  delightful  adorable       0   -0.07908  \n",
       "1267190  A superb recommendation .      superb  gorgeous       0   -0.20656  "
      ]
     },
     "execution_count": 509,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "effect_df.sort(columns=['knn_effect'],ascending=True).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>knn_effect</th>\n",
       "      <th>vt_effect</th>\n",
       "      <th>ctf_effect</th>\n",
       "      <th>csf_effect</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>tasty</td>\n",
       "      <td>yummy</td>\n",
       "      <td>-0.33333</td>\n",
       "      <td>-0.15257</td>\n",
       "      <td>-0.39367</td>\n",
       "      <td>-0.30843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>well</td>\n",
       "      <td>beautifully</td>\n",
       "      <td>-0.20000</td>\n",
       "      <td>-0.11394</td>\n",
       "      <td>-0.23093</td>\n",
       "      <td>-0.18963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tasty</td>\n",
       "      <td>delicious</td>\n",
       "      <td>-0.23333</td>\n",
       "      <td>-0.06241</td>\n",
       "      <td>-0.19966</td>\n",
       "      <td>-0.12515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tasty</td>\n",
       "      <td>super</td>\n",
       "      <td>-0.13333</td>\n",
       "      <td>-0.05382</td>\n",
       "      <td>-0.16118</td>\n",
       "      <td>-0.11619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tasty</td>\n",
       "      <td>delightful</td>\n",
       "      <td>-0.23333</td>\n",
       "      <td>-0.06429</td>\n",
       "      <td>-0.20855</td>\n",
       "      <td>-0.09204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tasty</td>\n",
       "      <td>excellent</td>\n",
       "      <td>0.06667</td>\n",
       "      <td>0.03710</td>\n",
       "      <td>0.07486</td>\n",
       "      <td>0.09396</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  source       target  knn_effect  vt_effect  ctf_effect  csf_effect\n",
       "5  tasty        yummy    -0.33333   -0.15257    -0.39367    -0.30843\n",
       "0   well  beautifully    -0.20000   -0.11394    -0.23093    -0.18963\n",
       "4  tasty    delicious    -0.23333   -0.06241    -0.19966    -0.12515\n",
       "3  tasty        super    -0.13333   -0.05382    -0.16118    -0.11619\n",
       "1  tasty   delightful    -0.23333   -0.06429    -0.20855    -0.09204\n",
       "2  tasty    excellent     0.06667    0.03710     0.07486     0.09396"
      ]
     },
     "execution_count": 526,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treat_pd = get_treat_words(effect_file,wdpair_file,yp_sents[-1])\n",
    "treat_pd.sort(columns=['csf_effect'],ascending=True)\n",
    "#treat_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- previous results for function fetch_words_forsents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76587, 0.51485 has more than one positive treatment word.\n"
     ]
    }
   ],
   "source": [
    "effect_file = yp_path+'5_Select/yp_wdpair_sents_4effects_posinters_poscom1_bigramcheck_limit.pickle'\n",
    "wdpair_file = yp_path+'5_Select/6_PairRank/0_1/yp_pairrank_avgeffect_coef.csv'\n",
    "res_file = yp_path+'5_Select/7_PairInSents/yp_pairinsents.csv'\n",
    "fetch_words_forsents(effect_file,wdpair_file,res_file)\n",
    "#139708, 0.51890 has more than one treatment word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71849, 0.52592 has more than one positive treatment word.\n"
     ]
    }
   ],
   "source": [
    "effect_file = tw_path+'5_Select/tw_wdpair_sents_4effects_posinters_poscom1_bigramcheck_limit.pickle'\n",
    "wdpair_file = tw_path+'5_Select/6_PairRank/0_1/tw_pairrank_avgeffect_coef.csv'\n",
    "res_file = tw_path+'5_Select/7_PairInSents/tw_pairinsents.csv'\n",
    "fetch_words_forsents(effect_file,wdpair_file,res_file)\n",
    "#123895, 0.52578 has more than one treatment word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14585, 0.79944 has more than one positive treatment word.\n"
     ]
    }
   ],
   "source": [
    "effect_file = airbnb_path+'5_Select/airbnb_wdpair_sents_4effects_posinters_poscom1_bigramcheck_limit.pickle'\n",
    "wdpair_file = airbnb_path+'5_Select/6_PairRank/0_1/airbnb_pairrank_avgeffect_coef.csv'\n",
    "res_file = airbnb_path+'5_Select/7_PairInSents/airbnb_pairinsents.csv'\n",
    "fetch_words_forsents(effect_file,wdpair_file,res_file)\n",
    "#82920, 0.80884 has more than one treatment word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76587, 3)"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pd = pd.read_csv(yp_path+'5_Select/7_PairInSents/yp_pairinsents.csv')\n",
    "test_pd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_src</th>\n",
       "      <th>src_words</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>['little', 'entire', 'favorite', 'portions', '...</td>\n",
       "      <td>What I ate : Alaskan crab legs -LRB- LOTS -RRB...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>['well', 'fabulous', 'everyone', 'place', 'atm...</td>\n",
       "      <td>Yummy food , fabulous coffee , great atmospher...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>['whole', 'getting', 'rude', 'birthday', 'awfu...</td>\n",
       "      <td>Once we made it through the pick-up issue and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>['tasty', 'okay', 'such', 'great', 'sure', 'fe...</td>\n",
       "      <td>I had room service there once a few times and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>['trendy', 'fine', 'cute', 'tasty', 'little', ...</td>\n",
       "      <td>Very tasty food , fine -LRB- not amazing -RRB-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   n_src                                          src_words  \\\n",
       "0     10  ['little', 'entire', 'favorite', 'portions', '...   \n",
       "1      9  ['well', 'fabulous', 'everyone', 'place', 'atm...   \n",
       "2      9  ['whole', 'getting', 'rude', 'birthday', 'awfu...   \n",
       "3      8  ['tasty', 'okay', 'such', 'great', 'sure', 'fe...   \n",
       "4      8  ['trendy', 'fine', 'cute', 'tasty', 'little', ...   \n",
       "\n",
       "                                            sentence  \n",
       "0  What I ate : Alaskan crab legs -LRB- LOTS -RRB...  \n",
       "1  Yummy food , fabulous coffee , great atmospher...  \n",
       "2  Once we made it through the pick-up issue and ...  \n",
       "3  I had room service there once a few times and ...  \n",
       "4  Very tasty food , fine -LRB- not amazing -RRB-...  "
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.6 Spearman rank correlation for sentences and word pairs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_spearmanr(effect_file):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        Treatment effect file\n",
    "    Function:\n",
    "        Calculate spearman rank correlation between sentence treatment effect computed by every two methods among four methods.\n",
    "    Return:\n",
    "        \n",
    "    \"\"\"\n",
    "    effect_pd = pd.DataFrame(pickle.load(open(effect_file,'rb')))\n",
    "    #effect_pd = pd.read_csv(effect_file)\n",
    "    method = ['knn','vt','ctf','csf']\n",
    "    for i in range(len(method)):\n",
    "        for j in range(i+1,len(method)):\n",
    "            print(method[i]+','+method[j],spearmanr(effect_pd[method[i]+'_effect'].values,effect_pd[method[j]+'_effect'].values))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1108, 7)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "effect_file = airbnb_path+'5_Select/airbnb_wdpair_sents_4effects_posinters_poscom1_bigramcheck_limit_limitvocab.pickle'\n",
    "#effect_pd = pd.DataFrame(pickle.load(open(effect_file,'rb')))\n",
    "#effect_file = airbnb_path+'5_Select/6_PairRank/0_1/airbnb_pairrank_limitvocab.csv'\n",
    "#effect_pd = pd.read_csv(effect_file)\n",
    "effect_pd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knn,vt SpearmanrResult(correlation=0.46903437477259624, pvalue=0.0)\n",
      "knn,ctf SpearmanrResult(correlation=0.56073354989311586, pvalue=0.0)\n",
      "knn,csf SpearmanrResult(correlation=0.45515039238535887, pvalue=0.0)\n",
      "\n",
      "vt,ctf SpearmanrResult(correlation=0.82225065729983104, pvalue=0.0)\n",
      "vt,csf SpearmanrResult(correlation=0.77319946905611348, pvalue=0.0)\n",
      "\n",
      "ctf,csf SpearmanrResult(correlation=0.73341005244381385, pvalue=0.0)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cal_spearmanr(effect_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.7 Percentage of negative instances among topn high treatment sentences according to each method\n",
    "> Sort all sentence in descending order and select topn (according to each method) <br>\n",
    "> Calculate the percentage of negative instances among topn <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def neg_perct_intopn(effect_file):\n",
    "    effect_df = pd.DataFrame(pickle.load(open(effect_file,'rb')))\n",
    "    perct = []\n",
    "    for method in ['knn_effect','vt_effect','ctf_effect','csf_effect']:\n",
    "        st_effect_df = effect_df.sort([method],ascending=False)\n",
    "        topn = []\n",
    "        for n in [100,1000,10000]:\n",
    "            topn_df = st_effect_df.iloc[:n]\n",
    "            topn[n] = len(topn_df[topn_df.true_y == 0])/n\n",
    "        perct.append({method:topn})\n",
    "    return pd.DataFrame(perct,columns=['knn_effect','vt_effect','ctf_effect','csf_effect'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "this_path = airbnb_path\n",
    "prefix = \"airbnb\"\n",
    "effect_file = this_path+'5_Select/'+prefix+'_wdpair_sents_4effects_posinters_poscom1_bigramcheck_limitvocab.pickle'\n",
    "effect_df = pd.DataFrame(pickle.load(open(effect_file,'rb'))).sort(['knn_effect'],ascending=False)\n",
    "print(effect_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "effect_df.iloc[0].sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n=100\n",
    "topn_df = effect_df.iloc[:n]\n",
    "len(topn_df[topn_df.true_y == 0])/n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Revise for previous results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1 Assign new treatment effect to existing amt labeled airbnb files\n",
    "- For cases when existing word pairs and sentences have re-calculated treatment effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def assign_new_effects(airbnb_effect_file,airbnb_join_file,airbnb_neweffect):\n",
    "    \"\"\"\n",
    "    Parameters: \n",
    "        new effect file, existing word pair and sentence file, new effect file assigned to existing word pairs and sentences\n",
    "    Function:\n",
    "        For existing word pair and sentences, mapping in new effect file, and assign new effect to them.\n",
    "    Return: \n",
    "        csv file with following fields:\n",
    "        ['source','target','src_sentence','tar_sentence','knn_effect','vt_effect','ctf_effect','csf_effect',\n",
    "        'true_y','id','src_ratings','src_ratings_avg','tar_ratings','tar_ratings_avg','amt_effect']\n",
    "    \"\"\"\n",
    "    airbnb_pd = pickle.load(open(airbnb_effect_file,'rb'))\n",
    "    join_pd = pd.read_csv(airbnb_join_file)\n",
    "    \n",
    "    new_info = []\n",
    "    for idx,row in join_pd.iterrows():\n",
    "        new_row = row.to_dict()\n",
    "        new_pd = airbnb_pd[(airbnb_pd.source == row.source) & (airbnb_pd.target == row.target) & (airbnb_pd.sentence == row.src_sentence)]\n",
    "        if(new_pd.shape[0] == 1):\n",
    "            new_row['knn_effect'] = new_pd.knn_effect.values[0]\n",
    "            new_row['vt_effect'] = new_pd.vt_effect.values[0]\n",
    "            new_row['ctf_effect'] = new_pd.ctf_effect.values[0]\n",
    "            new_row['csf_effect'] = new_pd.csf_effect.values[0]\n",
    "        else:\n",
    "            print(idx,row.source,row.target,new_pd.shape)\n",
    "        \n",
    "        new_info.append(new_row)\n",
    "        \n",
    "    pd.DataFrame(new_info).to_csv(airbnb_neweffect,columns=['source','target','src_sentence','tar_sentence',\n",
    "                                                            'knn_effect','vt_effect','ctf_effect','csf_effect','true_y','id'],\n",
    "                                                            index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 exciting stunning (2, 9)\n",
      "60 amazing incredible (0, 9)\n",
      "91 yummy good (2, 9)\n",
      "101 predominantly mostly (0, 9)\n"
     ]
    }
   ],
   "source": [
    "airbnb_effect_file = airbnb_path+'5_Select/airbnb_wdpair_sents_4effects_limitvocab.pickle'\n",
    "airbnb_join_file = '/data/2/zwang/2018_S_WordTreatment/V2_AMT/6_amt/hits/airbnb_join_old.csv'\n",
    "airbnb_neweffect = '/data/2/zwang/2018_S_WordTreatment/V2_AMT/6_amt/hits/airbnb_join.csv'\n",
    "assign_new_effects(airbnb_effect_file,airbnb_join_file,airbnb_neweffect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(723297, 9)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pd = pickle.load(open(airbnb_path+'5_Select/airbnb_wdpair_sents_4effects_posinters_poscom1_bigramcheck_limit_limitvocab.pickle','rb'))\n",
    "test_pd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_str = 'From amazing restaurants to the Walk of Fame, Dolby Theater, Chinese Theater, Hollywood and Highland Shopping mall, cafes, Runyon Canyon, Hollywood Sign, Griffith Park and more!'\n",
    "my_pd = test_pd[(test_pd.source == 'amazing') & (test_pd.target == 'spectacular') & (test_pd.sentence == my_str)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>csf_effect</th>\n",
       "      <th>ctf_effect</th>\n",
       "      <th>id</th>\n",
       "      <th>knn_effect</th>\n",
       "      <th>sentence</th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>true_y</th>\n",
       "      <th>vt_effect</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1444493</th>\n",
       "      <td>0.08962</td>\n",
       "      <td>0.12736</td>\n",
       "      <td>1444493</td>\n",
       "      <td>0.06667</td>\n",
       "      <td>From amazing restaurants to the Walk of Fame, ...</td>\n",
       "      <td>amazing</td>\n",
       "      <td>spectacular</td>\n",
       "      <td>0</td>\n",
       "      <td>0.01797</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         csf_effect  ctf_effect       id  knn_effect  \\\n",
       "1444493     0.08962     0.12736  1444493     0.06667   \n",
       "\n",
       "                                                  sentence   source  \\\n",
       "1444493  From amazing restaurants to the Walk of Fame, ...  amazing   \n",
       "\n",
       "              target  true_y  vt_effect  \n",
       "1444493  spectacular       0    0.01797  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_bigram_dfdict = pickle.load(open(vocab_bigram_file,'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_bigram_dfdict['drill']['a drill']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "354"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_bigram_dfdict['buddy']['my buddy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wd_tags_file = '/data/2/zwang/2018_S_WordTreatment/V2_airbnb/1_Process/wd_tags.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wd_tags_dfdict = pickle.load(open(wd_tags_file,'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'RB': 202})"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wd_tags_dfdict['predominantly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'RB': 7180})"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wd_tags_dfdict['especially']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mystr = \"The South is the largest collection of Victorian Brownstones in the world and it is a now a trendy, gentrified neighborhood with amazing restaurants and easy access to all that Boston has to offer.\"\n",
    "'end' in mystr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'good' in \"hi Good morning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
