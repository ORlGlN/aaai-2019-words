{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New low dimensional feature matrix\n",
    "- Binary classifier vs 3-class classification <br>\n",
    "> Binary: AMT rating increases(pos) vs others(neg)<br>\n",
    "> 3-class: AMT rating increases(1), keep same(0), decreases(-1) <br>\n",
    "\n",
    "- Coefficient of source word and target word <br>\n",
    "- Number of support words and opposite words abs(coef) > 0.1 <br>\n",
    "- Sentence length <br>\n",
    "- Depth of source word and target word (VT,CTF,CSF) <br>\n",
    "- Length of path for source sentence and target sentence <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle, pydot, random, re, ast, time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "from scipy.sparse import vstack\n",
    "from sklearn.externals.six import StringIO \n",
    "from IPython.display import display\n",
    "#from sklearn.datasets import load_iris\n",
    "from sklearn import tree\n",
    "from IPython.display import Image\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "project_path = '/data/2/zwang/2018_S_WordTreatment/'\n",
    "amt_path = '/data/2/zwang/2018_S_WordTreatment/V2_AMT/6_amt/'\n",
    "dataset_list = ['yp','tw','airbnb']\n",
    "methods = ['KNN','VT','CTF','CSF']\n",
    "full_name = {'yp':'Yelp','tw':'Twitter','airbnb':'Airbnb'}\n",
    "data_file = {}\n",
    "data_file['yp'] = project_path + 'V2_yelp/5_Select/yp_wdpair_sents_4effects_posinters_poscom1_bigramcheck_limit.pickle'\n",
    "data_file['tw'] = project_path + 'V2_twitter/5_Select/tw_wdpair_sents_4effects_posinters_poscom1_bigramcheck_limit.pickle'\n",
    "data_file['airbnb'] = project_path + 'V2_airbnb/5_Select/airbnb_wdpair_sents_4effects_posinters_poscom1_bigramcheck_limit_limitvocab.pickle'\n",
    "vocab_file = project_path+'V2_airbnb/0_Data/common_wds.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cal_causal_words(data_pd,pos_thresh,neg_thresh):\n",
    "    n_sup = []\n",
    "    n_opp = []\n",
    "    for sent in data_pd.sorted_sentence.values:\n",
    "        nsp = 0\n",
    "        nop = 0\n",
    "        for item in ast.literal_eval(sent):\n",
    "            if(item[1] > pos_thresh):\n",
    "                nsp += 1\n",
    "            elif(item[1] < neg_thresh):\n",
    "                nop += 1\n",
    "        n_sup.append(nsp)\n",
    "        n_opp.append(nop)\n",
    "    data_pd['n_sup'] = n_sup\n",
    "    data_pd['n_opp'] = n_opp\n",
    "    return data_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_feats = ['src_coef','tar_coef','n_sup','n_opp','n_words','src_probLR','tar_probLR',\n",
    "             'vt_srcwd_depth','vt_tarwd_depth','vt_src_pathlen','vt_tar_pathlen',\n",
    "             'ctf_srcwd_depth','ctf_tarwd_depth','ctf_src_pathlen','ctf_tar_pathlen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_feats = ['tar_src_dif_coef','n_sup','n_opp','n_words','src_probLR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_newfeats(coef_pd,lr_file,vt_file,ctf_file):\n",
    "    lr_pd = pd.read_csv(lr_file)\n",
    "    lr_pd['n_words'] = lr_pd['src_sentence'].apply(lambda x: len(re.findall('\\w+',x)))\n",
    "    lr_pd['src_probLR'] = lr_pd['src_prob']\n",
    "    lr_pd['tar_probLR'] = lr_pd['tar_prob']\n",
    "    vt_pd = pd.read_csv(vt_file)\n",
    "    vt_pd['vt_srcwd_depth'] = vt_pd['src_depth'].apply(lambda x: np.mean(ast.literal_eval(x)))\n",
    "    vt_pd['vt_tarwd_depth'] = vt_pd['tar_depth'].apply(lambda x: np.mean(ast.literal_eval(x)))\n",
    "    vt_pd['vt_src_pathlen'] = vt_pd['src_path_len'].apply(lambda x: np.mean(ast.literal_eval(x)))\n",
    "    vt_pd['vt_tar_pathlen'] = vt_pd['tar_path_len'].apply(lambda x: np.mean(ast.literal_eval(x)))\n",
    "    ctf_pd = pd.read_csv(ctf_file)\n",
    "    ctf_pd['ctf_srcwd_depth'] = ctf_pd['src_depth'].apply(lambda x: np.mean(ast.literal_eval(x)))\n",
    "    ctf_pd['ctf_tarwd_depth'] = ctf_pd['tar_depth'].apply(lambda x: np.mean(ast.literal_eval(x)))\n",
    "    ctf_pd['ctf_src_pathlen'] = ctf_pd['src_path_len'].apply(lambda x: np.mean(ast.literal_eval(x)))\n",
    "    ctf_pd['ctf_tar_pathlen'] = ctf_pd['tar_path_len'].apply(lambda x: np.mean(ast.literal_eval(x)))\n",
    "            \n",
    "    '''\n",
    "    Has formatting problem when matching, make error on cases like 0.26666666666 and 0.26700000001.\n",
    "    '''\n",
    "    coef_lr_pd = coef_pd.merge(lr_pd,on=['id','source','target'])\n",
    "    vt_ctf_pd = vt_pd.merge(ctf_pd,on=['id','source','target']) \n",
    "    super_pd = coef_lr_pd.merge(vt_ctf_pd,on=['id','source','target'])\n",
    "    \n",
    "    for col in ['amt_effect_mostcom','knn_effect','vt_effect','ctf_effect','csf_effect']:\n",
    "        super_pd[col]=super_pd[col+'_x_x']\n",
    "    return super_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_LR_newfeats(dataset_list,new_feats,clf_flag):\n",
    "    for di in range(len(dataset_list)):\n",
    "        print(\"Processing %s ...\" % (full_name[dataset_list[di]]))\n",
    "        coef_file = project_path+'V2_'+full_name[dataset_list[di]].lower()+'/9_Coef/'+dataset_list[di]+'_sentence_coef.csv'\n",
    "        org_coef_pd = pd.read_csv(coef_file)\n",
    "        org_coef_pd['tar_src_dif_coef'] = org_coef_pd['tar_coef'] - org_coef_pd['src_coef'] \n",
    "        coef_pd = cal_causal_words(org_coef_pd,0.1,-0.1)\n",
    "        lr_file = project_path+'V2_'+full_name[dataset_list[di]].lower()+'/8_Var/'+dataset_list[di]+'_effectVar_prob_byLR.csv'\n",
    "        vt_file = project_path+'V2_'+full_name[dataset_list[di]].lower()+'/8_Var/'+dataset_list[di]+'_VT_srctar_depth3.csv'\n",
    "        ctf_file = project_path+'V2_'+full_name[dataset_list[di]].lower()+'/8_Var/'+dataset_list[di]+'_CTF_srctar_depth.csv'\n",
    "        super_pd = cal_newfeats(coef_pd,lr_file,vt_file,ctf_file)\n",
    "        print(super_pd.shape)\n",
    "        \n",
    "        # construct feature matrix\n",
    "        X_feats = []\n",
    "        for i in range(super_pd.shape[0]):\n",
    "            row = super_pd.iloc[i][new_feats].values\n",
    "            X_feats.append([float(e) for e in row])\n",
    "        \n",
    "        # fit logistic-regression or linear-regression\n",
    "        if(clf_flag == 'binary'):\n",
    "            LR_clf = LogisticRegression(random_state=42,n_jobs=-1)\n",
    "            amt_labels = super_pd['amt_effect_mostcom'].apply(lambda x: 1 if x>0 else 0)\n",
    "            LR_clf.fit(X_feats,amt_labels)\n",
    "            print(Counter(amt_labels),'%.3f' % (np.mean(cross_val_score(LR_clf, X_feats, amt_labels, cv=3))))\n",
    "            for fi in LR_clf.coef_[0].argsort()[::-1]:\n",
    "                print(\"%.3f , %s\" % (LR_clf.coef_[0][fi],new_feats[fi]))\n",
    "        \n",
    "        elif(clf_flag == '3-class'):\n",
    "            LR_clf = LogisticRegression(random_state=42,n_jobs=-1)\n",
    "            amt_labels = super_pd['amt_effect_mostcom'].apply(lambda x: 1 if x>0 else (-1 if x<0 else 0))\n",
    "            LR_clf.fit(X_feats,amt_labels)\n",
    "            print(Counter(amt_labels),'%.3f' % (np.mean(cross_val_score(LR_clf, X_feats, amt_labels, cv=3))))\n",
    "            for cls in LR_clf.classes_:\n",
    "                print(\"class:\",cls)\n",
    "                ci = LR_clf.classes_[cls]\n",
    "                for fi in LR_clf.coef_[ci].argsort()[::-1]:\n",
    "                    print(\"%.3f , %s\" % (LR_clf.coef_[ci][fi],new_feats[fi])) # 3-class\n",
    "            \n",
    "        elif(clf_flag == 'linear'):\n",
    "            for md in ['knn_effect','vt_effect','ctf_effect','csf_effect']:\n",
    "                print(md)\n",
    "                LR_clf = LinearRegression(n_jobs=-1)\n",
    "                LR_clf.fit(X_feats,super_pd[md].values)\n",
    "\n",
    "                for idx in LR_clf.coef_.argsort()[::-1]:\n",
    "                    print(\"%.3f , %s \" % (LR_clf.coef_[idx], new_feats[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result for 15 new features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Yelp ...\n",
      "(120, 95)\n",
      "Counter({1: 61, 0: 41, -1: 18}) 0.567\n",
      "class: -1\n",
      "0.146 , src_probLR\n",
      "0.104 , src_coef\n",
      "0.087 , n_sup\n",
      "0.081 , vt_src_pathlen\n",
      "0.073 , n_opp\n",
      "0.054 , ctf_tar_pathlen\n",
      "-0.010 , n_words\n",
      "-0.027 , ctf_src_pathlen\n",
      "-0.040 , vt_srcwd_depth\n",
      "-0.050 , ctf_tarwd_depth\n",
      "-0.050 , ctf_srcwd_depth\n",
      "-0.069 , vt_tar_pathlen\n",
      "-0.070 , tar_probLR\n",
      "-0.105 , vt_tarwd_depth\n",
      "-0.602 , tar_coef\n",
      "class: 0\n",
      "0.874 , ctf_tarwd_depth\n",
      "0.874 , ctf_srcwd_depth\n",
      "0.799 , tar_coef\n",
      "0.195 , n_sup\n",
      "0.158 , vt_tarwd_depth\n",
      "0.039 , ctf_src_pathlen\n",
      "0.018 , vt_src_pathlen\n",
      "0.003 , ctf_tar_pathlen\n",
      "-0.020 , n_opp\n",
      "-0.022 , vt_tar_pathlen\n",
      "-0.047 , vt_srcwd_depth\n",
      "-0.104 , n_words\n",
      "-0.132 , tar_probLR\n",
      "-0.196 , src_coef\n",
      "-0.421 , src_probLR\n",
      "class: 1\n",
      "0.255 , vt_tar_pathlen\n",
      "0.222 , src_probLR\n",
      "0.204 , n_words\n",
      "0.143 , vt_srcwd_depth\n",
      "0.084 , tar_probLR\n",
      "0.025 , src_coef\n",
      "-0.020 , vt_tarwd_depth\n",
      "-0.066 , ctf_src_pathlen\n",
      "-0.114 , n_opp\n",
      "-0.164 , ctf_tar_pathlen\n",
      "-0.267 , vt_src_pathlen\n",
      "-0.365 , tar_coef\n",
      "-0.540 , n_sup\n",
      "-0.640 , ctf_tarwd_depth\n",
      "-0.640 , ctf_srcwd_depth\n",
      "Processing Twitter ...\n",
      "(120, 95)\n",
      "Counter({1: 57, 0: 45, -1: 18}) 0.450\n",
      "class: -1\n",
      "0.796 , src_probLR\n",
      "0.356 , src_coef\n",
      "0.323 , n_opp\n",
      "0.205 , ctf_tarwd_depth\n",
      "0.205 , ctf_srcwd_depth\n",
      "0.130 , n_sup\n",
      "0.099 , ctf_tar_pathlen\n",
      "0.072 , ctf_src_pathlen\n",
      "0.009 , vt_src_pathlen\n",
      "-0.070 , vt_tar_pathlen\n",
      "-0.090 , vt_tarwd_depth\n",
      "-0.116 , n_words\n",
      "-0.144 , tar_probLR\n",
      "-0.156 , vt_srcwd_depth\n",
      "-0.242 , tar_coef\n",
      "class: 0\n",
      "0.544 , tar_probLR\n",
      "0.187 , vt_srcwd_depth\n",
      "0.140 , tar_coef\n",
      "0.033 , n_words\n",
      "0.007 , vt_tar_pathlen\n",
      "0.005 , vt_tarwd_depth\n",
      "-0.012 , vt_src_pathlen\n",
      "-0.036 , ctf_src_pathlen\n",
      "-0.038 , n_sup\n",
      "-0.050 , ctf_tar_pathlen\n",
      "-0.212 , src_coef\n",
      "-0.264 , n_opp\n",
      "-0.324 , ctf_tarwd_depth\n",
      "-0.324 , ctf_srcwd_depth\n",
      "-0.993 , src_probLR\n",
      "class: 1\n",
      "0.902 , ctf_tarwd_depth\n",
      "0.902 , ctf_srcwd_depth\n",
      "0.175 , vt_tarwd_depth\n",
      "0.145 , src_probLR\n",
      "0.124 , n_words\n",
      "0.122 , tar_coef\n",
      "0.102 , vt_tar_pathlen\n",
      "-0.022 , ctf_src_pathlen\n",
      "-0.043 , vt_src_pathlen\n",
      "-0.070 , ctf_tar_pathlen\n",
      "-0.077 , vt_srcwd_depth\n",
      "-0.109 , n_opp\n",
      "-0.166 , n_sup\n",
      "-0.193 , src_coef\n",
      "-0.799 , tar_probLR\n",
      "Processing Airbnb ...\n",
      "(120, 95)\n",
      "Counter({0: 73, 1: 34, -1: 13}) 0.507\n",
      "class: -1\n",
      "0.885 , src_probLR\n",
      "0.551 , tar_probLR\n",
      "0.118 , src_coef\n",
      "0.110 , vt_src_pathlen\n",
      "0.086 , ctf_tar_pathlen\n",
      "0.077 , ctf_src_pathlen\n",
      "0.020 , n_words\n",
      "0.002 , vt_srcwd_depth\n",
      "-0.007 , vt_tarwd_depth\n",
      "-0.022 , n_opp\n",
      "-0.117 , ctf_tarwd_depth\n",
      "-0.117 , ctf_srcwd_depth\n",
      "-0.139 , n_sup\n",
      "-0.218 , vt_tar_pathlen\n",
      "-0.429 , tar_coef\n",
      "class: 0\n",
      "0.481 , tar_coef\n",
      "0.308 , vt_src_pathlen\n",
      "0.307 , ctf_tarwd_depth\n",
      "0.307 , ctf_srcwd_depth\n",
      "0.139 , n_opp\n",
      "0.040 , vt_tarwd_depth\n",
      "0.018 , n_sup\n",
      "-0.028 , n_words\n",
      "-0.059 , vt_srcwd_depth\n",
      "-0.072 , ctf_tar_pathlen\n",
      "-0.154 , src_coef\n",
      "-0.166 , ctf_src_pathlen\n",
      "-0.175 , vt_tar_pathlen\n",
      "-0.261 , tar_probLR\n",
      "-0.471 , src_probLR\n",
      "class: 1\n",
      "0.553 , vt_tar_pathlen\n",
      "0.304 , n_sup\n",
      "0.274 , ctf_tarwd_depth\n",
      "0.274 , ctf_srcwd_depth\n",
      "0.255 , src_coef\n",
      "0.131 , vt_tarwd_depth\n",
      "0.071 , ctf_src_pathlen\n",
      "0.064 , tar_coef\n",
      "0.015 , n_words\n",
      "-0.003 , vt_srcwd_depth\n",
      "-0.132 , ctf_tar_pathlen\n",
      "-0.371 , n_opp\n",
      "-0.556 , vt_src_pathlen\n",
      "-0.945 , tar_probLR\n",
      "-0.954 , src_probLR\n"
     ]
    }
   ],
   "source": [
    "fit_LR_newfeats(dataset_list,new_feats,clf_flag='3-class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Yelp ...\n",
      "(120, 95)\n",
      "Counter({1: 61, 0: 59}) 0.651\n",
      "0.874 , ctf_tarwd_depth\n",
      "0.874 , ctf_srcwd_depth\n",
      "0.799 , tar_coef\n",
      "0.195 , n_sup\n",
      "0.158 , vt_tarwd_depth\n",
      "0.039 , ctf_src_pathlen\n",
      "0.018 , vt_src_pathlen\n",
      "0.003 , ctf_tar_pathlen\n",
      "-0.020 , n_opp\n",
      "-0.022 , vt_tar_pathlen\n",
      "-0.047 , vt_srcwd_depth\n",
      "-0.104 , n_words\n",
      "-0.132 , tar_probLR\n",
      "-0.196 , src_coef\n",
      "-0.421 , src_probLR\n",
      "Processing Twitter ...\n",
      "(120, 95)\n",
      "Counter({0: 63, 1: 57}) 0.533\n",
      "0.546 , tar_probLR\n",
      "0.187 , vt_srcwd_depth\n",
      "0.140 , tar_coef\n",
      "0.033 , n_words\n",
      "0.007 , vt_tar_pathlen\n",
      "0.005 , vt_tarwd_depth\n",
      "-0.012 , vt_src_pathlen\n",
      "-0.036 , ctf_src_pathlen\n",
      "-0.037 , n_sup\n",
      "-0.050 , ctf_tar_pathlen\n",
      "-0.212 , src_coef\n",
      "-0.263 , n_opp\n",
      "-0.324 , ctf_tarwd_depth\n",
      "-0.324 , ctf_srcwd_depth\n",
      "-0.995 , src_probLR\n",
      "Processing Airbnb ...\n",
      "(120, 95)\n",
      "Counter({0: 86, 1: 34}) 0.582\n",
      "0.481 , tar_coef\n",
      "0.308 , vt_src_pathlen\n",
      "0.307 , ctf_tarwd_depth\n",
      "0.307 , ctf_srcwd_depth\n",
      "0.139 , n_opp\n",
      "0.040 , vt_tarwd_depth\n",
      "0.018 , n_sup\n",
      "-0.028 , n_words\n",
      "-0.059 , vt_srcwd_depth\n",
      "-0.072 , ctf_tar_pathlen\n",
      "-0.154 , src_coef\n",
      "-0.166 , ctf_src_pathlen\n",
      "-0.175 , vt_tar_pathlen\n",
      "-0.261 , tar_probLR\n",
      "-0.471 , src_probLR\n"
     ]
    }
   ],
   "source": [
    "fit_LR_newfeats(dataset_list,new_feats,clf_flag='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Yelp ...\n",
      "(120, 95)\n",
      "knn_effect\n",
      "0.070 , tar_coef \n",
      "0.009 , n_words \n",
      "0.008 , ctf_src_pathlen \n",
      "0.006 , vt_tar_pathlen \n",
      "0.002 , vt_src_pathlen \n",
      "0.002 , ctf_tar_pathlen \n",
      "0.000 , ctf_srcwd_depth \n",
      "0.000 , ctf_tarwd_depth \n",
      "-0.002 , n_sup \n",
      "-0.010 , vt_srcwd_depth \n",
      "-0.012 , n_opp \n",
      "-0.024 , vt_tarwd_depth \n",
      "-0.031 , src_coef \n",
      "-0.075 , src_probLR \n",
      "-0.085 , tar_probLR \n",
      "vt_effect\n",
      "0.141 , tar_probLR \n",
      "0.033 , tar_coef \n",
      "0.003 , vt_tar_pathlen \n",
      "0.002 , vt_srcwd_depth \n",
      "0.000 , ctf_src_pathlen \n",
      "0.000 , vt_src_pathlen \n",
      "0.000 , ctf_tarwd_depth \n",
      "-0.000 , ctf_srcwd_depth \n",
      "-0.000 , n_opp \n",
      "-0.001 , n_words \n",
      "-0.002 , ctf_tar_pathlen \n",
      "-0.004 , n_sup \n",
      "-0.004 , src_coef \n",
      "-0.006 , vt_tarwd_depth \n",
      "-0.151 , src_probLR \n",
      "ctf_effect\n",
      "0.262 , tar_probLR \n",
      "0.070 , tar_coef \n",
      "0.006 , vt_tar_pathlen \n",
      "0.005 , n_opp \n",
      "0.004 , n_sup \n",
      "0.003 , ctf_src_pathlen \n",
      "0.003 , vt_srcwd_depth \n",
      "0.000 , ctf_tarwd_depth \n",
      "-0.000 , ctf_srcwd_depth \n",
      "-0.003 , ctf_tar_pathlen \n",
      "-0.003 , vt_src_pathlen \n",
      "-0.004 , n_words \n",
      "-0.011 , vt_tarwd_depth \n",
      "-0.022 , src_coef \n",
      "-0.264 , src_probLR \n",
      "csf_effect\n",
      "0.099 , tar_probLR \n",
      "0.064 , tar_coef \n",
      "0.008 , n_opp \n",
      "0.006 , vt_tar_pathlen \n",
      "0.003 , vt_srcwd_depth \n",
      "0.001 , ctf_src_pathlen \n",
      "0.001 , n_sup \n",
      "0.000 , ctf_tarwd_depth \n",
      "-0.000 , ctf_srcwd_depth \n",
      "-0.000 , n_words \n",
      "-0.002 , vt_src_pathlen \n",
      "-0.004 , ctf_tar_pathlen \n",
      "-0.011 , vt_tarwd_depth \n",
      "-0.028 , src_coef \n",
      "-0.067 , src_probLR \n",
      "Processing Twitter ...\n",
      "(120, 95)\n",
      "knn_effect\n",
      "0.451 , tar_probLR \n",
      "0.033 , vt_src_pathlen \n",
      "0.026 , n_words \n",
      "0.023 , ctf_tar_pathlen \n",
      "0.000 , ctf_tarwd_depth \n",
      "-0.000 , ctf_srcwd_depth \n",
      "-0.000 , tar_coef \n",
      "-0.001 , ctf_src_pathlen \n",
      "-0.014 , src_coef \n",
      "-0.024 , vt_srcwd_depth \n",
      "-0.028 , n_sup \n",
      "-0.029 , vt_tar_pathlen \n",
      "-0.037 , n_opp \n",
      "-0.040 , vt_tarwd_depth \n",
      "-0.725 , src_probLR \n",
      "vt_effect\n",
      "0.157 , tar_probLR \n",
      "0.012 , vt_srcwd_depth \n",
      "0.012 , tar_coef \n",
      "0.006 , ctf_tar_pathlen \n",
      "0.005 , vt_src_pathlen \n",
      "0.002 , ctf_src_pathlen \n",
      "0.000 , n_sup \n",
      "0.000 , ctf_tarwd_depth \n",
      "-0.000 , ctf_srcwd_depth \n",
      "-0.003 , n_words \n",
      "-0.004 , n_opp \n",
      "-0.009 , vt_tar_pathlen \n",
      "-0.022 , src_coef \n",
      "-0.026 , vt_tarwd_depth \n",
      "-0.163 , src_probLR \n",
      "ctf_effect\n",
      "0.341 , tar_probLR \n",
      "0.014 , vt_srcwd_depth \n",
      "0.007 , tar_coef \n",
      "0.007 , n_sup \n",
      "0.006 , ctf_tar_pathlen \n",
      "0.006 , ctf_src_pathlen \n",
      "0.000 , ctf_tarwd_depth \n",
      "-0.000 , ctf_srcwd_depth \n",
      "-0.000 , vt_src_pathlen \n",
      "-0.002 , n_words \n",
      "-0.005 , n_opp \n",
      "-0.007 , vt_tar_pathlen \n",
      "-0.014 , src_coef \n",
      "-0.029 , vt_tarwd_depth \n",
      "-0.539 , src_probLR \n",
      "csf_effect\n",
      "0.210 , tar_probLR \n",
      "0.024 , vt_srcwd_depth \n",
      "0.014 , tar_coef \n",
      "0.005 , ctf_tar_pathlen \n",
      "0.004 , ctf_src_pathlen \n",
      "0.004 , n_sup \n",
      "0.000 , vt_src_pathlen \n",
      "0.000 , ctf_tarwd_depth \n",
      "-0.000 , ctf_srcwd_depth \n",
      "-0.001 , n_words \n",
      "-0.003 , n_opp \n",
      "-0.007 , vt_tar_pathlen \n",
      "-0.019 , src_coef \n",
      "-0.034 , vt_tarwd_depth \n",
      "-0.308 , src_probLR \n",
      "Processing Airbnb ...\n",
      "(120, 95)\n",
      "knn_effect\n",
      "0.037 , tar_coef \n",
      "0.020 , vt_srcwd_depth \n",
      "0.015 , n_sup \n",
      "0.008 , vt_src_pathlen \n",
      "0.008 , n_opp \n",
      "0.000 , ctf_tarwd_depth \n",
      "-0.000 , ctf_srcwd_depth \n",
      "-0.001 , ctf_tar_pathlen \n",
      "-0.002 , ctf_src_pathlen \n",
      "-0.007 , vt_tarwd_depth \n",
      "-0.009 , n_words \n",
      "-0.014 , vt_tar_pathlen \n",
      "-0.016 , src_coef \n",
      "-0.260 , src_probLR \n",
      "-0.399 , tar_probLR \n",
      "vt_effect\n",
      "0.103 , tar_probLR \n",
      "0.004 , vt_src_pathlen \n",
      "0.004 , tar_coef \n",
      "0.003 , vt_srcwd_depth \n",
      "0.001 , n_opp \n",
      "0.000 , vt_tarwd_depth \n",
      "0.000 , ctf_srcwd_depth \n",
      "0.000 , ctf_tarwd_depth \n",
      "-0.000 , n_words \n",
      "-0.000 , ctf_tar_pathlen \n",
      "-0.000 , n_sup \n",
      "-0.001 , ctf_src_pathlen \n",
      "-0.002 , src_coef \n",
      "-0.004 , vt_tar_pathlen \n",
      "-0.110 , src_probLR \n",
      "ctf_effect\n",
      "0.278 , tar_probLR \n",
      "0.021 , tar_coef \n",
      "0.014 , vt_srcwd_depth \n",
      "0.004 , n_sup \n",
      "0.002 , ctf_src_pathlen \n",
      "0.001 , n_opp \n",
      "0.000 , ctf_tarwd_depth \n",
      "0.000 , ctf_srcwd_depth \n",
      "-0.000 , vt_src_pathlen \n",
      "-0.001 , ctf_tar_pathlen \n",
      "-0.002 , n_words \n",
      "-0.005 , vt_tar_pathlen \n",
      "-0.009 , vt_tarwd_depth \n",
      "-0.011 , src_coef \n",
      "-0.557 , src_probLR \n",
      "csf_effect\n",
      "0.263 , tar_probLR \n",
      "0.012 , tar_coef \n",
      "0.008 , n_sup \n",
      "0.005 , vt_src_pathlen \n",
      "0.005 , vt_srcwd_depth \n",
      "0.001 , ctf_tar_pathlen \n",
      "0.001 , n_words \n",
      "0.000 , ctf_srcwd_depth \n",
      "0.000 , ctf_tarwd_depth \n",
      "-0.000 , ctf_src_pathlen \n",
      "-0.006 , n_opp \n",
      "-0.006 , vt_tar_pathlen \n",
      "-0.006 , vt_tarwd_depth \n",
      "-0.031 , src_coef \n",
      "-0.426 , src_probLR \n"
     ]
    }
   ],
   "source": [
    "fit_LR_newfeats(dataset_list,new_feats,clf_flag='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#vt_pd['src_depth'].apply(lambda x: np.mean(ast.literal_eval(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Result for 5 new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Yelp ...\n",
      "(120, 96)\n",
      "Counter({1: 61, 0: 41, -1: 18}) 0.616\n",
      "class: -1\n",
      "0.087 , n_opp\n",
      "0.084 , n_sup\n",
      "0.005 , src_probLR\n",
      "-0.036 , n_words\n",
      "-0.334 , tar_src_dif_coef\n",
      "class: 0\n",
      "0.530 , tar_src_dif_coef\n",
      "0.156 , n_sup\n",
      "-0.075 , n_words\n",
      "-0.077 , n_opp\n",
      "-0.569 , src_probLR\n",
      "class: 1\n",
      "0.343 , src_probLR\n",
      "0.205 , n_words\n",
      "-0.074 , n_opp\n",
      "-0.450 , n_sup\n",
      "-0.528 , tar_src_dif_coef\n",
      "Processing Twitter ...\n",
      "(120, 96)\n",
      "Counter({1: 57, 0: 45, -1: 18}) 0.425\n",
      "class: -1\n",
      "0.318 , src_probLR\n",
      "0.291 , n_opp\n",
      "0.149 , n_sup\n",
      "-0.131 , n_words\n",
      "-0.275 , tar_src_dif_coef\n",
      "class: 0\n",
      "0.160 , tar_src_dif_coef\n",
      "0.028 , n_words\n",
      "0.002 , n_sup\n",
      "-0.178 , n_opp\n",
      "-0.521 , src_probLR\n",
      "class: 1\n",
      "0.134 , n_words\n",
      "0.061 , tar_src_dif_coef\n",
      "-0.001 , src_probLR\n",
      "-0.168 , n_opp\n",
      "-0.238 , n_sup\n",
      "Processing Airbnb ...\n",
      "(120, 96)\n",
      "Counter({0: 73, 1: 34, -1: 13}) 0.600\n",
      "class: -1\n",
      "1.050 , src_probLR\n",
      "0.017 , n_words\n",
      "-0.028 , n_opp\n",
      "-0.110 , n_sup\n",
      "-0.165 , tar_src_dif_coef\n",
      "class: 0\n",
      "0.240 , tar_src_dif_coef\n",
      "0.112 , n_opp\n",
      "0.022 , n_sup\n",
      "-0.028 , n_words\n",
      "-0.797 , src_probLR\n",
      "class: 1\n",
      "0.209 , n_sup\n",
      "0.007 , n_words\n",
      "-0.170 , tar_src_dif_coef\n",
      "-0.248 , n_opp\n",
      "-1.213 , src_probLR\n"
     ]
    }
   ],
   "source": [
    "fit_LR_newfeats(dataset_list,new_feats,clf_flag='3-class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Yelp ...\n",
      "(120, 96)\n",
      "Counter({1: 61, 0: 59}) 0.734\n",
      "0.530 , tar_src_dif_coef\n",
      "0.156 , n_sup\n",
      "-0.075 , n_words\n",
      "-0.077 , n_opp\n",
      "-0.569 , src_probLR\n",
      "Processing Twitter ...\n",
      "(120, 96)\n",
      "Counter({0: 63, 1: 57}) 0.533\n",
      "0.160 , tar_src_dif_coef\n",
      "0.028 , n_words\n",
      "0.002 , n_sup\n",
      "-0.178 , n_opp\n",
      "-0.521 , src_probLR\n",
      "Processing Airbnb ...\n",
      "(120, 96)\n",
      "Counter({0: 86, 1: 34}) 0.717\n",
      "0.240 , tar_src_dif_coef\n",
      "0.112 , n_opp\n",
      "0.022 , n_sup\n",
      "-0.028 , n_words\n",
      "-0.797 , src_probLR\n"
     ]
    }
   ],
   "source": [
    "fit_LR_newfeats(dataset_list,new_feats,clf_flag='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Yelp ...\n",
      "(120, 96)\n",
      "knn_effect\n",
      "0.034 , tar_src_dif_coef \n",
      "-0.001 , n_words \n",
      "-0.005 , n_sup \n",
      "-0.009 , n_opp \n",
      "-0.189 , src_probLR \n",
      "vt_effect\n",
      "0.031 , src_probLR \n",
      "0.018 , tar_src_dif_coef \n",
      "0.002 , n_opp \n",
      "-0.001 , n_sup \n",
      "-0.004 , n_words \n",
      "ctf_effect\n",
      "0.079 , src_probLR \n",
      "0.044 , tar_src_dif_coef \n",
      "0.015 , n_opp \n",
      "0.014 , n_sup \n",
      "-0.013 , n_words \n",
      "csf_effect\n",
      "0.086 , src_probLR \n",
      "0.041 , tar_src_dif_coef \n",
      "0.015 , n_opp \n",
      "0.006 , n_sup \n",
      "-0.006 , n_words \n",
      "Processing Twitter ...\n",
      "(120, 96)\n",
      "knn_effect\n",
      "0.018 , tar_src_dif_coef \n",
      "0.016 , n_words \n",
      "-0.021 , n_sup \n",
      "-0.046 , n_opp \n",
      "-0.453 , src_probLR \n",
      "vt_effect\n",
      "0.024 , tar_src_dif_coef \n",
      "0.010 , n_sup \n",
      "0.002 , n_opp \n",
      "-0.009 , n_words \n",
      "-0.075 , src_probLR \n",
      "ctf_effect\n",
      "0.022 , tar_src_dif_coef \n",
      "0.022 , n_sup \n",
      "0.005 , n_opp \n",
      "-0.009 , n_words \n",
      "-0.294 , src_probLR \n",
      "csf_effect\n",
      "0.027 , tar_src_dif_coef \n",
      "0.016 , n_sup \n",
      "0.006 , n_opp \n",
      "-0.006 , n_words \n",
      "-0.164 , src_probLR \n",
      "Processing Airbnb ...\n",
      "(120, 96)\n",
      "knn_effect\n",
      "0.018 , n_sup \n",
      "0.014 , tar_src_dif_coef \n",
      "0.009 , n_opp \n",
      "-0.008 , n_words \n",
      "-0.637 , src_probLR \n",
      "vt_effect\n",
      "0.007 , tar_src_dif_coef \n",
      "0.000 , n_opp \n",
      "0.000 , n_words \n",
      "-0.001 , n_sup \n",
      "-0.023 , src_probLR \n",
      "ctf_effect\n",
      "0.029 , tar_src_dif_coef \n",
      "0.008 , n_sup \n",
      "0.001 , n_opp \n",
      "-0.002 , n_words \n",
      "-0.360 , src_probLR \n",
      "csf_effect\n",
      "0.034 , tar_src_dif_coef \n",
      "0.010 , n_sup \n",
      "0.001 , n_words \n",
      "-0.006 , n_opp \n",
      "-0.222 , src_probLR \n"
     ]
    }
   ],
   "source": [
    "fit_LR_newfeats(dataset_list,new_feats,clf_flag='linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict positive probability for each instance and then check correlation between P(y=1|x) and each treatment effect(KNN,VT,CTF,CSF)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_prob_newfeats(dataset_list,new_feats,LSE_effect,flag):\n",
    "    \"\"\"\n",
    "    Predict p(y=1|x) with new features.\n",
    "    In-bag prediction\n",
    "    \"\"\"\n",
    "    all_prob_corrs = []\n",
    "    for di in range(len(dataset_list)):\n",
    "        print(\"Processing %s ...\" % (full_name[dataset_list[di]]))\n",
    "        coef_file = project_path+'V2_'+full_name[dataset_list[di]].lower()+'/9_Coef/'+dataset_list[di]+'_sentence_coef.csv'\n",
    "        org_coef_pd = pd.read_csv(coef_file)\n",
    "        org_coef_pd['tar_src_dif_coef'] = org_coef_pd['tar_coef'] - org_coef_pd['src_coef'] \n",
    "        coef_pd = cal_causal_words(org_coef_pd,0.1,-0.1)\n",
    "        lr_file = project_path+'V2_'+full_name[dataset_list[di]].lower()+'/8_Var/'+dataset_list[di]+'_effectVar_prob_byLR.csv'\n",
    "        vt_file = project_path+'V2_'+full_name[dataset_list[di]].lower()+'/8_Var/'+dataset_list[di]+'_VT_srctar_depth3.csv'\n",
    "        ctf_file = project_path+'V2_'+full_name[dataset_list[di]].lower()+'/8_Var/'+dataset_list[di]+'_CTF_srctar_depth.csv'\n",
    "        super_pd = cal_newfeats(coef_pd,lr_file,vt_file,ctf_file)\n",
    "        print(super_pd.shape)\n",
    "        \n",
    "        # construct feature matrix\n",
    "        X_feats = []\n",
    "        for i in range(super_pd.shape[0]):\n",
    "            row = super_pd.iloc[i][new_feats].values\n",
    "            X_feats.append([float(e) for e in row])\n",
    "        X_feats = np.array(X_feats)\n",
    "        \n",
    "        # fit binary logistic-regression\n",
    "        LR_clf = LogisticRegression(random_state=42,n_jobs=-1)\n",
    "        amt_labels = super_pd[AMT_method].apply(lambda x: 1 if x>0 else 0)\n",
    "        if(flag=='inb'):\n",
    "            LR_clf.fit(X_feats,amt_labels)\n",
    "            pos_prob = LR_clf.predict_proba(X_feats)[:,1]\n",
    "        elif(flag=='oob'):\n",
    "            pos_prob=[]\n",
    "            for ti in range(X_feats.shape[0]):\n",
    "                sel_idx = [idx for idx in range(X_feats.shape[0]) if idx != ti]\n",
    "                LR_clf.fit(X_feats[sel_idx,:],amt_labels[sel_idx])\n",
    "                pos_prob.append(LR_clf.predict_proba(X_feats[ti])[0,1])\n",
    "                \n",
    "        prob_corrs = []\n",
    "        for eft in LSE_effect:\n",
    "            prob_corrs.append(pearsonr(pos_prob,super_pd[eft])[0])\n",
    "        all_prob_corrs.append(pd.DataFrame(prob_corrs,index=LSE_effect,columns=[full_name[dataset_list[di]]]))\n",
    "        \n",
    "        \n",
    "    display(pd.concat(all_prob_corrs,axis=1))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Yelp ...\n",
      "(120, 96)\n",
      "Processing Twitter ...\n",
      "(120, 96)\n",
      "Processing Airbnb ...\n",
      "(120, 96)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Yelp</th>\n",
       "      <th>Twitter</th>\n",
       "      <th>Airbnb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>knn_effect</th>\n",
       "      <td>0.365381</td>\n",
       "      <td>0.341582</td>\n",
       "      <td>0.452208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vt_effect</th>\n",
       "      <td>0.622568</td>\n",
       "      <td>0.412877</td>\n",
       "      <td>0.304252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ctf_effect</th>\n",
       "      <td>0.550506</td>\n",
       "      <td>0.217210</td>\n",
       "      <td>0.490342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>csf_effect</th>\n",
       "      <td>0.387922</td>\n",
       "      <td>0.134598</td>\n",
       "      <td>0.276877</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Yelp   Twitter    Airbnb\n",
       "knn_effect  0.365381  0.341582  0.452208\n",
       "vt_effect   0.622568  0.412877  0.304252\n",
       "ctf_effect  0.550506  0.217210  0.490342\n",
       "csf_effect  0.387922  0.134598  0.276877"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "LSE_effect = ['knn_effect','vt_effect','ctf_effect','csf_effect']\n",
    "predict_prob_newfeats(dataset_list,new_feats,LSE_effect,AMT_method = 'amt_effect_mostcom',flag='inbag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Yelp ...\n",
      "(120, 96)\n",
      "Processing Twitter ...\n",
      "(120, 96)\n",
      "Processing Airbnb ...\n",
      "(120, 96)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Yelp</th>\n",
       "      <th>Twitter</th>\n",
       "      <th>Airbnb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>knn_effect</th>\n",
       "      <td>0.366059</td>\n",
       "      <td>0.284777</td>\n",
       "      <td>0.464504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vt_effect</th>\n",
       "      <td>0.617978</td>\n",
       "      <td>0.381676</td>\n",
       "      <td>0.315482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ctf_effect</th>\n",
       "      <td>0.544271</td>\n",
       "      <td>0.136027</td>\n",
       "      <td>0.500171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>csf_effect</th>\n",
       "      <td>0.384367</td>\n",
       "      <td>0.112451</td>\n",
       "      <td>0.279512</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Yelp   Twitter    Airbnb\n",
       "knn_effect  0.366059  0.284777  0.464504\n",
       "vt_effect   0.617978  0.381676  0.315482\n",
       "ctf_effect  0.544271  0.136027  0.500171\n",
       "csf_effect  0.384367  0.112451  0.279512"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "LSE_effect = ['knn_effect','vt_effect','ctf_effect','csf_effect']\n",
    "predict_prob_newfeats(dataset_list,new_feats,LSE_effect,flag='oob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
