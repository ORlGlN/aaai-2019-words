{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Lexical Substitution Effect estimation using Virtual Twins RandomForest\n",
    "- n_trees = min(200, train_vec.shape[0]) <br>\n",
    "- max_features='log2' <br>\n",
    "- min_samples_leaf=2 (updated later) <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, time, operator, re, json, csv, pickle, copy, ast\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from ast import literal_eval\n",
    "from scipy.sparse import vstack\n",
    "from scipy.spatial.distance import cdist\n",
    "import sklearn.metrics.pairwise\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "AMT_path = '/data/2/zwang/2018_S_WordTreatment/V2_AMT/'\n",
    "airbnb_path = '/data/2/zwang/2018_S_WordTreatment/V2_airbnb/'\n",
    "tw_path = '/data/2/zwang/2018_S_WordTreatment/V2_twitter/'\n",
    "yp_path = '/data/2/zwang/2018_S_WordTreatment/V2_yelp/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VT_treatment_for_pair(swd,twd,sents,labels,divide,effect):\n",
    "    source_pos_treatment = []\n",
    "    source_neg_treatment = []\n",
    "    target_pos_treatment = []\n",
    "    target_neg_treatment = []\n",
    "    \n",
    "    for i in range(len(sents)):\n",
    "        if(i<divide and labels[i]==1):\n",
    "            source_pos_treatment.append((sents[i],round(float(effect[i]),5)))\n",
    "        elif(i<divide and labels[i]==0):\n",
    "            source_neg_treatment.append((sents[i],round(float(effect[i]),5)))\n",
    "        elif(i>=divide and labels[i]==1):\n",
    "            target_pos_treatment.append((sents[i],round(float(effect[i]),5)))\n",
    "        elif(i>=divide and labels[i]==0):\n",
    "            target_neg_treatment.append((sents[i],round(float(effect[i]),5)))\n",
    "            \n",
    "    return {'source':swd,'target':twd,\n",
    "            'source_pos_sents_treatment':[source_pos_treatment],'source_neg_sents_treatment':[source_neg_treatment],\n",
    "            'target_pos_sents_treatment':[target_pos_treatment],'target_neg_sents_treatment':[target_neg_treatment]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_10RFC(swd,twd,vocab,X_src,y_src,X_src_testidx,X_src_trainidx,X_tar,y_tar,X_tar_testidx,X_tar_trainidx,nfold,n_trees):\n",
    "    # train 10 CF for 10 folds of X_src\n",
    "    src_pred = []\n",
    "    src_effect = []\n",
    "    tar_pred = []\n",
    "    tar_effect = []\n",
    "    for i in range(X_src.shape[0]):\n",
    "        src_pred.append(-2)\n",
    "        src_effect.append(-2)\n",
    "    for i in range(X_tar.shape[0]):\n",
    "        tar_pred.append(-2)\n",
    "        tar_effect.append(-2)\n",
    "    \n",
    "    src_wdi = vocab[swd]\n",
    "    tar_wdi = vocab[twd]\n",
    "        \n",
    "    for i in range(nfold):\n",
    "        test_vec_src = X_src[X_src_testidx[i]]\n",
    "        test_vec_tar = X_tar[X_tar_testidx[i]]\n",
    "        train_vec_src = X_src[X_src_trainidx[i]]\n",
    "        train_vec_tar = X_tar[X_tar_trainidx[i]]\n",
    "        train_vec = vstack((train_vec_src,train_vec_tar))\n",
    "        \n",
    "        train_y_src = np.array(y_src)[X_src_trainidx[i]]\n",
    "        train_y_tar = np.array(y_tar)[X_tar_trainidx[i]]\n",
    "        train_y = list(train_y_src) + list(train_y_tar)\n",
    "\n",
    "        \n",
    "#         if(train_vec.shape[0] < n_trees):\n",
    "#             RF_clf = RandomForestClassifier(n_estimators = train_vec.shape[0], max_features='log2', min_samples_leaf=2, \n",
    "#                                             random_state=42, n_jobs=-1, oob_score=True)\n",
    "#         else:\n",
    "#             RF_clf = RandomForestClassifier(n_estimators = n_trees, max_features='log2', min_samples_leaf=2, \n",
    "#                                             random_state=42, n_jobs=-1, oob_score=True)\n",
    "\n",
    "        RF_clf = RandomForestClassifier(n_estimators = min(200,train_vec.shape[0]), max_features='log2', min_samples_leaf=2, \n",
    "                                            random_state=42, n_jobs=-1, oob_score=True)\n",
    "        \n",
    "        RF_clf.fit(train_vec, train_y)\n",
    "        #print(test_vec_src.shape)\n",
    "        #print(list(RF_clf.classes_).index(1))\n",
    "        RFC_pred_src = RF_clf.predict_proba(test_vec_src)[:,list(RF_clf.classes_).index(1)]\n",
    "        RFC_pred_tar = RF_clf.predict_proba(test_vec_tar)[:,list(RF_clf.classes_).index(1)]\n",
    "        \n",
    "        src_treat_vec = test_vec_src\n",
    "        src_treat_vec[:,src_wdi] = 0\n",
    "        src_treat_vec[:,tar_wdi] = 1\n",
    "        RFC_pred_src_treat = RF_clf.predict_proba(src_treat_vec)[:,list(RF_clf.classes_).index(1)]\n",
    "        \n",
    "        tar_treat_vec = test_vec_tar\n",
    "        tar_treat_vec[:,tar_wdi] = 0\n",
    "        tar_treat_vec[:,src_wdi] = 1\n",
    "        RFC_pred_tar_treat = RF_clf.predict_proba(tar_treat_vec)[:,list(RF_clf.classes_).index(1)]\n",
    "\n",
    "        j=0\n",
    "        for idx in X_src_testidx[i]:\n",
    "            src_pred[idx] = RFC_pred_src[j]\n",
    "            src_effect[idx] = RFC_pred_src_treat[j] - RFC_pred_src[j]\n",
    "            j += 1\n",
    "        j=0\n",
    "        for idx in X_tar_testidx[i]:\n",
    "            tar_pred[idx] = RFC_pred_tar[j]\n",
    "            tar_effect[idx] = RFC_pred_tar_treat[j] - RFC_pred_tar[j]\n",
    "            j += 1\n",
    "    \n",
    "    return src_pred, src_effect, tar_pred, tar_effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_folds(swd,twd,vocab,X_sents,labels,divide,n_trees,nfold):\n",
    "    \"\"\"\n",
    "    split X_src into 10 folds, and split X_tar into 10 folds\n",
    "    \"\"\" \n",
    "    X_src = X_sents[:divide]\n",
    "    X_tar = X_sents[divide:]\n",
    "    y_src = labels[:divide]\n",
    "    y_tar = labels[divide:]\n",
    "    \n",
    "    X_src_testidx = []\n",
    "    X_src_trainidx = []\n",
    "    X_tar_testidx = []\n",
    "    X_tar_trainidx = []\n",
    "    skf = StratifiedKFold(n_splits=nfold, random_state=42)\n",
    "    #if((X_src.shape[0]<=nfold) or (X_tar.shape[0]<=nfold)):\n",
    "    #    return\n",
    "    #print(X_src.shape,X_tar.shape)\n",
    "    for train_index, test_index in skf.split(X_src, y_src):\n",
    "        X_src_testidx.append(list(test_index))\n",
    "        X_src_trainidx.append(list(train_index))\n",
    "    #src_pred, src_effect = train_RFC(swd,twd,vocab,X_src,y_src,X_tar,y_tar,X_src_testidx,X_src_trainidx,nfold,n_trees)\n",
    "    \n",
    "    for train_index, test_index in skf.split(X_tar, y_tar):\n",
    "        X_tar_testidx.append(list(test_index))\n",
    "        X_tar_trainidx.append(list(train_index))\n",
    "    #tar_pred, tar_effect = train_RFC(twd,swd,vocab,X_tar,y_tar,X_src,y_src,X_tar_testidx,X_tar_trainidx,nfold,n_trees)\n",
    "    src_pred, src_effect, tar_pred, tar_effect = train_10RFC(swd,twd,vocab,\n",
    "                                                             X_src,y_src,X_src_testidx,X_src_trainidx,\n",
    "                                                             X_tar,y_tar,X_tar_testidx,X_tar_trainidx,\n",
    "                                                             nfold,n_trees)\n",
    "    \n",
    "    return list(src_pred)+list(tar_pred), list(src_effect)+list(tar_effect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_VT_treatment(wdpair_file,treat_file,log_file,vocab_file,n_trees,nfold):\n",
    "    wdpairs_pos_neg_pd = pd.DataFrame(pickle.load(open(wdpair_file,'rb')))\n",
    "    \n",
    "    if(vocab_file):\n",
    "        my_vocab = list(set(pd.read_csv(vocab_file).word.values))\n",
    "        ct_vec = CountVectorizer(min_df=3,binary=True,vocabulary=my_vocab)\n",
    "        #print(len(my_vocab))\n",
    "    else:\n",
    "        ct_vec = CountVectorizer(min_df=3,binary=True)\n",
    "\n",
    "    \n",
    "    with open(treat_file,'wt') as fw, open(log_file,'wt') as flog:\n",
    "        my_fields = ['source','target','source_pos_sents_treatment','source_neg_sents_treatment','target_pos_sents_treatment','target_neg_sents_treatment']\n",
    "        csv_f = csv.DictWriter(fw,fieldnames=my_fields)\n",
    "        csv_f.writeheader()\n",
    "        \n",
    "        log_fields = ['swd','twd','shape']\n",
    "        log_csv = csv.DictWriter(flog,fieldnames=log_fields)\n",
    "        log_csv.writeheader()\n",
    "\n",
    "        for idx, row in wdpairs_pos_neg_pd.iterrows():\n",
    "            if(idx % 100 == 0):\n",
    "                print(\"-------------\",idx,\"-------------\")\n",
    "                \n",
    "            swd = row['source']\n",
    "            twd = row['target']\n",
    "            swd_pos_sents = row['source_pos_sents']\n",
    "            swd_neg_sents = row['source_neg_sents']\n",
    "            twd_pos_sents = row['target_pos_sents']\n",
    "            twd_neg_sents = row['target_neg_sents']\n",
    "\n",
    "            #if(len(swd_pos_sents)>nfold and len(swd_neg_sents)>nfold and len(twd_pos_sents)>nfold and len(twd_neg_sents)>nfold):\n",
    "            sents = swd_pos_sents + swd_neg_sents + twd_pos_sents + twd_neg_sents\n",
    "            labels = list(np.ones(len(swd_pos_sents)))+list(np.zeros(len(swd_neg_sents)))+list(np.ones(len(twd_pos_sents)))+list(np.zeros(len(twd_neg_sents)))\n",
    "            divide = len(swd_pos_sents)+len(swd_neg_sents)\n",
    "\n",
    "            X_sents = ct_vec.fit_transform(sents)\n",
    "            vocab = ct_vec.vocabulary_\n",
    "            #print(swd,twd,X_sents.shape)\n",
    "            #print(vocab)\n",
    "\n",
    "\n",
    "            log_csv.writerow({'swd':swd, 'twd':twd, 'shape': X_sents.shape})\n",
    "            RFC_pred, RFC_effect = make_folds(swd,twd,vocab,X_sents,labels,divide,n_trees,nfold)\n",
    "\n",
    "            treat_info = VT_treatment_for_pair(swd,twd,sents,labels,divide,RFC_effect)\n",
    "            \n",
    "            if(treat_info):\n",
    "                csv_f.writerow({'source':treat_info['source'],'target':treat_info['target'],\n",
    "                        'source_pos_sents_treatment':treat_info['source_pos_sents_treatment'],'source_neg_sents_treatment':treat_info['source_neg_sents_treatment'],\n",
    "                            'target_pos_sents_treatment':treat_info['target_pos_sents_treatment'],'target_neg_sents_treatment':treat_info['target_neg_sents_treatment']})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Only using AMT labeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------- 0 -------------\n",
      "4.858896497885386\n"
     ]
    }
   ],
   "source": [
    "prefix = \"yp\"\n",
    "pair_file = AMT_path+'AMT_WdSents/Data/'+prefix+'_AMT_wdsents_markPPN.pickle'\n",
    "treat_file = AMT_path +\"AMT_WdSents/3_VT/\"+prefix+\"_vt_200tree_treatment.csv\"\n",
    "log_file =  AMT_path+\"AMT_WdSents/3_VT/\"+prefix+\"_vt_200tree_log.csv\"\n",
    "#vocab_file=airbnb_path+'0_Data/common_wds.csv'\n",
    "start = time.time()\n",
    "cal_VT_treatment(pair_file,treat_file,log_file,vocab_file='',n_trees=200,nfold=10)\n",
    "end = time.time()\n",
    "print((end-start)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23, 6)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_pd = pd.read_csv(treat_file,index_col=False)\n",
    "res_pd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>source_pos_sents_treatment</th>\n",
       "      <th>source_neg_sents_treatment</th>\n",
       "      <th>target_pos_sents_treatment</th>\n",
       "      <th>target_neg_sents_treatment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>yummy</td>\n",
       "      <td>good</td>\n",
       "      <td>[[(\"Despite the yummy spice tuna roll and teri...</td>\n",
       "      <td>[[('I had a nibble ... well ... okay , I had l...</td>\n",
       "      <td>[[('It was the only part of the meal I had to ...</td>\n",
       "      <td>[[(\"I 'm always on the look out for good deals...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>yummy</td>\n",
       "      <td>tasty</td>\n",
       "      <td>[[(\"Despite the yummy spice tuna roll and teri...</td>\n",
       "      <td>[[('I had a nibble ... well ... okay , I had l...</td>\n",
       "      <td>[[(\"Love the veal here and pasta dishes I 've ...</td>\n",
       "      <td>[[(\"-RRB- , an odd little tasty cake -LRB- i k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fabulous</td>\n",
       "      <td>amazing</td>\n",
       "      <td>[[('_NNP_ stage , fantastic lighting and fabul...</td>\n",
       "      <td>[[('This is a fabulous restaurant . ', 0.0928)...</td>\n",
       "      <td>[[('We will definitely go back to have the ama...</td>\n",
       "      <td>[[('A truly amazing deal . ', -0.09343), ('Thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gorgeous</td>\n",
       "      <td>great</td>\n",
       "      <td>[[('A previous reviewer mentioned gorgeous sta...</td>\n",
       "      <td>[[('Gorgeous theater lobby almost like a _NNP_...</td>\n",
       "      <td>[[(\"The atmosphere is great - you 're higher t...</td>\n",
       "      <td>[[('Totally great for a one-time experience , ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>boyfriend</td>\n",
       "      <td>buddy</td>\n",
       "      <td>[[('When I checked in with my boyfriend for a ...</td>\n",
       "      <td>[[('I love meeting girlfriends here for drinks...</td>\n",
       "      <td>[[('The weakness in your knees , The cigarette...</td>\n",
       "      <td>[[('My buddy was intrigued by the _NNP_ wings ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      source   target                         source_pos_sents_treatment  \\\n",
       "0      yummy     good  [[(\"Despite the yummy spice tuna roll and teri...   \n",
       "1      yummy    tasty  [[(\"Despite the yummy spice tuna roll and teri...   \n",
       "2   fabulous  amazing  [[('_NNP_ stage , fantastic lighting and fabul...   \n",
       "3   gorgeous    great  [[('A previous reviewer mentioned gorgeous sta...   \n",
       "4  boyfriend    buddy  [[('When I checked in with my boyfriend for a ...   \n",
       "\n",
       "                          source_neg_sents_treatment  \\\n",
       "0  [[('I had a nibble ... well ... okay , I had l...   \n",
       "1  [[('I had a nibble ... well ... okay , I had l...   \n",
       "2  [[('This is a fabulous restaurant . ', 0.0928)...   \n",
       "3  [[('Gorgeous theater lobby almost like a _NNP_...   \n",
       "4  [[('I love meeting girlfriends here for drinks...   \n",
       "\n",
       "                          target_pos_sents_treatment  \\\n",
       "0  [[('It was the only part of the meal I had to ...   \n",
       "1  [[(\"Love the veal here and pasta dishes I 've ...   \n",
       "2  [[('We will definitely go back to have the ama...   \n",
       "3  [[(\"The atmosphere is great - you 're higher t...   \n",
       "4  [[('The weakness in your knees , The cigarette...   \n",
       "\n",
       "                          target_neg_sents_treatment  \n",
       "0  [[(\"I 'm always on the look out for good deals...  \n",
       "1  [[(\"-RRB- , an odd little tasty cake -LRB- i k...  \n",
       "2  [[('A truly amazing deal . ', -0.09343), ('Thi...  \n",
       "3  [[('Totally great for a one-time experience , ...  \n",
       "4  [[('My buddy was intrigued by the _NNP_ wings ...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"Despite the yummy spice tuna roll and teriyaki salmon , I 'll skip this place next time . \",\n",
       " 0.15973)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ast.literal_eval(res_pd.iloc[0]['source_pos_sents_treatment'])[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------- 5 -------------\n",
      "1.0724186539649962\n"
     ]
    }
   ],
   "source": [
    "this_path = airbnb_path\n",
    "prefix = \"airbnb\"\n",
    "pair_file = this_path+\"1_Process/\"+prefix+\"_wdpair_sents_limit5000.pickle\"\n",
    "treat_file = this_path +\"3_VirtualTwins/\"+prefix+\"_vt_200tree_treatment.csv\"\n",
    "log_file =  this_path+\"3_VirtualTwins/\"+prefix+\"_vt_200tree_log.csv\"\n",
    "vocab_file = this_path+'0_Data/common_wds.csv'\n",
    "start = time.time()\n",
    "cal_VT_treatment(pair_file,vocab_file,treat_file,log_file,n_trees=200,nfold=10)\n",
    "end = time.time()\n",
    "print((end-start)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------- 5 -------------\n",
      "1.266097374757131\n"
     ]
    }
   ],
   "source": [
    "this_path = yp_path\n",
    "prefix = \"yp\"\n",
    "pair_file = this_path+\"1_Process/\"+prefix+\"_wdpair_sents_limit5000.pickle\"\n",
    "treat_file = this_path +\"3_VirtualTwins/\"+prefix+\"_vt_200tree_treatment.csv\"\n",
    "log_file =  this_path+\"3_VirtualTwins/\"+prefix+\"_vt_200tree_log.csv\"\n",
    "start = time.time()\n",
    "cal_VT_treatment(pair_file,treat_file,log_file,n_trees=200,nfold=10)\n",
    "end = time.time()\n",
    "print((end-start)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------- 0 -------------\n",
      "------------- 100 -------------\n",
      "------------- 200 -------------\n",
      "------------- 300 -------------\n",
      "------------- 400 -------------\n",
      "------------- 500 -------------\n",
      "------------- 600 -------------\n",
      "------------- 700 -------------\n",
      "------------- 800 -------------\n",
      "------------- 900 -------------\n",
      "168.15899878342947\n"
     ]
    }
   ],
   "source": [
    "this_path = tw_path\n",
    "prefix = \"tw\"\n",
    "pair_file = this_path+\"1_Process/\"+prefix+\"_wdpair_sents_limit5000.pickle\"\n",
    "treat_file = this_path +\"3_VirtualTwins/\"+prefix+\"_vt_200tree_treatment.csv\"\n",
    "log_file =  this_path+\"3_VirtualTwins/\"+prefix+\"_vt_200tree_log.csv\"\n",
    "start = time.time()\n",
    "cal_VT_treatment(pair_file,treat_file,log_file,n_trees=200,nfold=10)\n",
    "end = time.time()\n",
    "print((end-start)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
