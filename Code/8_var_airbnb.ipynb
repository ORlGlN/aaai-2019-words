{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "###  Get variance for each amt labeled sentence\n",
    "> Variance for each (word pair, sentence pair) of each method (e.g., variance among 200 trees in RF).\n",
    "\n",
    "#### 0 Randomly select 90% of the whole data to fit the following methods and repeat to get variance\n",
    "#### 1 VT-RF variance\n",
    "> 1.1 Fit VT-RF with all sentences containing source and target except the current one <br>\n",
    "> 1.2 Get path for source sentence and target sentence, each tree has a path, 200 trees in total, path with max and min probability <br>\n",
    "> 1.3 Compute treatment effect of each pair of sentence, get variance for source and target sentence separately <br>\n",
    "\n",
    "#### 2 CTF-RF variance\n",
    "> 2.1 Fit source RF with all sentences containing source word, target RF with all sentences containing target word, except current one <br>\n",
    "> 2.2 Get path for source sentence and target sentence, path with max and min probability <br>\n",
    "> 2.3 Compute treatment effect of each pair of sentence, get variance for source and target sentence separately <br>\n",
    "\n",
    "#### 3 CSF variance\n",
    "> 3.1 Fit causal forest and get variance directly <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pickle, pydot, random, re, time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import vstack\n",
    "from sklearn.externals.six import StringIO  \n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn import tree\n",
    "from IPython.display import Image\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import rpy2\n",
    "import rpy2.robjects as ro\n",
    "from rpy2.robjects import r\n",
    "from rpy2.robjects import Matrix\n",
    "import rpy2.robjects.numpy2ri\n",
    "from rpy2.robjects.vectors import Matrix\n",
    "from rpy2.robjects.packages import importr\n",
    "importr('grf')\n",
    "rpy2.robjects.numpy2ri.activate()\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "project_path = '/data/2/zwang/2018_S_WordTreatment/'\n",
    "amt_path = '/data/2/zwang/2018_S_WordTreatment/V2_AMT/6_amt/'\n",
    "full_name = {'yp':'Yelp','tw':'Twitter','airbnb':'Airbnb'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def fit_KNN(X_src_tf,src_smp_lbs,X_tar_tf,tar_smp_lbs,X_test_tf,src_wdi_tf,tar_wdi_tf):\n",
    "    \"\"\"\n",
    "    Get 30 neighbors from target sentences, and 30 from source sentences, take difference to get treatment effect.\n",
    "    \"\"\"\n",
    "    X_test_tf_ = X_test_tf[:]\n",
    "    X_test_tf_[:,src_wdi_tf]=0\n",
    "    \n",
    "    src_sim = cosine_similarity(X_test_tf_,X_src_tf)\n",
    "    tar_sim = cosine_similarity(X_test_tf_,X_tar_tf)\n",
    "\n",
    "    KNN_srcp = []\n",
    "    KNN_tarp = []\n",
    "    KNN_effects = []\n",
    "    for i in range(X_test_tf_.shape[0]):\n",
    "        src_neighbor_idx = src_sim[i].argsort()[::-1][:min(30,len(src_sim[i]))]\n",
    "        src_neighbor_labels = [src_smp_lbs[idx] for idx in src_neighbor_idx]\n",
    "\n",
    "        tar_neighbor_idx = tar_sim[i].argsort()[::-1][:min(30,len(tar_sim[i]))]\n",
    "        tar_neighbor_labels = [tar_smp_lbs[idx] for idx in tar_neighbor_idx]\n",
    "        \n",
    "        KNN_srcp.append(float('%.3f' % (np.mean(src_neighbor_labels))))\n",
    "        KNN_tarp.append(float('%.3f' % (np.mean(tar_neighbor_labels))))\n",
    "        KNN_effects.append(float('%.3f' % (np.mean(tar_neighbor_labels) - np.mean(src_neighbor_labels))))\n",
    "        \n",
    "    return KNN_srcp,KNN_tarp,KNN_effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def fit_VTRF(X_src_ct,src_smp_lbs,X_tar_ct,tar_smp_lbs,X_test_ct,src_wdi_ct,tar_wdi_ct):\n",
    "    \"\"\"\n",
    "    Fit one random forest using both source and target sentences.\n",
    "    Make virtual twins for test instances.\n",
    "    Predict for test instances and virtual instances, take difference as treatment effects.\n",
    "    \"\"\"\n",
    "    X_train = vstack((X_src_ct,X_tar_ct))\n",
    "    Y_train = list(src_smp_lbs) + list(tar_smp_lbs)\n",
    "    \n",
    "    RF_clf = RandomForestClassifier(n_estimators = min(200,X_train.shape[0]), max_features='log2', min_samples_leaf=2, \n",
    "                                        random_state=42, n_jobs=-1, oob_score=True)\n",
    "    RF_clf.fit(X_train, Y_train)\n",
    "    \n",
    "    # create virtual twins for test instances\n",
    "    X_test_vt = X_test_ct[:]\n",
    "    X_test_vt[:,src_wdi_ct] = 0\n",
    "    X_test_vt[:,tar_wdi_ct] = 1\n",
    "    \n",
    "    # Take difference as treatment effect\n",
    "    vt_pred = np.array(RF_clf.predict_proba(X_test_vt)[:,list(RF_clf.classes_).index(1)])\n",
    "    src_pred = np.array(RF_clf.predict_proba(X_test_ct)[:,list(RF_clf.classes_).index(1)])\n",
    "        \n",
    "    return [float('%.3f' % (src_e)) for src_e in src_pred],[float('%.3f' % (tar_e)) for tar_e in vt_pred],[float('%.3f' % (effect)) for effect in vt_pred-src_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def fit_CFRF(X_src_ct,src_smp_lbs,X_tar_ct,tar_smp_lbs,X_test_ct,src_wdi_ct,tar_wdi_ct):\n",
    "    \"\"\"\n",
    "    Fit one source random forest using source sentences and one target random forest using target sentences.\n",
    "    Make virtual twins for test instances.\n",
    "    Predict for test instances using source random forest, and predict for virtual instances using target random forest.\n",
    "    Take difference as treatment effects.\n",
    "    \"\"\"\n",
    "    src_RF_clf = RandomForestClassifier(n_estimators = min(200,X_src_ct.shape[0]), max_features='log2', min_samples_leaf=2, \n",
    "                                        random_state=42, n_jobs=-1, oob_score=True)\n",
    "    tar_RF_clf = RandomForestClassifier(n_estimators = min(200,X_tar_ct.shape[0]), max_features='log2', min_samples_leaf=2, \n",
    "                                        random_state=42, n_jobs=-1, oob_score=True)\n",
    "\n",
    "    src_RF_clf.fit(X_src_ct, src_smp_lbs)\n",
    "    tar_RF_clf.fit(X_tar_ct, tar_smp_lbs)\n",
    "    \n",
    "    # create virtual twins for test instances\n",
    "    X_test_vt = X_test_ct[:]\n",
    "    X_test_vt[:,src_wdi_ct] = 0\n",
    "    X_test_vt[:,tar_wdi_ct] = 1\n",
    "\n",
    "    vt_pred = np.array(tar_RF_clf.predict_proba(X_test_vt)[:,list(tar_RF_clf.classes_).index(1)])\n",
    "    src_pred = np.array(src_RF_clf.predict_proba(X_test_ct)[:,list(src_RF_clf.classes_).index(1)])\n",
    "        \n",
    "    return [float('%.3f' % (src_e)) for src_e in src_pred],[float('%.3f' % (tar_e)) for tar_e in vt_pred],[float('%.3f' % (effect)) for effect in vt_pred-src_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def fit_CSF(X_src_ct,src_smp_lbs,X_tar_ct,tar_smp_lbs,X_test_ct,src_wdi_ct,tar_wdi_ct):\n",
    "    \"\"\"\n",
    "    Fit one causal forest, set treatment variable values.\n",
    "    Calculate treatment effect directly.\n",
    "    \"\"\"\n",
    "    X_test_arr = X_test_ct.toarray()\n",
    "    R_test_ct = r.matrix(X_test_arr, nrow=X_test_arr.shape[0], ncol=X_test_arr.shape[1])\n",
    "    train_vec = vstack((X_src_ct,X_tar_ct)).toarray()\n",
    "    R_train_vec = r.matrix(train_vec, nrow=train_vec.shape[0], ncol=train_vec.shape[1])\n",
    "    R_train_y = list(src_smp_lbs) + list(tar_smp_lbs)\n",
    "    R_train_w = list(np.zeros(len(src_smp_lbs))) + list(np.ones(len(tar_smp_lbs)))\n",
    "    \n",
    "    r.assign('X.train',R_train_vec)\n",
    "    r.assign('X.test',R_test_ct)\n",
    "    r.assign('Y',ro.FloatVector(R_train_y))\n",
    "    r.assign('W',ro.IntVector(R_train_w))\n",
    "    \n",
    "    r('c.src_forest200 <- causal_forest(X.train,Y,W, num.trees=200, num.threads=8, seed=set.seed(10),precompute.nuisance = FALSE)')\n",
    "    r('c.pred_src200 <- predict(c.src_forest200, X.test, num.threads=8, estimate.variance = TRUE)')\n",
    "\n",
    "    r('c.src_forest2000 <- causal_forest(X.train,Y,W, num.trees=2000, num.threads=8, seed=set.seed(10),precompute.nuisance = FALSE)')\n",
    "    r('c.pred_src2000 <- predict(c.src_forest2000, X.test, num.threads=8, estimate.variance = TRUE)')\n",
    "\n",
    "    \n",
    "    CSF_pred_200 = [float('%.3f' % (pred)) for pred in r('c.pred_src200')[0]]\n",
    "    CSF_var_200 = [float('%.3f' % (var)) for var in r('c.pred_src200')[1]]\n",
    "    CSF_pred_2000 = [float('%.3f' % (pred)) for pred in r('c.pred_src2000')[0]]\n",
    "    CSF_var_2000 = [float('%.3f' % (var)) for var in r('c.pred_src2000')[1]]\n",
    "    \n",
    "    return CSF_pred_200,CSF_var_200,CSF_pred_2000,CSF_var_2000\n",
    "    #return r('c.pred_src200'),r('c.pred_src2000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_containing_sents(word_pair,data_file):\n",
    "    \"\"\"\n",
    "    Get all sentences containing source word and target word\n",
    "    \"\"\"\n",
    "    data_pd = pd.DataFrame(pickle.load(open(data_file,'rb')))\n",
    "    src_tar_pd = data_pd[(data_pd.source == word_pair[0]) & (data_pd.target == word_pair[1])]\n",
    "    tar_src_pd = data_pd[(data_pd.source == word_pair[1]) & (data_pd.target == word_pair[0])]\n",
    "    \n",
    "    src_sents = []\n",
    "    for sent in src_tar_pd.sentence.values:\n",
    "        src_sents.append(re.sub('-RRB-',')',re.sub('-LRB-','(',sent)))\n",
    "        \n",
    "    tar_sents = []\n",
    "    for sent in tar_src_pd.sentence.values:\n",
    "        tar_sents.append(re.sub('-RRB-',')',re.sub('-LRB-','(',sent)))\n",
    "        \n",
    "    return src_sents, list(src_tar_pd.true_y.values), tar_sents, list(tar_src_pd.true_y.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def vectorize_sents(src_sents,tar_sents,current_sents,my_vocab):\n",
    "    if(len(my_vocab)>0):\n",
    "        ct_vec = CountVectorizer(min_df=3,binary=True,vocabulary=my_vocab)\n",
    "        tfidf_vec = TfidfVectorizer(norm='l2',vocabulary=my_vocab)\n",
    "    else:\n",
    "        ct_vec = CountVectorizer(min_df=3,binary=True)\n",
    "        tfidf_vec = TfidfVectorizer(norm='l2')\n",
    "    \n",
    "    X_ct = ct_vec.fit_transform(src_sents+tar_sents+current_sents)\n",
    "    X_tfidf = tfidf_vec.fit_transform(src_sents+tar_sents+current_sents)\n",
    "    return X_ct[:len(src_sents)],X_ct[len(src_sents):X_ct.shape[0]-len(current_sents)],X_ct[-len(current_sents):], X_tfidf[:len(src_sents)],X_tfidf[len(src_sents):X_tfidf.shape[0]-len(current_sents)],X_tfidf[-len(current_sents):], ct_vec.vocabulary_, tfidf_vec.vocabulary_    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_samples(X_ct,X_tf,labels,perct):\n",
    "    smp_idx = random.sample(range(X_ct.shape[0]),int(X_ct.shape[0]*perct))\n",
    "    X_smp_ct = X_ct[smp_idx]\n",
    "    X_smp_tf = X_tf[smp_idx]\n",
    "    smp_lbs = [labels[idx] for idx in smp_idx]\n",
    "    return X_smp_ct,X_smp_tf,smp_lbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def compute_variance(amt_path,data_file,vocab_file,n_repeat,perct):\n",
    "    \"\"\"\n",
    "    Get containing sentences for each word pair, and exclude the 3 sentences present in file.\n",
    "    Select 90% of containing sentences as training instances, fit model and return prediction, repeat for 100 times.\n",
    "    Compute variance using 100 repeated predictions.\n",
    "    \"\"\"\n",
    "    for dataset in ['yp','tw','airbnb']:\n",
    "        if(dataset != 'airbnb'):\n",
    "            continue\n",
    "        amt_pd = pd.read_csv(amt_path+'results/'+dataset+'_result_summary_nodummy.csv')\n",
    "\n",
    "        all_info = []\n",
    "        for wdpair, row in amt_pd.groupby(['source','target']):\n",
    "            print(wdpair)\n",
    "            \n",
    "            # Step 1, get all containing sentences\n",
    "            current_sents = list(row.src_sentence.values)\n",
    "            all_src_sents, all_src_labels, all_tar_sents, all_tar_labels = get_containing_sents(wdpair,data_file[dataset])\n",
    "            \n",
    "            # Step 2: exclude current sentences from containing sentences, for out-of-bag prediction.\n",
    "            current_ids = [all_src_sents.index(cur_sent) for cur_sent in current_sents]\n",
    "            src_train_sents = []\n",
    "            src_train_labels = []\n",
    "            for i in range(len(all_src_sents)):\n",
    "                if(i not in current_ids):\n",
    "                    src_train_sents.append(all_src_sents[i])\n",
    "                    src_train_labels.append(all_src_labels[i])\n",
    "\n",
    "            # Vectorize sentences\n",
    "            if(dataset == 'airbnb'):\n",
    "                my_vocab = list(set(pd.read_csv(vocab_file).word.values))\n",
    "            else:\n",
    "                my_vocab = []\n",
    "\n",
    "            X_src_ct,X_tar_ct,X_test_ct,X_src_tf,X_tar_tf,X_test_tf,ct_vocab,tf_vocab = vectorize_sents(src_train_sents,all_tar_sents,current_sents,my_vocab)\n",
    "\n",
    "            #return X_src_ct,X_tar_ct,X_test_ct,X_src_tf,X_tar_tf,X_test_tf\n",
    "            src_wdi_ct = ct_vocab[wdpair[0]]\n",
    "            src_wdi_tf = tf_vocab[wdpair[0]]\n",
    "            tar_wdi_ct = ct_vocab[wdpair[1]]\n",
    "            tar_wdi_tf = tf_vocab[wdpair[1]]\n",
    "\n",
    "            # Step 3: select 90% as training samples to fit different models and predict for testing data, repeat for 100 times\n",
    "            KNN_src = []\n",
    "            KNN_tar = []\n",
    "            KNN_effect = []\n",
    "            VTRF_src = []\n",
    "            VTRF_tar = []\n",
    "            VTRF_effect = []\n",
    "            CFRF_src = []\n",
    "            CFRF_tar = []\n",
    "            CFRF_effect = []\n",
    "            CSF_effect200 = []\n",
    "            CSF_effect2000 = []\n",
    "\n",
    "            for i in range(n_repeat):\n",
    "                \"\"\"\n",
    "                Separate sample selection for source and target, to keep source and target in same ratio.\n",
    "                \"\"\"\n",
    "                X_src_smp_ct,X_src_smp_tf,src_smp_lbs = get_samples(X_src_ct,X_src_tf,src_train_labels,perct)\n",
    "                X_tar_smp_ct,X_tar_smp_tf,tar_smp_lbs = get_samples(X_tar_ct,X_tar_tf,all_tar_labels,perct)\n",
    "\n",
    "                KNN_srcp,KNN_tarp,KNN_eft = fit_KNN(X_src_smp_tf,src_smp_lbs,X_tar_smp_tf,tar_smp_lbs,X_test_tf,src_wdi_tf,tar_wdi_tf)\n",
    "                KNN_src.append(KNN_srcp)\n",
    "                KNN_tar.append(KNN_tarp)\n",
    "                KNN_effect.append(KNN_eft)\n",
    "                VT_src,VT_tar,VT_effect = fit_VTRF(X_src_smp_ct,src_smp_lbs,X_tar_smp_ct,tar_smp_lbs,X_test_ct,src_wdi_ct,tar_wdi_ct)\n",
    "                VTRF_src.append(VT_src)\n",
    "                VTRF_tar.append(VT_tar)\n",
    "                VTRF_effect.append(VT_effect)\n",
    "                CTF_src,CTF_tar,CTF_effect = fit_CFRF(X_src_smp_ct,src_smp_lbs,X_tar_smp_ct,tar_smp_lbs,X_test_ct,src_wdi_ct,tar_wdi_ct)\n",
    "                CFRF_src.append(CTF_src)\n",
    "                CFRF_tar.append(CTF_tar)\n",
    "                CFRF_effect.append(CTF_effect)\n",
    "#                 rpred200,rpred2000 = fit_CSF(X_src_smp_ct,src_smp_lbs,X_tar_smp_ct,tar_smp_lbs,X_test_ct,src_wdi_ct,tar_wdi_ct)\n",
    "#                 return rpred200, rpred2000\n",
    "                pred200,var200,pred2000,var2000 = fit_CSF(X_src_smp_ct,src_smp_lbs,X_tar_smp_ct,tar_smp_lbs,X_test_ct,src_wdi_ct,tar_wdi_ct)\n",
    "                CSF_effect200.append(pred200)\n",
    "                CSF_effect2000.append(pred2000)\n",
    "\n",
    "            # Step 4: calculate variance using repeated predictions.\n",
    "\n",
    "            CSF_direct_pred200,CSF_direct_var200, CSF_direct_pred2000,CSF_direct_var2000 = fit_CSF(X_src_ct,src_train_labels,X_tar_ct,all_tar_labels,X_test_ct,src_wdi_ct,tar_wdi_ct)\n",
    "\n",
    "            for i in range(row.shape[0]):\n",
    "                new_info = row.iloc[i].to_dict()\n",
    "                new_info['n_src'] = len(all_src_sents)\n",
    "                new_info['n_tar'] = len(all_tar_sents)\n",
    "                new_info['KNN_srcp'] = list(np.array(KNN_src)[:,i])\n",
    "                new_info['KNN_tarp'] = list(np.array(KNN_tar)[:,i])\n",
    "                new_info['KNN_var'] = list(np.array(KNN_effect)[:,i])\n",
    "                new_info['VTRF_srcp'] = list(np.array(VTRF_src)[:,i])\n",
    "                new_info['VTRF_tarp'] = list(np.array(VTRF_tar)[:,i])\n",
    "                new_info['VTRF_var'] = list(np.array(VTRF_effect)[:,i])\n",
    "                new_info['CFRF_srcp'] = list(np.array(CFRF_src)[:,i])\n",
    "                new_info['CFRF_tarp'] = list(np.array(CFRF_tar)[:,i])\n",
    "                new_info['CFRF_var'] = list(np.array(CFRF_effect)[:,i])\n",
    "                new_info['CSF_pred_var200'] = list(np.array(CSF_effect200)[:,i])\n",
    "                new_info['CSF_pred_var2000'] = list(np.array(CSF_effect2000)[:,i])\n",
    "                new_info['CSF_direct_var200'] = CSF_direct_var200[i]\n",
    "                new_info['CSF_direct_var2000'] = CSF_direct_var2000[i]\n",
    "\n",
    "                all_info.append(new_info)\n",
    "            \n",
    "\n",
    "        pd.DataFrame(all_info).to_csv(project_path+'V2_'+full_name[dataset].lower()+'/8_Var/'+dataset+'_effect_var_prob4.csv',\n",
    "        columns = list(amt_pd.columns)+['n_src','n_tar','KNN_srcp','KNN_tarp','KNN_var','VTRF_srcp','VTRF_tarp','VTRF_var','CFRF_srcp','CFRF_tarp','CFRF_var','CSF_pred_var200','CSF_pred_var2000','CSF_direct_var200','CSF_direct_var2000'],\n",
    "                                      index = False)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('amazing', 'fabulous')\n",
      "('amazing', 'fantastic')\n",
      "('amazing', 'incredible')\n",
      "('amazing', 'outstanding')\n",
      "('amazing', 'spectacular')\n",
      "('amazing', 'wonderful')\n",
      "('apartment', 'condo')\n",
      "('apartments', 'homes')\n",
      "('best', 'nicest')\n",
      "('cheap', 'inexpensive')\n",
      "('comfortable', 'cozy')\n",
      "('dynamic', 'vibrant')\n",
      "('events', 'festivals')\n",
      "('exciting', 'fabulous')\n",
      "('exciting', 'interesting')\n",
      "('exciting', 'spectacular')\n",
      "('exciting', 'stunning')\n",
      "('famous', 'big')\n",
      "('famous', 'excellent')\n",
      "('famous', 'grand')\n",
      "('famous', 'old')\n",
      "('famous', 'renowned')\n",
      "('famous', 'special')\n",
      "('famous', 'wonderful')\n",
      "('hot', 'excellent')\n",
      "('huge', 'spectacular')\n",
      "('nice', 'gorgeous')\n",
      "('notable', 'unique')\n",
      "('open', 'outstanding')\n",
      "('predominantly', 'especially')\n",
      "('predominantly', 'mostly')\n",
      "('rapidly', 'quickly')\n",
      "('square', 'plaza')\n",
      "('store', 'boutique')\n",
      "('trips', 'tours')\n",
      "('various', 'several')\n",
      "('yummy', 'delicious')\n",
      "('yummy', 'good')\n",
      "394.30571784178414\n"
     ]
    }
   ],
   "source": [
    "data_file = {}\n",
    "data_file['yp'] = project_path + 'V2_yelp/5_Select/yp_wdpair_sents_4effects_posinters_poscom1_bigramcheck_limit.pickle'\n",
    "data_file['tw'] = project_path + 'V2_twitter/5_Select/tw_wdpair_sents_4effects_posinters_poscom1_bigramcheck_limit.pickle'\n",
    "data_file['airbnb'] = project_path + 'V2_airbnb/5_Select/airbnb_wdpair_sents_4effects_posinters_poscom1_bigramcheck_limit_limitvocab.pickle'\n",
    "\n",
    "vocab_file = project_path + 'V2_airbnb/0_Data/common_wds.csv'\n",
    "#effect_var_file = project_path+'V2_yelp/8_Var/yp_effect_var.csv'\n",
    "start = time.time()\n",
    "compute_variance(amt_path,data_file,vocab_file,n_repeat=100,perct=0.85)\n",
    "end = time.time()\n",
    "print((end-start)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def compute_variance(amt_path,data_file,vocab_file,dataset,n_repeat,perct):\n",
    "    \"\"\"\n",
    "    Get containing sentences for each word pair, and exclude the 3 sentences present in file.\n",
    "    Select 90% of containing sentences as training instances, fit model and return prediction, repeat for 100 times.\n",
    "    Compute variance using 100 repeated predictions.\n",
    "    \"\"\"\n",
    "    \n",
    "    amt_pd = pd.read_csv(amt_path+'results/airbnb_result_summary_nodummy.csv')\n",
    "\n",
    "    all_info = []\n",
    "    for wdpair, row in amt_pd.groupby(['source','target']):\n",
    "        print(wdpair)\n",
    "        # Step 1, get all containing sentences\n",
    "        current_sents = list(row.src_sentence.values)\n",
    "        all_src_sents, all_src_labels, all_tar_sents, all_tar_labels = get_containing_sents(wdpair,data_file)\n",
    "\n",
    "        # Step 2: exclude current sentences from containing sentences, for out-of-bag prediction.\n",
    "        current_ids = [all_src_sents.index(cur_sent) for cur_sent in current_sents]\n",
    "        src_train_sents = []\n",
    "        src_train_labels = []\n",
    "        for i in range(len(all_src_sents)):\n",
    "            if(i not in current_ids):\n",
    "                src_train_sents.append(all_src_sents[i])\n",
    "                src_train_labels.append(all_src_labels[i])\n",
    "\n",
    "        # Vectorize sentences\n",
    "        if(dataset == 'airbnb'):\n",
    "            my_vocab = list(set(pd.read_csv(vocab_file).word.values))\n",
    "        else:\n",
    "            my_vocab = []\n",
    "\n",
    "        X_src_ct,X_tar_ct,X_test_ct,X_src_tf,X_tar_tf,X_test_tf,ct_vocab,tf_vocab = vectorize_sents(src_train_sents,all_tar_sents,current_sents,my_vocab)\n",
    "\n",
    "        #return X_src_ct,X_tar_ct,X_test_ct,X_src_tf,X_tar_tf,X_test_tf\n",
    "        src_wdi_ct = ct_vocab[wdpair[0]]\n",
    "        src_wdi_tf = tf_vocab[wdpair[0]]\n",
    "        tar_wdi_ct = ct_vocab[wdpair[1]]\n",
    "        tar_wdi_tf = tf_vocab[wdpair[1]]\n",
    "\n",
    "        # Step 3: select 90% as training samples to fit different models and predict for testing data, repeat for 100 times\n",
    "        KNN_pred = []\n",
    "        VTRF_pred = []\n",
    "        CFRF_pred = []\n",
    "        CSF_pred200 = []\n",
    "        CSF_pred2000 = []\n",
    "\n",
    "        for i in range(n_repeat):\n",
    "            X_src_smp_ct,X_src_smp_tf,src_smp_lbs = get_samples(X_src_ct,X_src_tf,src_train_labels,perct)\n",
    "            X_tar_smp_ct,X_tar_smp_tf,tar_smp_lbs = get_samples(X_tar_ct,X_tar_tf,all_tar_labels,perct)\n",
    "\n",
    "            KNN_pred.append(fit_KNN(X_src_smp_tf,src_smp_lbs,X_tar_smp_tf,tar_smp_lbs,X_test_tf,src_wdi_tf,tar_wdi_tf))\n",
    "            VTRF_pred.append(fit_VTRF(X_src_smp_ct,src_smp_lbs,X_tar_smp_ct,tar_smp_lbs,X_test_ct,src_wdi_ct,tar_wdi_ct))\n",
    "            CFRF_pred.append(fit_CFRF(X_src_smp_ct,src_smp_lbs,X_tar_smp_ct,tar_smp_lbs,X_test_ct,src_wdi_ct,tar_wdi_ct))\n",
    "            pred200,var200,pred2000,var2000 = fit_CSF(X_src_smp_ct,src_smp_lbs,X_tar_smp_ct,tar_smp_lbs,X_test_ct,src_wdi_ct,tar_wdi_ct)\n",
    "            CSF_pred200.append(pred200)\n",
    "            CSF_pred2000.append(pred2000)\n",
    "\n",
    "        # Step 4: calculate variance using repeated predictions.\n",
    "\n",
    "        CSF_direct_pred200,CSF_direct_var200, CSF_direct_pred2000,CSF_direct_var2000 = fit_CSF(X_src_ct,src_train_labels,X_tar_ct,all_tar_labels,X_test_ct,src_wdi_ct,tar_wdi_ct)\n",
    "\n",
    "        for i in range(row.shape[0]):\n",
    "            new_info = row.iloc[i].to_dict()\n",
    "            new_info['n_src'] = len(all_src_sents)\n",
    "            new_info['n_tar'] = len(all_tar_sents)\n",
    "            new_info['KNN_var'] = list(np.array(KNN_pred)[:,i])\n",
    "            new_info['VTRF_var'] = list(np.array(VTRF_pred)[:,i])\n",
    "            new_info['CFRF_var'] = list(np.array(CFRF_pred)[:,i])\n",
    "            new_info['CSF_pred_var200'] = list(np.array(CSF_pred200)[:,i])\n",
    "            new_info['CSF_pred_var2000'] = list(np.array(CSF_pred2000)[:,i])\n",
    "            new_info['CSF_direct_var200'] = CSF_direct_var200[i]\n",
    "            new_info['CSF_direct_var2000'] = CSF_direct_var2000[i]\n",
    "\n",
    "            all_info.append(new_info)\n",
    "\n",
    "\n",
    "    pd.DataFrame(all_info).to_csv(project_path+'V2_airbnb/8_Var/airbnb_effect_var.csv',\n",
    "                                  columns = list(amt_pd.columns)+['n_src','n_tar','KNN_var','VTRF_var','CFRF_var','CSF_pred_var200','CSF_pred_var2000','CSF_direct_var200','CSF_direct_var2000'],\n",
    "                                  index = False)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('amazing', 'fabulous')\n",
      "('amazing', 'fantastic')\n",
      "('amazing', 'incredible')\n",
      "('amazing', 'outstanding')\n",
      "('amazing', 'spectacular')\n",
      "('amazing', 'wonderful')\n",
      "('apartment', 'condo')\n",
      "('apartments', 'homes')\n",
      "('best', 'nicest')\n",
      "('cheap', 'inexpensive')\n",
      "('comfortable', 'cozy')\n",
      "('dynamic', 'vibrant')\n",
      "('events', 'festivals')\n",
      "('exciting', 'fabulous')\n",
      "('exciting', 'interesting')\n",
      "('exciting', 'spectacular')\n",
      "('exciting', 'stunning')\n",
      "('famous', 'big')\n",
      "('famous', 'excellent')\n",
      "('famous', 'grand')\n",
      "('famous', 'old')\n",
      "('famous', 'renowned')\n",
      "('famous', 'special')\n",
      "('famous', 'wonderful')\n",
      "('hot', 'excellent')\n",
      "('huge', 'spectacular')\n",
      "('nice', 'gorgeous')\n",
      "('notable', 'unique')\n",
      "('open', 'outstanding')\n",
      "('predominantly', 'especially')\n",
      "('predominantly', 'mostly')\n",
      "('rapidly', 'quickly')\n",
      "('square', 'plaza')\n",
      "('store', 'boutique')\n",
      "('trips', 'tours')\n",
      "('various', 'several')\n",
      "('yummy', 'delicious')\n",
      "('yummy', 'good')\n"
     ]
    }
   ],
   "source": [
    "data_file = project_path + 'V2_airbnb/5_Select/airbnb_wdpair_sents_4effects_posinters_poscom1_bigramcheck_limit_limitvocab.pickle'\n",
    "vocab_file = project_path + 'V2_airbnb/0_Data/common_wds.csv'\n",
    "#effect_var_file = project_path+'V2_yelp/8_Var/yp_effect_var.csv'\n",
    "compute_variance(amt_path,data_file,vocab_file,dataset='airbnb',n_repeat=100,perct=0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "res_pd = pd.read_csv(project_path+'V2_yelp/8_Var/yp_effect_var.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[-0.29999999999999999, -0.29999999999999999, -0.33300000000000002, -0.33300000000000002, -0.23300000000000001, -0.36699999999999999, -0.23300000000000001, -0.33300000000000002, -0.26700000000000002, -0.29999999999999999]'"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(res_pd.KNN_var.values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yelp'"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_name['yp'].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.3, -0.3, -0.333, -0.333, -0.233, -0.367, -0.233, -0.333, -0.267, -0.3]"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = ast.literal_eval(res_pd.KNN_var.values[0])\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.25"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.var([6,9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Container object of 3 artists>"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWoAAAEACAYAAACatzzfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ffbada72400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "x = [1,2,3,4,5,6]\n",
    "y = [[1,2,3],[2,3,5],[1,3,9],[6,8,7],[5,4,6],[8,2,0]]\n",
    "y_mean = [np.mean(l) for l in y]\n",
    "y_floor = [np.mean(l)-np.min(l) for l in y]\n",
    "y_ceil = [np.max(l)-np.mean(l) for l in y]\n",
    "y_err = [np.var(l) for l in y]\n",
    "plt.errorbar(x,y_mean,yerr=[y_floor,y_ceil],fmt='-o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"This cute girl in a pink shirt ( I 'm not sure if this was intentional but she matched some of the macaroons and gelato ) waited on me .\""
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new1 = re.sub('-LRB-','(',my_sent)\n",
    "new2 = re.sub('-RRB-',')',new1)\n",
    "new2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1394, 1076), (194, 1076), (12, 1076))"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_src_ct.shape, X_tar_ct.shape, X_test_ct.shape, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1600, 1076), 1394, 194)"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_ct.shape, nsrc_sents, ntar_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-241-3b871724a824>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_smp_ct\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_smp_tf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msmp_lbs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_tar_ct\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_tar_tf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mall_tar_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-75-4f889fd1eba6>\u001b[0m in \u001b[0;36mget_samples\u001b[0;34m(X_ct, X_tf, labels, perct)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mX_smp_ct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_ct\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msmp_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mX_smp_tf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_tf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msmp_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0msmp_lbs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msmp_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mX_smp_ct\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_smp_tf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msmp_lbs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-75-4f889fd1eba6>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mX_smp_ct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_ct\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msmp_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mX_smp_tf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_tf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msmp_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0msmp_lbs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msmp_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mX_smp_ct\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_smp_tf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msmp_lbs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "X_smp_ct,X_smp_tf,smp_lbs = get_samples(X_tar_ct,X_tar_tf,all_tar_labels,0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CFRF_pred200' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-174-37418f697e88>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCFRF_pred200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'CFRF_pred200' is not defined"
     ]
    }
   ],
   "source": [
    "len(np.array(CFRF_pred200)[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CFRF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>0.283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>0.063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>0.266</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CFRF\n",
       "108  0.283\n",
       "109  0.063\n",
       "110  0.266"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pd = pd.DataFrame(CFRF_pred[0],columns=['CFRF']).rename({0:108,1:109,2:110})\n",
    "test_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'amt_effect_mean': 0.19999999999999973,\n",
       " 'amt_effect_median': 0.0,\n",
       " 'amt_effect_mostcom': -0.5,\n",
       " 'csf_effect': 0.49689,\n",
       " 'ctf_effect': 0.45440999999999998,\n",
       " 'id': 1867234,\n",
       " 'knn_effect': 0.40000000000000002,\n",
       " 'source': 'belongings',\n",
       " 'src_ratings': '[2, 3, 3, 3, 4, 4, 4, 4, 4, 5]',\n",
       " 'src_sentence': \"This company should change their name to ` No Longer Starving ' , because the customers will pay ANY amount just to get their belongings back .\",\n",
       " 'tar_ratings': '[3, 3, 3, 3, 4, 4, 4, 4, 5, 5]',\n",
       " 'tar_sentence': \"This company should change their name to ` No Longer Starving ' , because the customers will pay ANY amount just to get their properties back .\",\n",
       " 'target': 'properties',\n",
       " 'true_y': 0,\n",
       " 'vt_effect': 0.087720000000000006}"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row.iloc[0].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>src_sentence</th>\n",
       "      <th>tar_sentence</th>\n",
       "      <th>knn_effect</th>\n",
       "      <th>vt_effect</th>\n",
       "      <th>ctf_effect</th>\n",
       "      <th>csf_effect</th>\n",
       "      <th>true_y</th>\n",
       "      <th>id</th>\n",
       "      <th>src_ratings</th>\n",
       "      <th>tar_ratings</th>\n",
       "      <th>amt_effect_mean</th>\n",
       "      <th>amt_effect_median</th>\n",
       "      <th>amt_effect_mostcom</th>\n",
       "      <th>CFRF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>belongings</td>\n",
       "      <td>properties</td>\n",
       "      <td>This company should change their name to ` No ...</td>\n",
       "      <td>This company should change their name to ` No ...</td>\n",
       "      <td>0.40000</td>\n",
       "      <td>0.08772</td>\n",
       "      <td>0.45441</td>\n",
       "      <td>0.49689</td>\n",
       "      <td>0</td>\n",
       "      <td>1867234</td>\n",
       "      <td>[2, 3, 3, 3, 4, 4, 4, 4, 4, 5]</td>\n",
       "      <td>[3, 3, 3, 3, 4, 4, 4, 4, 5, 5]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>belongings</td>\n",
       "      <td>properties</td>\n",
       "      <td>You can store your belongings in a locker .</td>\n",
       "      <td>You can store your properties in a locker .</td>\n",
       "      <td>0.16667</td>\n",
       "      <td>0.05213</td>\n",
       "      <td>0.17848</td>\n",
       "      <td>0.11292</td>\n",
       "      <td>0</td>\n",
       "      <td>1867247</td>\n",
       "      <td>[2, 3, 3, 3, 3, 4, 4, 4, 5, 5]</td>\n",
       "      <td>[3, 3, 3, 3, 3, 3, 4, 4, 4, 5]</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>belongings</td>\n",
       "      <td>properties</td>\n",
       "      <td>After two hours of putting up a fuss about my ...</td>\n",
       "      <td>After two hours of putting up a fuss about my ...</td>\n",
       "      <td>0.26667</td>\n",
       "      <td>0.10390</td>\n",
       "      <td>0.29366</td>\n",
       "      <td>0.28979</td>\n",
       "      <td>0</td>\n",
       "      <td>1867252</td>\n",
       "      <td>[1, 1, 2, 3, 3, 3, 3, 4, 4, 4]</td>\n",
       "      <td>[1, 1, 2, 2, 3, 3, 3, 4, 4, 5]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.266</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         source      target  \\\n",
       "108  belongings  properties   \n",
       "109  belongings  properties   \n",
       "110  belongings  properties   \n",
       "\n",
       "                                          src_sentence  \\\n",
       "108  This company should change their name to ` No ...   \n",
       "109        You can store your belongings in a locker .   \n",
       "110  After two hours of putting up a fuss about my ...   \n",
       "\n",
       "                                          tar_sentence  knn_effect  vt_effect  \\\n",
       "108  This company should change their name to ` No ...     0.40000    0.08772   \n",
       "109        You can store your properties in a locker .     0.16667    0.05213   \n",
       "110  After two hours of putting up a fuss about my ...     0.26667    0.10390   \n",
       "\n",
       "     ctf_effect  csf_effect  true_y       id                     src_ratings  \\\n",
       "108     0.45441     0.49689       0  1867234  [2, 3, 3, 3, 4, 4, 4, 4, 4, 5]   \n",
       "109     0.17848     0.11292       0  1867247  [2, 3, 3, 3, 3, 4, 4, 4, 5, 5]   \n",
       "110     0.29366     0.28979       0  1867252  [1, 1, 2, 3, 3, 3, 3, 4, 4, 4]   \n",
       "\n",
       "                        tar_ratings  amt_effect_mean  amt_effect_median  \\\n",
       "108  [3, 3, 3, 3, 4, 4, 4, 4, 5, 5]              0.2                0.0   \n",
       "109  [3, 3, 3, 3, 3, 3, 4, 4, 4, 5]             -0.1               -0.5   \n",
       "110  [1, 1, 2, 2, 3, 3, 3, 4, 4, 5]              0.0                0.0   \n",
       "\n",
       "     amt_effect_mostcom   CFRF  \n",
       "108                -0.5  0.283  \n",
       "109                 0.0  0.063  \n",
       "110                 0.0  0.266  "
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([row,test_pd],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Possible imporvement\n",
    "- Increase minimal node size\n",
    "- Print total number of containing sentences for each source and target word\n",
    "- plot error bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'b', 'c', 'd', 'e', 'f']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "['a','b','c']+['d','e','f']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
